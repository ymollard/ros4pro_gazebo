{"./":{"url":"./","title":"Qu'est-ce que c'est ?","keywords":"","body":"Cours et Travaux pratiques de robotique ROS4PRO est un ensemble de ressources francophones pour l'apprentissage de la robotique opensource avec le framework ROS qui équipera plus de la moitié des robots vendus en 2024. Vous pouvez librement les suivre, les dupliquer et les enrichir puis partager vos améliorations. Comment suivre les TP ? 🤖 Il est probable que vous suiviez ces travaux pratiques dans le cadre d'une formation qui met à votre disposition du matériel robotique. Selon votre matériel, plusieurs parcours sont disponibles ci-dessous. Si vous n'avez pas de matériel, certains parcours peuvent être suivis partiellement en simulation. 💻 Vous devez disposer d'un ordinateur de type PC ainsi que Ubuntu 18.04 et ROS Melodic installés. 📀 Pour suivre tous les parcours sans avoir à installer ROS et les dépendances vous-même, vous pouvez télécharger cette clé USB Live et suivre les instructions de flash. Parcours possibles Parcours n° 1 : Poppy Ergo Jr + Keras + Turtlebot (possible partiellement en simulation) Introduction Navigation Manipulation Perception Intégration Parcours n° 2 : Sawyer + Keras + Turtlebot Introduction Navigation Manipulation Perception Intégration Légende Les pictogrammes suivants sont utilisés : 💻 : Procédure exécuter sur votre poste de travail Ubuntu 🤖 : Procédure à exécuter sur le robot, en utilisant SSH 📀 : Cette procédure est déjà faîte pour vous si vous lancez Ubuntu via une clé USB Live 🐍 : Code Python à enregistrer et exécuter sur votre poste de travail 📥 : Ressource à Télécharger ✍ : Répondre aux questions par écrit Photos de quelques TP passés en présentiel ou distanciel 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"introduction/":{"url":"introduction/","title":"I. Introduction à Linux et ROS","keywords":"","body":"I. Introduction à Linux, ROS et ROS-I Prérequis Lycée et + Notions de programmes informatiques, terminaux et commandes Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Démarrer Ubuntu et ROS Selon votre situation, une clé USB bootable peut vous être fournie. Dans ce cas, vous devez faire \"booter\" votre poste de travail sur la clé USB Live fournie. Dans le cas contraire, il est nécessaire d'avoir vous même installé Ubuntu 18.04 et ROS Melodic. Dans ce cas il se peut que vous ayez à installer vous-même des éléments supplémentaires tout le long des travaux pratiques. 2. Prise en main du terminal : le rosier 🌹 ⌨️ Pour prendre en main le terminal Linux et les commandes de base, à partir d'un terminal, créez les fichiers et dossiers nécessaires pour réaliser cette hierarchie de fichiers ressemblant à un rosier : Vous aurez besoin des commandes suivantes :h ls, pour lister les fichiers et dossiers courants cd, pour changer le dossier courant mkdir, pour créer un nouveau dossier touch, pour créer un nouveau fichier vide nano, pour créer un nouveau fichier et écrire à l'intérieur tree, pour afficher la hierarchie de fichiers 3. Tutoriels 🧑‍🏫 Vous êtes désormais prêt à utiliser ROS ! Suivez les tutoriels ROS suivants pour découvrir et tester les concepts de base, sachant que votre distribution ROS s'appelle melodic : Understanding ROS Nodes : Maîtriser ROS master (roscore) et lancer des nœuds (rosrun) Understanding ROS Topics : Envoyer et recevoir des messages dans un topic (rostopic) Understanding ROS Services and Parameters : Déclarer et appeler un service requête/réponse (rosservice, rossrv) ❓ Quizz : quizz au tableau pour mémoriser les commandes importantes 4. ⚙️ Préparer vos robots Pour l'un ou l'autre de vos 2 robots, réalisez les étapes de préparation suivantes expliquées dans la FAQ robots : Flasher sa carte SD Connecter le robot en wifi Se connecter via SSH au robot Personnaliser le nom de votre robot (si nécessaire) 5. FAQ 📥 Mise à jour pendant le TP Il se peut que l'enseignant mette à jour les ressources pendant le cours. Dans ce cas exécutez les commandes suivantes pour récupérer les dernières mises-à-jour : roscd ros4pro git pull origin poppy_tb3_keras Si l'erreur suivante survient : error: Vos modifications locales aux fichiers suivants seraient écrasées par la fusion : Veuillez valider ou remiser vos modifications avant la fusion. Abandon Alors les fichiers spécifiés ne peuvent pas être mis à jour car cela détruirait les modifications que vous avez apportées à la liste des fichiers indiquée. Il est recommandé de demander conseil avant d'essayer une autre action pour récupérer la mise à jour. Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"navigation/turtlebot/":{"url":"navigation/turtlebot/","title":"Turtlebot","keywords":"","body":"II. Robotique de navigation avec Turtlebot Prérequis Lycée et + Notions de Python et commandes dans un terminal Aisance en géométrie 2D Le TP d'introduction Ce TP est compatible avec la simulation si vous n'avez pas de Turtlebot Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. Travaux pratiques 1. Assemblage du Turtlebot (avec un robot réel) ⚠️ Attention la documentation officielle du Turtlebot convient très bien pour l'électromécanique mais la documentaiton logicielle est obsolète, ne tapez aucune commande de la documentation sans avoir demandé si elle convient ! ⚠️ Attention : vous ne pourrez faire aucune erreur de câblage sauf avec le câble d'alimentation de la Raspberry Pi qui doit impérativement être branché comme sur le schéma ci-dessous au risque de déteriorer définitivement le matériel. ▶️ Suivez cette vidéo pour assembler votre Turtlebot Burger : 2. Bringup du TB3 (avec un robot réel) 🔍 Vérifiez d'abord la configuration réseau de ROS sur votre PC et sur le TB3 : ROS_MASTER_URI doit pointer vers le Turtlebot. Vérifiez également que vous avez connecté le robot au Wifi et renommé votre robot en y ajoutant votre numéro de groupe (par ex burger8) avec les instructions de l'introduction. 💻 Lancez roscore dans un premier terminal. 🤖 Sur le TB3 lancer la commande roslaunch turtlebot3_bringup turtlebot3_robot.launch. S'il n'y a aucune erreur vous êtes prêt à piloter le robot depuis votre poste de travail, que ce soit pour la téléopération, la cartographie ou la navigation autonome. 2.bis. Bringup du Turtlebot (en simulation) ⚠️ Attention la simulation du TB3 n'est a utiliser qu'en dernier recours pour remplacer votre robot s'il ne fonctionne pas. Avant de passer en simulation demandez de l'aide pour réparer votre robot. 📥 Vous devez télécharger et installer le paquet ROS de simulation du TB3 : 💻 Lancez cd ~/catkin_ws/src dans un terminal pour vous déplacer dans le dossier contenant les sources de vos paquets ROS. 💻 Lancez git clone https://github.com/ros4pro/turtlebot3_simulations.git dans le même terminal, le dossier turtlebot3_simulations est créé dans le répertoire ~/catkin_ws/src. 💻 Lancez cd ..; catkin_make, le nouveau paquet est installé. Après la compilation lancez source ~/.bashrc dans chaque terminal pour les mettre à jour ou fermez les tous. 🔍 La simulation remplace le robot donc vous ne devez ni essayer de lancer le bringup du TB3 et ni vous connecter au robot. À la place vous devez lancer le simulateur et configurer ROS_MASTER_URI pour pointer vers votre PC (ROS master = cette machine). 💻 Lancez roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch, le simulateur Gazebo se lance et vous devez voir le TB3 au milieu de la fenêtre. Plusieurs environnements de simulation sont disponibles : turtlebot3_empty_world.launch : un monde vide, ne contenant que le TB3 et un sol. turtlebot3_house.launch : une maison avec plusieurs pièces et du mobilier. turtlebot3_world.launch : le TB3 est au milieu d'un carré. turtlebot3_stage_1.launch : le TB3 est dans une arène carrée. turtlebot3_stage_2.launch : le TB3 est dans une arène carré avec 4 obstacles fixes. turtlebot3_stage_3.launch : le TB3 est dans une arène carré avec 4 obstacles fixes. turtlebot3_stage_4.launch : le TB3 est dans une grande arène carrée avec plusieurs obstacles et des murs. 3. Téléopération 🎮 La première étape consiste à vérifier que votre poste de travail peut effectivement prendre le contrôle du Turtlebot, en le téléopérant via les touches du clavier. 💻 Dans un nouveau terminal lancez la commande roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch et gardez le focus sur le terminal pour controler le robot avec le clavier grâce aux touches indiquées. Vérifiez que vous pouvez avancer, reculer, tourner à gauche et à droite. Vous pouvez tuer ce dernier avec Ctrl+C lorsque vous avez terminé. 4. Cartographie 🗺️ Nous allons désormais créer la carte de l'environnement dans lequel votre Turtlebot évoluera lorsqu'il naviguera de manière autonome. 💻 Lancez le commande roslaunch turtlebot3_slam turtlebot3_slam.launch. RViz se lance et vous devriez apercevoir le robot, les scans du LIDAR et la carte en construction. 💻 Dans un nouveau terminal lancez la commande roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch et gardez le focus sur le terminal pour contrôler le robot avec le clavier comme précédemment. Cependant cette fois-ci, votre carte est en cours d'enregistrement. Quand la carte est terminée ne quittez ni RViz ni le terminal de la cartographie. 💻 Dans un nouveau terminal lancez la commande roscd turtlebot3_navigation/maps/ pour aller dans le dossier où la carte est enregistrée. 💾 La commande qui va suivre va supprimer la carte précédente s'il y en a une, le cas échéant faites-en une copie si vous souhaitez la conserver. Lancez la commande roslaunch ros4pro map_saver.launch qui va sauvegarder la carte dans les fichiers maps.yaml et maps.pgm et écraser les anciens. 5. Navigation 💻 Lancez le commande roslaunch turtlebot3_navigation turtlebot3_navigation.launch pour lancer la localisation et la navigation autonome. 👀 Sur RViz vous devez voir le robot, les scans du LIDAR, les particules de AMCL et la carte que vous avez enregistrée. 📍 Si le robot est mal localisé, utilisez l'outil 2D Pose Estimate sur RViz. Cliquez et Glissez avec la souris pour positionner le robot sur la carte. 📍 Pour donner des ordres de navigation, utilisez l'outil 2D Nav Goal sur RViz. Cliquez et Glissez avec la souris sur la carte là où le robot doit aller. 6. Scenario de navigation 🚗 L'objectif final du TP est de faire passer le robot par une suite de 4 ou 5 points de passage, comme pour une patrouille, avec un retour au point de départ. Si cela n'est pas déjà fait, choisissez plusieurs points de passage faciles à mesurer avec un mètre depuis le point de départ, avec un grand nombre d'obstacles sur le chemin. Si l'environnement a fortement changé, pensez à enregistrer une nouvelle carte. 🐍 Les commandes pour naviguer jusqu'à chaque point de passage seront des instructions dans un fichier Python. Le noeud navigation_scenario.py auquel vous pourrez accéder en tapant roscd ros4pro/src/nodes est une ébauche de script Python pour y parvenir. 🐍 Complétez ce fichier Python afin d'exécuter le scenario et ainsi effectuer la patrouille. Pour exécuter le scénario lancez la navigation en arrière plan comme indiqué dans 2.5 Navigation puis lancez la commande rosrun ros4pro navigate_waypoints.py. 🧳 Challenge additionnel : Carry my luggage Challenge inspiré de l'épreuve \"Carry my luggage\" de la RoboCup @Home. Pour info, le réglement de la compétition se trouve ici (mais ça n'apporte rien pour votre projet) : https://athome.robocup.org/wp-content/uploads/2019_rulebook.pdf 🗺️ Prérequis : avoir une carte représentative de l'environnement. ➡️ Phase 1 : Follow me Vous avez toute liberté pour préparer le début de l'épreuve (ex. comment faire que le robot soit bien localisé dès le début ?). Le robot part d'un point connu et doit suivre un humain qui va à un endroit inconnu par le robot (mais à l'intérieur de la carte). L'humain commence l'épreuve en étant en face du robot à une distance de 50 cm. Le robot doit suivre l'humain en maintenant une distance comprise entre 20cm minimum et 1m maximum. Pour être valide, l'humain doit avoir un déplacement non trivial : il ne va pas toujours tout droit et il fait varier sa vitesse de marche dans la limite du raisonnable. Distance minimum de marche demandée 4 mètres (mais vous êtes libres de faire plus si ça vous arrange, ça n'impactera pas directement la note). Il faut obligatoirement que le robot traverse une porte. Lorsque l'humain est arrivé à sa destination, il s'arrête pendant une durée d'au moins 3 secondes. Le robot doit alors comprendre que la phase 1 est terminée et passer à la phase 2. ↩️ Phase 2 : Go home Le robot doit repartir et naviguer en totale autonomie jusqu'à son point de départ. Sur le retour, vous rajouterez jusqu'à : 1 obstacle statique sur son chemin de retour 1 obstacle dynamique (typiquement un humain qui lui coupe la route) 1 obstacle qui bloque complètement le passage prévu par le robot (il faut qu'il ait la possiblité d'arriver à destination par un autre chemin) Si le robot arrive à destination (à +-20cm, +-15°) la phase 2 est validée. ↙️ Phase 3 : Dock Le robot doit chercher où se trouve sa base et s'y accoster. La position grossière de la base est connue mais cette partie n'est validée que si le robot réussi un accostage précis sans contact : la distance entre le robot et la base soit être supériere à 5mm et inférieure à 2cm. Vous avez toute liberté pour choisir un objet qui représentera la base du robot. Un pot de peinture par exemple serait un choix pertinent (la symétrie radiale peut simplifier la détection). Documentation FAQ des robots Documentation du TB3 (obsolète pour les commandes logicielles !) gmapping move_base 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"manipulation/edo/":{"url":"manipulation/edo/","title":"E.DO","keywords":"","body":"III. Robotique de manipulation avec E.DO La robotique de manipulation regroupe la manipulation d'objets avec des robots : des bras articulés à 5 ou 6 axes, les robots SCARA (Selective Compliance Assembly Robot Arm), les robots cartésiens (linéaires), les robots parallèles ... Dans ce TP nous utilisons un robot E.DO du fabriquant Comau. Prérequis Lycée et + Notions de commandes dans un terminal et d'adressage IP Le TP d'introduction Ce TP est compatible avec la simulation si vous n'avez pas d'E.DO : sauter directement au 2.3.bis Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Préparer le matériel 🔌 Votre E.DO comporte un connecteur Ethernet sur la base, c'est sur ce connecteur que vous pouvez connecter un câble réseau. Nous vous conseillons de brancher un câble RJ45 entre le robot et votre ordinateur pour commencer.Plus tard vous pourrez aussi vous connecter au point d'accès Wifi du robot pour communiquer en sans-fil. A l'intérieur de la base se trouve une Raspberry Pi et une carte SD préconfigurée par le fabriquant, elle est accessible en dévissant les trappes. Votre robot est donc compatible avec la plupart des procédures de la FAQ Raspberry Pi. ⚠️ Le robot peut être vendu avec plusieurs modèles d'effecteurs (= pince) ou bien sans effecteur du tout. Ce TP présuppose que vous avez la version avec la pince à 2 états : ouvert et fermé. 1.1 Changer l'adresse IP de l'ordinateur ROS E.DO est livré préconfiguré avec son propre réseau IP. Veuillez éditer la configuration réseau Ethernet (câblé) de votre ordinateur Ubuntu en utilisant le gestionnaire réseau (network manager, en haut à droite, vers l'horloge). Attribuez la configuration IP fixe suivante : Adresse IP statique 10.42.0.1 Masque de sous-réseau 24 ou bien 255.255.255.0 ⚠️ Attention, si vous souhaitez disposer aussi d'une connexion à Internet, cela ne focntionnera plus à cause du changement d'adresse IP. Vous pouvez cependant connecter votre ordinateur à un réseau Wifi pour disposer aussi d'un accès Internet. 1.2 Pinguer le robot Mettez votre robot sous-tension puis depuis un terminal Ubuntu tapez ping 10.42.0.49. Si tout va bien, un message apparaitra chaque seconde en indiquant le délai de communication avec le robot en millisecondes (ms), jsuqu'à ce que vous l'interompiez avec Ctrl + C : $ ping 10.42.0.49 PING 10.42.0.49 (10.42.0.49) 56(84) bytes of data. 64 bytes from 10.42.0.49: icmp_seq=1 ttl=114 time=3.7 ms 64 bytes from 10.42.0.49: icmp_seq=2 ttl=114 time=5.6 ms ^C Si un message d'erreur s'affichage à la place du délai en millisecondes, vous avez un problème réseau. Vérifiez l'étape 1.1 et que votre robot a bien démarré avec sa configuraiton réseau d'origine. Si vous voyez les délais en millisecondes, vous pouvez continuer. 1.3. Configurer l'environnement ROS du robot 💻 Avec SSH, connectez-vous à la Raspberry Pi du robot : ssh edo@10.42.0.49 Le mot de passe par défaut est raspberry. Si la connexion est un succès vous deviez voir un message d'information italien comandi tmux 🇮🇹 avec quelques explications à propos de rostopic, que nous devez connaître puisqu nous l'déjà abordée dans l'introduction. Si ce message n'apparait pas, vérifiez la configuration réseau du titre 1.1. 🤖 Avec la commande nano, éditez ensuite le script ministarter du robot via SSH : nano ~/ministarter 🤖 Descendez avec les flèches du clavier pour identifier ces deux lignes : export ROS_MASTER_URI=http://192.168.12.1:11311 export ROS_IP=192.168.12.1 puis remplacez-les par ces 2 lignes modifiées : export ROS_MASTER_URI=http://10.42.0.49:11311 export ROS_IP=10.42.0.49 Quittez en tapant Ctrl-X, puis y et Entrée pour valider le nom de fichier et l'enregistrer. 🤖 Tapez ensuite sudo reboot pour redémarrer le robot et attendez qu'il soit de nouveau prêt. ⚠️ Il n'est nécessaire de configurer l'environnement ROS qu'une seule fois sur chaque nouveau robot, car la nouvelle configuration est ensuite enregistrée définitivement sur la carte SD de la Raspberry Pi internet à E.DO. 1.4 Configurer l'environnement ROS de l'ordinateur 📀💻 Pour installer tous les packages ROS nécessaires sur votre ordinateur, exécutez les commandes suivantes : roscd && cd src git clone https://github.com/eDO-community/eDO_control_v3.git git clone https://github.com/eDO-community/eDO_moveit.git git clone https://github.com/eDO-community/eDO_description.git git clone https://github.com/eDO-community/eDO_core_msgs.git pip install getkey numpy cd .. catkin_make Un script de démarrage nommé start.bash configure les variables d'environnement our vous à chque fois que vous devrez travailler avec votre E.DO. Ce script ajoute un préfixe jaune devant l'invite de commande pour savoir quel est le ROS master actuellement sélectionné. 📀💻 Pour exécutez ce script tapez : roscd edo_control ./start.bash Vous devriez voir apparaître en préfixe le ROS master de votre E.DO, avant de taper toute autre commande ROS, comme ci-dessous. Essayez un rostopic echo pour vérifier si tout va bien : [http://10.42.0.49:11311] me@workstation :~$ rostopic echo /machine_state -n1 current_state: 0 opcode: 0 Les valeurs current_state: 0 et opcode: 0 indiquent l'état actuel du robot, dans un topic ROS dédié nommé /machine_state. Si vous ne voyez pas ces deux valeurs en tapant la commande, vous pourriez avoir un problème de réseau ou de configuration ROS. 2. Travaux pratique 2.1. Calibrer le robot Pourquoi calibrer ? Chacun des joints (axes moteurs) de votre robot possède un encodeur : un capteur permettant au moteur de déterminer sa position angulaire courante (par exemple tourné à 90° ou à 150°, etc). Plusieurs technologies d'encodeurs existent ayant chacune des avantages et inconvénients. Un des inconvénients des encodeurs d'E.DO est qu'ils ne peuvent mesurer que des déplacements angulaires relatifs à leur angle de démarrage, mais ne savent pas où est l'angle 0°. Vous ne pouvez pas commander un moteur d'aller en position 90° s'il ne sait pas où et le 0°. L'étape de calibration sert à indique au robot où sont les angles de référence 0° de chacun de ses moteurs, un par un. Quand calibrer ? Vous devrez calibrer votre robot après chaque cycle de marche-arrêt. La raison pour cela est que seule une partie des moteurs possèdent des freins, pour éviter q'ils se décalibrent en étant à l'arrêt. En effet, si vos moteurs sont à l'arrêt et sans frein les empêchant de tourner, la gravité ou une action humaine pourrait les faire tourner malgré eux, sans qu'ils puissent enregistrer ces rotations puisqu'ils ne sont pas sous tension. Ils perdraient ainsi la trace de leur angle 0°. Il faut donc calibrer à chaque démarrage, ou alors s'assurer qu'aucun des moteurs ne bouge pendant que le robot n'est pas sous tension, ce qui est difficile à s'assurer sans frein. Comment calibrer ? E.DO est livré avec une application Android, permettant nottamment de le calibrer. Le constructeur fournit une application Android permettant entre autre de calibrer le robot. Vous pourriez utiliser cette application mais puisque nous sommes sur un TP ROS, nous allons le faire avec ROS. 💻 Démarrez la procédure de calibration en tapant : roslaunch edo_control calibrate.launch Vous devirez d'avord voir un message d'avertissement JOINT_UNCALIBRATED qui indique que les joints ne sont effectivement pas calibrés, et qui détaille la procédure de calibration en anglais. Suivez cette procédure jusqu'au bout. A chaque fois que vous voyez Calibrating joint X cela signifie que la procédure va calibrer le joint X. Pour chaque joint appuyez sur les touches gauche et droite pour aligner physiquement les marqueurs d'alignement de chaque moteur. Une fois que votre joint est calibré appuyez sur Entrée pour passer au suivant. ℹ️ La position 0° calibrée de chaque joint doit conduire petit-à-petit votre robot à se tenir droit comme un i. Si à la fin de la calibration votre robot n'est pas droit comme un i pointant vers le plafond, vous vous êtes trompé sur l'alignement d'un ou plusieurs moteurs, vous pouvez relancer la procédure. La commande de calibration se ferme d'elle-même lorsque tous les joints ont été calibrés. 2.2. Démarrer le contrôle du robot et ouvrir/fermer la pince 💻 Lancez le launchfile de contrôle du robot sur votre ordinateur : roslaunch edo_control control.launch Cela va activer les interfaces de communication suivant avec le robot : Le topic /joint_states, qui affiche tout l'état des joints à environ 90 Hz : positions angulaires, vitesses et torques Le serveur d'action /follow_joint_trajectory qui permettra d'exécuter des trajectoires avec MoveIt un peu plus tard dans le TP Le topic /open_gripper qui permet d'ouvrir et de ferme la pince Tester la commande de la pince : Sur ce dernier topic, vous pouvez publier true pour ouvrir la pince (course maximale de 60 mm) et false pour la fermer (course minimale de 0 mm). 💻 Conservez le contrôle du robot démarré dans un terminal. Dans un autre terminal testez l'ouverture de la pince avec rostopic : rostopic pub /open_gripper std_msgs/Bool \"data: true\" Remplacez true par false pour refermer la pince ! 2.3. Démarrer MoveIt pour planifier des trajectoires (avec un robot réel) 💻 Conservez le contrôle du robot démarré dans un terminal. Tapez la commande suivante pour démarrer MoveIt avec E.DO : roslaunch edo_moveit demo.launch Vous devriez voir l'interface de MoveIt qui démarre dans RViz similairement à la capture d'écran ci-dessous : Utilisez ensuite les outils MoveIt pour planifier et exécuter des trajectoires sur votre robot : Dans la zone Motion Planning en bas à gauche de RViz, sélectionnez l'onglet Planning Dans la vue 3D du robot, bougez la balle bleue correspondant à l'effecteur (la pince) quelque part dans l'espace autour du robot Le robot orange correspond à la configuration cible que vous allez demander d'atteindre à votre robot Cliquez sur Plan and Execute pour planifier une trajectoire vers cette cible et l'exécuter sur le robot réel Si vous ne voyez pas votre vrai robot bouger, vérifier dans le terminal à partir duquel vous avez démarré MoveIt : il se peut que des messages d'erreurs apparaissent en rouge pour vous aider à localiser le problème. Ce TP s'arrête ici mais il est possible d'ajouter des éléments de collision (obstacles) que le planificateur de trajectoire contournera. Pour ce faire vous pouvez charger un fichier comprenant les coordonnées des obstacles dans l'onglet Scene Objects de la zone Motion Planning ou bien pour le faire via Python vous pouvez vous inspirer de la partie de déclaration des obstacles de cet autre TP. Que faire si les freins des moteurs s'activent aléatoirement pendant le mouvement Si vos trajectoires sont interrompues par des activations intempestives des freins (suivies d'une désactivation), vérifiez d'abord que votre robot a été calibré avec précision. Si c'est le cas, cela signifie que votre robot subit trop de force pour exécuter le mouvement que vous lui demander. Le système de sécurité active donc les freins. Ralentissez les vitesses des joints dans kinematics.yaml ou bien alléger le poids de votre robot ou bien demandez une position cible qui fera moins forcer les moteurs. 2.3.bis. Démarrer MoveIt pour planifier des trajectoires (avec un robot simulé) 📀💻 Pour installer tous les packages ROS nécessaires sur votre ordinateur et travailler avec un robot simulé, il ne faut pas avoir configuré l'environnement décrit en 1.4, exécutez directement les commandes suivantes : roscd && cd src git clone https://github.com/eDO-community/eDO_control_v3.git git clone https://github.com/eDO-community/eDO_moveit.git git clone https://github.com/eDO-community/eDO_description.git git clone https://github.com/eDO-community/eDO_core_msgs.git roscd && catkin_make source ~/.bashrc 💻 Enfin, cette commande ci-dessous lancera MoveIt avec un E.DO simulé : roslaunch edo_moveit demo.launch simulated:=true Vous devriez voir l'interface de MoveIt qui démarre dans RViz similairement à la capture d'écran ci-dessous : Utilisez ensuite les outils MoveIt pour planifier et exécuter des trajectoires sur votre robot simulé : Dans la zone Motion Planning en bas à gauche de RViz, sélectionnez l'onglet Planning Dans la vue 3D du robot, bougez la balle bleue correspondant à l'effecteur (la pince) quelque part dans l'espace autour du robot Le robot orange correspond à la configuration cible que vous allez demander d'atteindre à votre robot Cliquez sur Plan and Execute pour planifier une trajectoire vers cette cible et l'exécuter sur le robot simulé Ce TP s'arrête ici mais il est possible d'ajouter des éléments de collision (obstacles) que le planificateur de trajectoire contournera. Pour ce faire vous pouvez charger un fichier comprenant les coordonnées des obstacles dans l'onglet Scene Objects de la zone Motion Planning ou bien pour le faire via Python vous pouvez vous inspirer de la partie de déclaration des obstacles de cet autre TP. 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"manipulation/ergo-jr/":{"url":"manipulation/ergo-jr/","title":"Poppy Ergo Jr","keywords":"","body":"III. Robotique de manipulation avec Poppy Ergo Jr La robotique de manipulation regroupe la manipulation d'objets avec des robots. Dans ce TP nous utilisons un robot opensource Poppy Ergo Jr qui peut être 100% imprimé en 3D à la maison ou à l'école. Prérequis Lycée et + Notions de commandes dans un terminal et d'adressage IP Notions de Python Notions de géométrie 3D Le TP d'introduction Ce TP est compatible avec la simulation si vous n'avez pas de Poppy Ergo Jr : sauter directement au 2.3.bis Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Préparer le matériel (avec un robot réel) 1.1. Préparer la carte SD 📥 Pour éviter tout problème lié à une précédente utilisation du robot, commencez par flasher la carte SD fournie avec l'image ROS en utilisant la procédure vue lors de l'introduction. Pendant cette étape, assemblez votre robot en parrallèle. 1.2. Assembler Poppy Ergo Jr 🔧 Pour assembler votre robot, veuillez suivre le guide d'assemblage, en suivant les étapes faîtes pour ROS le cas échéant ; et en comparant minutieusement chaque pièce aux photos pour vérifier leur orientation car il est très facile d'assembler ce robot à l'envers même s'il a au final la même allure. Si votre robot est pré-assemblé, recommencez à minima toutes les configurations des moteurs qui pourraient être incorrectes. ✅ Vérification : Pour vérifier que votre assemblage est correct, connectez-vous en SSH au robot (si ce n'est pas déjà fait) puis exécutez : ssh pi@poppy.local # password raspberrypi # Effacer éventuellement l'ancienne clé ECDSA si vous avez un message d'erreur roslaunch poppy_controllers control.launch Vous devriez voir apparaître Connection successful. Si l'erreur \"Connection to the robot can't be established\" est affichée, alors votre robot n'a pas été monté correctement. La suite de ce message d'erreur indique quel(s) moteur(s) pose(nt) problème pour vous aider à le résoudre. Fermez avec Ctrl+C puis utilisez de nouveau Poppy Configure si un moteur est mal configuré. Remarque : Si vos moteurs clignotent en rouge : votre code a créé une collision et ils se sont mis en alarme. Pour désactiver l'alarme il faut débrancher et rebrancher l'alimentation, ce qui fera aussi redémarrer le robot 2. Travaux pratiques 2.1. Comprendre la représentation d'un robot ROS Un robot intégré à ROS est composé d'au minimum : un descripteur URDF un contrôleur qui gère les E/S avec le robot 2.1.1. Comprendre le descripteur URDF 💻📀 Clonez le package ROS Poppy Ergo Jr Description sur votre PC, il contient le fichier de description URDF du robot : git clone https://github.com/poppy-project/poppy_ergo_jr_description.git 💻 Compilez votre workspace puis sourcez votre .bashrc, enfin rdv dans le dossier urdf de ce package, puis exécutez la commande urdf_to_graphiz qui convertit un fichier URDF en représentation graphique dans un PDF : sudo apt install liburdfdom-tools roscd poppy_ergo_jr_description/urdf urdf_to_graphiz poppy_ergo_jr.urdf Ouvrez le PDF obtenu puis déterminez : Que représentent les rectangles ? Que représentent les bulles ? Que représentent les flèches et surtout les valeurs xyz et rpy associées ? 2.1.2. Comprendre les E/S du contrôleur 🤖 Le contrôleur se trouve déjà sur le robot. Vous pouvez directement vous connecter au robot et le démarrer : ssh pi@poppy.local # password raspberrypi # Effacer éventuellement l'ancienne clé ECDSA si vous avez un message d'erreur roslaunch poppy_controllers control.launch 💻 Sur votre PC, faîtes pointer votre ROS_MASTER_URI sur poppy.local. Rappel : nano ~/.bashrc # Pour changer votre ROS_MASTER_URI source ~/.bashrc # Pour charger votre .bashrc et donc le nouveau master 2.1.2.a. Topics du robot ✍ Avec l'utilitaire rostopic, lister les topics disponibles puis consultez celui qui décrit l'état courant des joints, en particulier : Quel est son nom ? Quel est le type de message qu'il transmet ? A quelle fréquence (en Hertz) est-ce qu'il met à jour l'état des joints ? 2.1.2.b. Services du robot ✍ Avec les utilitaires rosservice et rossrv, listez les services disponibles puis consultez celui qui met le robot en mode compliant. En particulier : Quel est le nom de topic du service mettant le robot en compliant ? Quel est le type de ce service ? Consultez le détail des champs. Quels sont les champs de la requête de ce service ? Consultez le détail des champs. Quels sont les champs de la réponse de ce service ? Appelez ce service pour activer et désactiver le mode compliant et essayez de faire bouger votre robot à la main à chaque fois. Que déduisez-vous de la signification du mode compliant ? Conseil : aidez-vous de l'autocomplétion avec la touche 2.1.2.c. Tracer la courbe des positions des moteurs en temps réel Mettez votre robot en mode compliant, démarrez rqt_plot pour tracer les positions des 6 moteurs ... bougez les moteurs à la main et vérifiez que rqt_plot actualise la courbe en temps réel. 2.2. Cinématique, et planification avec MoveIt dans RViz 2.2.1. Démarrer avec MoveIt 💻📀 Installez MoveIt puis clonez le package ROS Poppy Ergo Jr MoveIt Configuration, il contient le code nécessaire pour que ce robot fonctionne avec MoveIt : sudo apt install ros-melodic-moveit git clone https://github.com/poppy-project/poppy_ergo_jr_moveit_config.git 💻 Compilez votre workspace puis sourcez votre .bashrc. Démarrez MoveIt avec roslaunch avec le paramètre fake_execution à false pour se connecter au vrai robot : roslaunch poppy_ergo_jr_moveit_config demo.launch fake_execution:=false gripper:=true Rviz doit démarrer avec un Poppy Ergo Jr en visu. Note : si vous devez passer en simulation à ce moment suite à un défaut matériel, pensez à changer votre ROS_MASTER_URI pour localhost puis mettre simplement fake_execution à true. 2.2.2. Planification 💻 Dans l'onglet Planning, section Query puis Planning group, sélectionnez le groupe arm_and_finger, bougez le goal (la sphère 3D bleue) en position et en orientation puis cliquez sur Plan. ✍ Trois représentations 3D de robots se superposent, déterminez le rôle de chacun d'entre eux en testant également la fonctionnalité Plan and Execute : Que désigne le robot gris parfois mobile mais lent ? Que désigne le robot orange (fixe) ? Que désigne le robot gris qui répète infiniment un mouvement rapide ? Dans RViz, activer l'affichage du modèle de collision dans Displays, Scene Robot, Show Robot Collision, quelle est la forme de ce modèle utilisé par OMPL pour simplifier le calcul des collisions ? 2.2.3. Planning groups 💻✍ Testez également le groupe arm en plus du premier arm_and_finger et lancez des planifications de mouvement pour tester : Quelle est la différence entre ces 2 groupes ? Quel est le groupe pour lequel le goal est le plus facilement manipulable ? Pourquoi ce groupe est-il plus facilement manipulable que l'autre ? Déduisez-en ce que désigne exactement un planning group 2.2.4. Transformations tf Nous allons visualiser et interroger l'arbre des transformations nommé tf 💻✍ Démarrer MoveIt puis dans un autre terminal lancer rosrun tf2_tools view_frames.py. Un fichier PDF nommé frames.pdf a été créé : les frames (repères géométriques) qu'ils contient sont les mêmes que ceux dessinés par Rviz en rouge-vert-bleu. Comment est nommé le repère de base ? Comment sont nommés les deux effecteurs finaux possibles ? La commande rosrun tf2_tools echo.py frameA frameB renvoie la transformation actuelle de frameB dans frameA. Modifiez cette commande pour déterminer quelle est la position actuelle d'un des effecteurs dans le repère de base. Ses coordonnées peuvent vous servir par la suite, pour les définir comme cible à atteindre. 2.3. Ecrire un noeud Python ROS pour l'Ergo Jr 2.3.1. Créer un nouveau package et un nouveau noeud Python 💻 Nous allons créer un nouveau package ROS nommé ros4pro_custom sur votre laptop de développement, qui contient notre code: cd ~/catkin_ws/src catkin_create_pkg ros4pro_custom # Cette commande créé le package mkdir -p ros4pro_custom/src # On créé un dossier src dans le package touch ros4pro_custom/src/manipulate.py # On créé un noeud Python \"manipulate.py\" chmod +x ros4pro_custom/src/manipulate.py # On rend ce noeud exécutable pour pouvoir le lancer avec rosrun 💻🐍 Bien que vous devriez avoir compris comment créer un noeud ROS en Python dans les tutoriels d'introduction, voici un rappel de noeud ROS minimal qui boucle toutes les secondes en Python : #!/usr/bin/env python import rospy rospy.init_node('ros4pro_custom_node') rate = rospy.Rate(1) while not rospy.is_shutdown(): rospy.loginfo(\"Hello world from our new node!\") rate.sleep() 💻 Compilez votre workspace puis sourcez votre .bashrc. Exécutez votre noeud avec rosrun : cd ~/ros_ws catkin_make rosrun ros4pro_custom manipulate.py Votre noeud doit afficher un message toutes les secondes, vous pouvez le tuer avec Ctrl+C. Nous allons ajouter du code petit à petit. Attention à l'ajouter au bon endroit pour créer un script cohérent. 2.3.2. Planifier et exécuter des mouvements avec MoveIt Le MoveGroupCommander est le commandeur de robot de MoveIt, il suffit de lui indiquer quel est le nom du groupe à commander puis donner une cible et appeler la fonction go() pour l'atteindre en évitant les obstacles. Cette cible peut être dans l'espace cartésien ou dans l'espace des joints : 2.3.2.a. 🐍 Cible dans l'espace cartésien from moveit_commander.move_group import MoveGroupCommander commander = MoveGroupCommander(\"arm_and_finger\") commander.set_pose_target([0.00, 0.079, 0.220] + [0.871, -0.014, 0.079, 0.484]) commander.go() Les coordonnées cartésiennes de la cible sont les coordonnées de l'effecteur (càd moving_tip pour le groupe arm_and_finger ou bien fixed_tip pour le groupe arm) dans le repère base_link, exprimées sous la forme x, y, z, qx, qy, qz, qw. 2.3.2.b. 🐍 Cible dans l'espace des joints (sans évitement de collision) Il est également possible de définir une cible dans l'espace des joints en fournissant une liste des 6 angles moteurs dans ce cas il n'y a pas d'évitement de collision: commander.set_joint_value_target([0, 0, 0, 0, 0, 0]) commander.go() 2.3.2.c. ✍ Mise en pratique A l'aide des fonctions et commandes vues en 3.1.4. et 4.2.1., vérifiez que vous savez prendre les coordonnées cartésiennes courante et les définir comme cible puis l'atteindre A l'aide des fonctions et commandes vues en 2.2.1. et 4.2.2., vérifiez que vous savez prendre les positions des joints courantes et les définir comme cible puis l'atteindre A l'aide du mode compliant, prendre les coordonnées cartésiennes de l'effecteur et et les positions des joints pour deux configurations différentes du robot A et B (e.g. effecteur vers le haut et effecteur vers le bas) Faîtes bouger le robot infiniement entre les cibles cartésiennes A et B, nous y ajouterons des obstacles plus tard 2.3.3. Déclarer des obstacles Afin que les algorithmes de planification de trajectoire d'OMPL (tels que RRTConnect) puissent éviter les obstacles, il est nécessaire que MoveIt ait connaissance de leur position et leur forme. Il est possible d'utiliser une caméra de profondeur (aka caméra RGB-D, mais nous n'en avons pas ici) ou bien déclarer les objets depuis le code Python grâce à l'interface PlanningSceneInterface. 🐍 Par exemple, ce code déclarer une boite de céréales comme objet de collision en spécifiant sa position et son orientation sous forme d'objet PosteStamped ainsi que sa taille en mètres : from geometry_msgs.msg import PoseStamped from moveit_commander.planning_scene_interface import PlanningSceneInterface scene = PlanningSceneInterface() rospy.sleep(1) ps = PoseStamped() ps.header.frame_id = \"base_link\" ps.pose.position.x = 0.15 ps.pose.position.y = 0 ps.pose.position.z = 0.15 ps.pose.orientation.w = 1 scene.add_box(\"boite_de_cereales\", ps, (0.08, 0.24, 0.3)) rospy.sleep(1) Les coordonnées des objets de collision sont données sous la forme d'objet PoseStamped incluant la position, l'orientation et le repère frame_id, et la taille est donnée sous forme de tuple (longueur, largeur, hauteur). Modifier l'obstacle \"boite_de_cereales\" proposé en exemple afin qu'un obstacle viennent perturber le mouvement entre les deux poses de votre programme en 3.2.2. et vérifiez que MoveIt contourne toujours ces obstacles sans jamais les collisionner. Note: Accessoirement, il est possible d'attacher et de détacher les objets de collision au robot, ceci permet par exemple de simuler la saisie et la dépose d'objets physique dans RViz avec MoveIt. cf la documentation MoveIt pour Python ou même le code de PlanningSceneInterface 2.3.4. Enregistrer et rejouer un mouvement de pick-and-place Référez-vous à la documentation du Poppy Controllers afin d'enregistrer et de rejouer des mouvements en utilisant la compliance du robot. Faîtes quelques essais avec plusieurs mouvements qui s'alternent pour bien comprendre le fonctionnement. Enregistrez un mouvement de pick-and-place pour attraper un cube et le déposer à un autre endroit 2.4. Récupérer les images de la caméra en Python 💻📀 Avec la carte SD ROS, l'image de la caméra est accessible par appel d'un service dédié. Nous aurons besoin de récupérer le package Poppy Controllers et le compiler d'abord : cd ~/ros_ws/src git clone https://github.com/poppy-project/poppy_controllers.git # Nous aurons besoin de ce package cd ~/ros_ws/ catkin_make source ~/.bashrc 🐍 Testez ce code pour vérifier que vous pouvez récupérer l'image en Python via le service ROS /get_image fourni par le contrôleur. import cv2 from poppy_controllers.srv import GetImage from cv_bridge import CvBridge get_image = rospy.ServiceProxy(\"get_image\", GetImage) response = get_image() bridge = CvBridge() image = bridge.imgmsg_to_cv2(response.image) cv2.imshow(\"Poppy camera\", image) cv2.waitKey(200) Cette image peut ensuite être traitée par un réseau de neurones, une fonction OpenCV, etc ... Documentation Tutoriaux de MoveIt Code du MoveIt Commander Python Documentation de l’API MoveIt en Python Documentation de Poppy Ergo Jr 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"manipulation/sawyer/":{"url":"manipulation/sawyer/","title":"Sawyer","keywords":"","body":"III. Robotique de manipulation avec Sawyer La robotique de manipulation regroupe la manipulation d'objets avec des robots : des bras articulés à 5 ou 6 axes, les robots SCARA (Selective Compliance Assembly Robot Arm), les robots cartésiens (linéaires), les robots parallèles ... Dans ce TP nous utilisons un robot Sawyer du fabriquant Rethink Robotics. Prérequis BAC+2 et + Aisance en Python et commandes dans un terminal Aisance en géométrie 3D Le TP d'introduction Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Documentation 1.1. Les liens Tutoriaux de MoveIt Code du MoveIt Commander Python Documentation de l’API MoveIt en Python Tutoriaux du SDK Sawyer 1.2. Le plus important Voici les quelques fonctions les plus importantes traduites depuis la documentation. 1.2.1. Déclarer un commandeur de robot commander = MoveGroupCommander(\"right_arm\") Ce commandeur est celui qui identifie notre robot et permettra d'en prendre le contrôle. 1.2.2. Exécuter un mouvement vers une cible Définir une cible, dans l'espace des joints : commander.set_joint_value_target([-1.0, -2.0, 2.8, 1.5, 0.1, -0.3, 3.0]) Les 7 valeurs sont les angles cibles des 7 joints en radians Attention : les cibles dans l'espace des joints n'auront pas d'évitement de collisions Ou bien définir une cible dans l'espace cartésien : commander.set_pose_target([0.5, 0.05, 1.1, 0, 0, 0, 1]) Les 7 valeurs sont la position et l'orientation [x, y, z, qx, qy, qz, qw] cible de l'effecteur dans le repère base 1.2.3. Planifier & exécuter le mouvement vers la cible La fonction \"go\" déclenche le calcul de trajectoire et l'exécution instantanée si le calcul a réussi : commander.go() 1.2.4. Exécuter une trajectoire cartésienne Par opposition à la cible cartésienne, dans cet exemple au lieu de ne définir qu'une cible finale, on demande à MoveIt de suivre une trajectoire rectiligne dans l'espace cartésien. Cette trajectoire est précalculée en entier grâce à la fonction commander.compute_cartesian_path([pose1, pose2]), resolution, jump) où : [pose1, pose2] est la liste des points à suivre de manière rectiligne de type geometry_msgs.Pose resolution est la distance en mètre entre 2 points cartésiens générés (par exemple 0.01 pour générer un point tous les centimètres) jump est le seuil maximal autorisé au delà duquel un saut trop important entre 2 positions angulaires en radians ne sera pas exécutée car il demanderait une vitesse excessive. jump est la somme des seuils sur tous les joints (par exemple 3.14). La fonction commander.compute_cartesian_path(...) renvoie : trajectory: la trajectoire cartésienne calculée ratio: Un ratio entre 0 et 1 indique la quantité de la trajectoire qui a pu être calculée avec succès sans générer de saut. Un ratio inférieur à 0.95 signifie probablement que la trajectoire est inutilisble car elle ne suit pas le chemin demandé. Par exemple, étant données 2 points p1 et p2 de type geometry_msgs.Pose, cet appel est valide : trajectory, ratio = commander.compute_cartesian_path([pose1, pose2]), 0.01, 3.14) Enfin, exécuter la trajectoire, uniquement si le ratio indique au moins 95% de succès : commander.execute(trajectory) 1.2.5. Définir des objets de collision Lorsqu'on ajoute des objets de collision, les appels à go() dans l'espace cartésien planifieront, si possible, une trajectoire en évitant les objets déclarés comme objets de collision. La scène de planification est notre interface pour ajouter ou supprimer des objets de collision : scene = PlanningSceneInterface() On peut ensuite effectuer les ajouts ou suppressions. Par exemple, on ajoute un objet de collision cubique de taille 10x8x2 cm à la position [1.2, 0.5, 0.55] et avec l'orientation [0, 0, 0, 1] (= rotation identité) dans le repère base: ps = PoseStamped() ps.header.frame_id = \"base\" ps.pose.position.x = 1.2 ps.pose.position.y = 0.5 ps.pose.position.z = 0.55 ps.pose.orientation.x = 0 ps.pose.orientation.y = 0 ps.pose.orientation.z = 0 ps.pose.orientation.w = 1 scene.add_box(\"ma_boite\", list_to_pose_stamped2([[1.2, 0.5, 0.55], [0, 0, 0, 1]]), (0.10, 0.08, 0.02)) Les objets de collision apparaissent en vert dans RViz s'ils sont définis correctement. Note: après une modification de la scène, est généralement utile de faire une pause rospy.sleep(1) afin de laisser le temps à la scène d'être mise à jour sur tout le système avant toute nouvelle planification de trajectoire 2. Travaux pratiques 2.1. Utiliser MoveIt dans le visualisateur Rviz Avec roslaunch, lancer sawyer_moveit.launch provenant du package sawyer_moveit_config: roslaunch sawyer_moveit_config sawyer_moveit.launch Via l’interface graphique, changer l’orientation et la position de l’effecteur puis demander à MoveIt de planifier et exécuter une trajectoire pour l’atteindre. Cochez la bonne réponse : Cette méthode permet-elle de définir une cible dans l’espace : ◻ cartésien ◻ des joints Trois robots semblent superposés en 3D, quelles sont leurs différences : Le robot orange est ... ? Le robot rapide est ... ? Le robot lent est ... ? Utilisez rostopic echo pour afficher en temps réel les messages du topic /robot/joint_states. Exécutez un mouvement et observer les valeurs changer. Que représente le topic /robot/joint_states ? Indiquez comment se nomment les 7 joints de Sawyer depuis la base jusqu’à l’effecteur : Premier joint : Deuxième joint : Troisième joint : Quatrième joint : Cinquième joint : Sixième joint : Dernier joint : 2.2. Utiliser MoveIt via son client Python Dans le package ros4pro, ouvrir le nœud manipulate.py. Repérez les 3 exemples : d'exécution d'une trajectoire cartésienne de planification vers une cible cartésienne de planification vers une cible dans l'espace des joints Durant la suite du TP, nous démarrerons notre programme de manipulation avec le launchfile manipulate.launch du package ros4pro qui fonctionne par défaut en mode simulé, à savoir : roslaunch ros4pro manipulate.launch Celui-ci démarre automatiquement manipulate.py, il est donc inutile de le démarrer par un autre moyen. 2.2.1. Modifier la scène de planification La scène représente tout ce qui rentre en compte dans les mouvements du robot et qui n’est pas le robot lui-même : les obstacles et/ou les objets à attraper. Ces éléments sont déclarés à MoveIt comme des objets de collision (détachés du robot ou attachés c’est-à-dire qu’ils bougent avec lui). Prenez les mesures du feeder puis déclarez-les comme objets de collision dans votre noeud Python via l’interface PlanningSceneInterface. Complétez le TODO associé à la question 3.2.1. dans manipulate.py. Planifiez et exécutez un mouvement RViz pour vérifier que les collisions sont évitées. Déclarez un nouvel obstacle qui entrave forcément le chemin du robot et vérifiez. Vérifiez ce qu’il se passe lorsque le planner ne trouve aucune solution. 2.2.2. Effectuer un pick-and-place avec un cube simulé (optionnel) Nous considérons un cube situé à la pose ᵇᵃˢᵉP꜀ᵤ₆ₑ ᵇᵃˢᵉP꜀ᵤ₆ₑ_ᵥₑᵣₜ = [[0.32, 0.52, 0.32], [1, 0, 0, 0]], ce qui correspond exactement à l'emplacement entouré en vert sur le feeder. Pour l’approche, on positionnera le gripper 18cm au dessus du cube le long de son axe z. Sachant cela, déduire la matrice de transformation ᶜᵘᵇᵉP₉ᵣᵢₚₚₑᵣ en notation [[x, y, z], [x, y, z, w]] ? Exprimez ᵇᵃˢᵉP₉ᵣᵢₚₚₑᵣ en fonction de la pose ᵇᵃˢᵉP꜀ᵤ₆ₑ du cube dans le repère base via une multiplication matricielle Ensuite, dans votre code : Inventez un cube en simulation dans votre code à la pose ᵇᵃˢᵉP꜀ᵤ₆ₑ_ᵥₑᵣₜ = [[0.32, 0.52, 0.32], [1, 0, 0, 0]] c’est-à-dire à la surface du feeder. Pour ce faire utilisez : l’interface PlanningSceneInterface pour ajouter, supprimer un cube ou l’attacher à l’effecteur right_gripper_tip du robot TransformBroadcaster pour publier la frame tf nommée cube au centre du cube à attraper 2.2.3. Générer les 4 trajectoires du pick-and-place Pour rappel, voici les 4 étapes d'un pick pour attraper et relacher le cube simulé : 1. trajectoire d’approche : aller sans collision à `ᵇᵃˢᵉP꜀ᵤ₆ₑ_ᵥₑᵣₜ` c'est à dire 18cm au dessus du cube sur son axe z (axe bleu) 2. trajectoire de descente : suivre une trajectoire cartésienne de 50 points descendant le long de l'axe z pour atteindre `ᵇᵃˢᵉP꜀ᵤ₆ₑ_ᵥₑᵣₜ` avec l’effecteur `right_gripper_tip`. Puis fermer l'effecteur. 3. trajectoire de retraite : retourner au point d’approche par une trajectoire cartésienne 4. trajectoire de dépose : si le cube a bel-et-bien été attrapé avec succès, aller sans collision au point de dépose `ᵇᵃˢᵉP꜀ₔₑₚₒₛₑ = [[0.5, 0, 0.1], [0.707, 0.707, 0, 0]]` Dans manipulate.py, les deux dernières trajectoires sont incomplètes : retraite et dépose nommées release et place. Compléter les 2 TODO associés à la question 3.2.3 dans manipulate.py Vérifier que votre pick-and-place a l'air correct en simulation d'abord et que vous distinguez correctement les 4 étapes du pick-and-place. 2.3. Exécutez le pick-and-place sur le Sawyer réel Changez votre rosmaster pour celui de Sawyer :export ROS_MASTER_URI=http://021608CP00013.local:11311 Le centre du cube ᵇᵃˢᵉP꜀ᵤ₆ₑ_ᵥₑᵣₜ précédent est celui de la zone verte sur le feeder. Vérifiez que votre pick-and-place fonctionne avec 1 seul cube à l’emplacement vert. Pour commander le robot réel modifiez le paramètre simulate : roslaunch ros4pro manipulate.launch simulate:=false Vous devriez constater que votre pick-and-place fonctionne parfois et qu'il échoue dans certaines situations. 2.4. Préparer le pipeline du scenario final (optionnel) Ajoutez à manipulate.py le code nécessaire pour votre scenario final : Prise de photo (scan) : positionner l’effecteur de telle manière que « right_hand_camera » se trouve à la verticale du feeder Récupérer l’image rectifiée sur le topic dédié : image_view peut aider à visualiser l’image pour déboguer dans un terminal pour récupérer l’image en Python, implémentez un Subscriber. Cette photo sera envoyée au réseau de neurones lorsqu’il sera implémenté Pick-and-Place successifs : Effectuez 3 pick-and-place successifs sans intervention humaine de 3 cubes dont la position est connue à l’avance. Ces positions seront retournées par le réseau de neurones lorsqu’il sera implémenté Si vous n’avez pas de collision de manière reproductible, vous pouvez accélérer les vitesses dans sawyer_moveit_config/config/joint_limits.yaml (pas les accélérations) 2.5. Prévenir les échecs de planification Le path-planning étant réalisé pendant l’exécution et non précalculé, il est possible que MoveIt ne trouve aucune solution de mouvement. Pour remédier à cela, plusieurs pistes s’offrent à nous : Réessayer encore et encore … bêtement, jusqu’à un potentiel succès Planifier toute les trajectoires d’un coup avant exécution, si l’une ne peut être calculée, regénérer la précédente et recommencer. Cela montre ses limites : et si le robot n’est pas à l’emplacement attendu entre deux trajectoires, par exemple si l’opérateur l’a bougé manuellement ? Fournir une « seed » à l’IK ou au path-planning. Cette solution est approfondie ci-après : MoveIt possède un système de calcul de la géométrie directe et inverse, respectivement via les services /compute_fk et /compute_ik. Observer les types de service (rosservice info) et le contenu de la requête (rossrv show) puis appeler ces deux services sur ces deux exemples : Calculer la FK pour les angles -π/2, π/2, -π/2, 0, 0, 0, 0. Donner le résultat au format [[x, y, z,], [qx, qy, qz, qw]] Calculer l’IK pour la position d’effecteur [[0.5, 0, 0.1], [0.707, 0.707, 0, 0]]. Donner le résultat au format [angle1, angle2, angle3, angle4, angle5, angle6, angle7] Réexécutez le même calcul d’IK sur la même position d’effecteur, une seconde puis une troisième fois. Observez que le résultat est différent. Pourquoi ? Fournissez une « seed » au format [angle1, angle2, angle3, angle4, angle5, angle6, angle7] de votre choix à l’IK pour influencer le résultat. Quelle seed proposez-vous pour maximiser les chances de succès du path-planning ? 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"perception/opencv/":{"url":"perception/opencv/","title":"OpenCV","keywords":"","body":" IV. Perception avec OpenCV Introduction à OpenCV Ouverture d'une image Seuil sur la couleur Détection des cubes Intégration avec ROS IV. Perception avec OpenCV Le domaine de \"Computer Vision\" (CV, ou vision par ordinateur) est une branche de l'intelligence artificielle, qui traite des techniques permettant d'extraire des informations de \"haut niveau\" utiles à partir d'images. Donc ce domaine développé depuis les années 60, on retrouve généralement des techniques provenant des mathématiques, du traitement d'images, des neurosciences, de l'apprentissage artificiel… Nous allons ici effleurer ce domaine en nous familiarisant avec OpenCV. Introduction à OpenCV OpenCV est une bibliothèque logicielle qui est devenue le \"standard\" du domaine. Cette bibliothèque fournit un énorme ensemble de fonctionnalités et d'algorithmes à la pointe de l'état de l'art. Entre autres sont disponibles: Des mécanismes d'entrées/sorties des images et flux vidéos (caméras, fichiers…) Des mécanismes de traitement d'images (gestion des formats, couleurs, déformations… ) Des milliers d'algorithmes développés par la communauté et les industriels (reconnaissance d'image, suivi d'objet, vision 3D, apprentissage…) Ouverture d'une image Téléchargez l'image: Créez un fichier couleurs.py import numpy as np import cv2 as cv img = cv.imread('ergo_cubes.jpg') Quelle information nous donne print(img.shape) ? On peut accéder à chaque pixel par indexation du tableau img avec img[LIGNE, COLONNE] (ce qui est très inefficace), que représente la valeurs données par img[170,255] ? Pour accéder au différents canaux de couleur on peut de même utiliser: img[:,:,CANAL] avec CANAL la couleur voulue. On peut facilement créer des régions d'intérêt (ROI) en utilisant les mécanismes disponibles dans python: roi=img[140:225, 210:310] OpenCV offre également quelques fonctionnalités pratiques d'interface utilisateur (GUI). Pour afficher une image: cv.imshow(\"Mon image\", roi) #on donne un nom unique à chaque fenêtre cv.waitKey(0) #permet d'attendre à a l'infini Enfin, on peut écrire les images dans des fichiers: cv.imwrite(\"roi.png\", roi) Affichez les trois canaux de couleur dans des fenêtres différentes Seuil sur la couleur Nous avons vu que les images sont généralement représentés dans l'espace BGR, ce qui est cohérent avec le fonctionnement du pixel de l'écran (et du capteur), mais moins évidant lorsque l'on souhaite travailler sur les couleurs. Comment par exemple définir le volume 3D dans l'espace BGR représentant le \"rose\"? C'est pourquoi pour traiter la couleur, il est recommandé de convertir l'encodage de l'image dans un autre espace. L'espace le plus couramment utilisé est le HSV (Hue, Saturation, Value ou Teinte, Saturation, Valeur). Pour convertir une image de BGR vers HSV il suffit d'utiliser: img_HSV = cv.cvtColor(img, cv.COLOR_BGR2HSV) On notera que l'espace HSV est encodé avec H dans [0, 179], S dans [0,255] et V dans [0,255] On peut ensuite appliquer un seuil avec: img_seuil = cv.inRange(img_HSV, (MIN_H, MIN_S, MIN_V), (MAX_H, MAX_S, MAX_V)) Le résultat de la fonction de seuil inRange est une image binaire Expérimentez avec les valeurs de seuil pour ne faire apparaître que le cube rouge Note: il est facile de créer des \"trackbars\" pour changer en temps réel les valeurs, voir le tutoriel Détection des cubes Nous sommes maintenant capable de sélectionner des pixels en fonction de leur couleur, il nous faut encore \"regrouper\" ces informations afin de détecter et reconnaître les cubes. Une méthode simple consiste à considérer que les pixels d'une couleur choisie font partie d'un \"blob\" (une région de pixels voisins) représentant le même objet. Dans l'image binaire résultat du seuil, il nous suffit de chercher le contour des zones blanches. Pour cela nous allons utiliser la fonction findContours() (voir le tutoriel) imgret, contours, hierarchy = cv.findContours( img_seuil, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE) imgret est la même image que img_seuil contours est une liste contenant tous les contours trouvés hierarchy contient les informations sur la hiérarchie des contours (les contours à l'intérieur des contours) Sur une image \"naturelle\" (avec du bruit) les contours trouvés seront rarement parfaits. Il est possible de \"filtrer\" ces contours en ne considérant par exemple que ceux dons la surface est cohérente avec les objets recherchés (voir le tutoriel) Parcourez la liste des contours et dessinez les contours dont la surface est comprise entre 2500 et 3500 On utilisera une boucle sur contours, la fonction contourArea() retournant la surface d'un contour, ainsi que la fonction de dessin drawContours() (dessinez sur l'image d'origine) Une fois le contour du cube trouvé, nous pouvons chercher son centre avec la fonction moments() avec une fonction telle que: def trouver_centroid(cnt): M = cv.moments(cnt) if M['m00'] > 0.0: cx = int(M['m10']/M['m00']) cy = int(M['m01']/M['m00']) return (x, y) else: return (0, 0) Nous pouvons ensuite utiliser la position obtenue pour écrire un texte: cv.putText(img, 'cube', (x, y), cv.FONT_HERSHEY_SIMPLEX, 1,(255, 255, 255), 1, cv.LINE_AA) Maintenant que nous sommes capable de détecter un cube d'une couleur, étendez le programme pour détecter la présence et la position des 3 cubes Intégration avec ROS Nous allons maintenant intégrer cette détection de cube coloré à ROS en lisant l'image de la caméra de Ergo Jr simulée par Gazebo. On peut visualiser les images avec l'outil rqt_image_view: rosrun rqt_image_view rqt_image_view Les images brutes sont publiées sur le topic: /ergo_jr/camera_ergo/image_raw Attrapez chacun des cubes et récupérez des images de la caméra qui vous servirons à vérifier le bon fonctionnement de votre programme précédant Dans votre package ROS créez le fichier ros4pro/src/vision.py ```python import rospy from sensor_msgs.msg import Image from std_srvs.srv import Trigger, TriggerResponse from cv_bridge import CvBridge import cv2 as cv import numpy as np class NodeVision(object): def __init__(self): # Params self.image = None self.debug_img = None self.br = CvBridge() #pour la conversion entre les imags OpenCV et les images ROS # Node cycle rate (in Hz). self.loop_rate = rospy.Rate(10) # Pour publier des images pour le debuggage self.img_pub = rospy.Publisher( '/ergo_jr/camera_ergo/debug_img', Image, queue_size=1) # Pour récupérer les images du robot simulé rospy.Subscriber( '/ergo_jr/camera_ergo/image_raw', Image, self.callback) # Créaction d'un service (on utilise le srv standard Trigger) self.service_vision = rospy.Service( '/ergo_jr/cube_detection', Trigger, self.handle_cube) def trouver_cube(self,img): raise NotImplementedError(\"Complétez la partie 2.4 avant d'exécuter\") # ICI le traitement OpenCV # retour du résultat resp = TriggerResponse() # Si pas de cube # resp.success = False # Sinon # resp.success = True # resp.message=\"COULEUR\" return resp def handle_cube(self, req): #Méthode callback qui sera éxécutée à chaque appel du service # retour du résultat resp = TriggerResponse() resp.success = False # uniquement si l'image existe if self.image is not None: imgtmp = self.image.copy() # on appelle la méthode de traitement d'image resp = self.trouver_cube(imgtmp) return resp def callback(self, msg): #méthode callback qui sera éxécutée à chaque reception d'un message self.image = self.br.imgmsg_to_cv2(msg, \"bgr8\") #On converti l'image ROS en une image OpenCV def start(self): rospy.loginfo(\"Démarage du node vision\") while not rospy.is_shutdown(): if self.image is not None: # éventuellement, publication d'une image de débug, ici une copie de l'image d'origine self.debug_img = self.image.copy() self.img_pub.publish( self.br.cv2_to_imgmsg(self.debug_img, \"bgr8\")) #On converti l'image OpenCV en une image ROS self.loop_rate.sleep() if __name__ == '__main__': rospy.init_node(\"Vision\") vision = NodeVision() vision.start() ``` À partir de ce squelette, intégrez votre programme de détection des cubes colorés On notera qu'il est nécessaire d'utiliser CvBridge() afin de faire le lien entre les images OpenCV et les images ROS. On peut appeler le service créé avec la commande: rosservice call /ergo_jr/cube_detection [TAB] Dans votre programme de mouvement, utilisez l'appel à ce service afin de détecter la couleur du cube attrapé et faites une pile de cube Rouge/Vert/Bleu Modifiez les couleurs dans le fichier launch spawn_cubes.launch pour tester différentes combinaisons 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"perception/keras/":{"url":"perception/keras/","title":"Keras","keywords":"","body":"IV. Perception avec Keras Keras est un système d'apprentissage automatique utilisant des réseaux de neurones. Nous allons l'utiliser ici sur des imagettes sur lesquelles sont inscrites des chiffres marqués manuellement au feutre avec différentes calligraphies. Le réseau de neurones que vous allez créer devra apprendre lui-même à déterminer quel chiffre est marqué, ce que l'on appelle classifier. Prérequis BAC+2 et + Bonne compréhension de Python et numpy Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 0. Installation Le code source se trouve à cet emplacement. Pour effectuer cet atelier, vous devez installer quelques packages: pip install tensorflow keras imageio matplotlib scikit-image numpy git clone https://gitlab.inria.fr/apere/ros_workshop.git 1. Documentation Pour manipuler le code, vous aurez besoin de consulter plusieurs documentations. De manière générale, vous aurez besoin de numpy. Si vous n'êtes pas familier avec, vous pouvez consulter: Numpy cheatsheet Numpy Documentation Pour la partie détection et pré-processing, nous utiliserons scikit-image: Scikit-Image Documentation Enfin, pour la partie reconnaissance, nous utiliserons keras. Vous devriez trouver ce qu'il vous faut dans le tutoriel suivant: Keras Documentation 2. Partie apprentissage Commencez par ouvrir le fichier learning.py. 2.1 Chargement des données Prenez connaissance du code, puis lancez le en exécutant à la ligne de commande: python learning.py Avant d'appuyer sur entrée, répondez aux questions suivantes: Que contiennent les variables x_train et y_train? Pourquoi la fonction load_data renvoie-t-elle également les variables x_test et y_test? Quelles sont les formes respectives de x_train et y_train? 2.2 Prévisualisation des données brutes Appuyez sur entrée pour continuer, et observez les images. Répondez aux questions suivantes: Quelle sont les valeurs des pixels blancs (représentés en jaune) et des pixels noirs ? Observez bien les données et leurs labels. Toutes les images sont elles simples à classifier correctement? Ferriez vous des erreurs en les classifiants ? 2.3 Préparation des données Fermez la fenêtre et appuyez à nouveau sur entrée. Répondez aux questions suivantes: Quelle sont les formes respectives de x_train et y_train maintenant? Pourquoi ces changements ? 2.4 Prévisulation des données préparées Appuyez à nouveau sur entrée et observez les images: Quelles sont les valeurs des pixels blanc et des pixels noirs maintenant? Regardez la fonction prepare_input. Quelle transformation des images est effectuée? 2.5 Le modêle Arrêtez le script en appuyant sur ctrl+c. Dans le fichier, modifiez la fonction build_model pour implémenter le réseau LeNet vu pendant la présentation. Une fois cela fait, relancez le script et faites défiler jusqu'à la partie 2.5. Observez le résumé du modêle: Observez le nombre de paramêtres par couche. Quelles sont les couches qui ont le plus grand nombre de paramêtre? Qu'en concluez vous sur l'utilité des couches par convolution ? 2.6 La fonction de cout et l'optimiseur Arrêtez le script en appuyant sur ctrl+c. Durant la présentation, nous avons vu que deux fonctions de coût sont régulièrement utilisées dans l'entrainement des réseaux de neurones: L'érreur quadratique moyenne (Mean squared error) L'entropie croisée (Cross entropy) Dans le fichier, modifiez la fonction get_loss pour implémenter la fonction de coût adaptée à notre problême. Pendant la présentation, nous avons vu que l'optimiseur est l'algorithme qui permet de se déplacer sur la surface déssinée par la fonction de cout dans l'espace des paramêtres. Cet algorithme permet de chercher l'endroit ou la fonction de cout est minimale. Un des algorithmes les plus simples s'appelle la descente de gradient (GD) et consiste à se déplacer dans le sens de la pente la plus forte à chaque pas de temps: Dans quelle hypothèse cet algorithme permet il de trouver le minimum global de la fonction de coût selon vous ? Pensez vous que cette hypothèse soit vérifiée pour les réseaux de neurones ? Que se passe-t-il si cette hypothèse n'est pas vérifiée ? Adam est un optimiseur plus complexe que GD. Sur l'image suivante, on voit plusieurs optimiseurs se déplacer sur une fonction de cout. Concentrez-vous sur Adam et GD: Quelle semble être la caractéristique de Adam comparée a GD? Une autre caracteristique de l'algorithme GD, est que la taille du pas qui est effectué à chaque itération est fixe. L'image suivante montre Adam et GD dans un cas ou la pente devient trés forte. Repondez aux questions suivantes: GD arrive-t-il à converger ? Comprenez vous pourquoi ? Adam ne semble pas soumis au même problême que GD ? Quelle autre caractéristique de Adam cela montre t il? Enfin: Quelle conclusion pouvez vous tirer sur l'utilité de GD pour entrainer des réseaux de neurones ? Quel est l'algorithme utilisé dans le code ? 2.7 Entrainement Relancez le code et appuyez sur entrée jusqu'au déclenchement de la partie 2.7. Vous devriez voir les itérations d'entrainement se succéder: Observez l'évolution de la précision sur l'ensemble d'entrainement et l'ensemble de test. Les valeurs sont elles identiques ? À partir de combien d'époques le réseau est il entrainé selon vous ? En réglant le nombre d'itération d'apprentissage dans le code (argument epochs de la fonction fit), arrivez vous à observer une phase de sur-apprentissage ? 2.8 Poids appris Appuyez sur entrée pour visualiser les noyaux appris par le réseau de neurones: En observant les noyaux de la première couche, arrivez vous à distinguer le genre de features qui seront extraites par chacun? Pouvez vous en faire de même pour la deuxième couche ? 2.9 Activations Appuyez sur entrée, puis rentrez un indice (un entier de n'importe quelle valeur inferieure a 12000): Après la première couche de convolution, les features extraites correspondent elles à celles que vous imaginiez ? Après la première couche de pooling, les features présentes auparavant sont elles conservées ? Après la deuxième couche de pooling, diriez vous que de l'information spatiale est toujours présente ? Autrement dit, les activations ressemblent elles toujours à des images ? 2.10 Entrainement final Arrêtez le script en appuyant sur ctrl+c. Jusqu'à présent, nous avons travaillé sur l'ensemble des données, mais pour la suite nous n'aurons besoin que des images de 1 et de 2. Changez la valeur de la variable CLASSES pour ne garder que les classes qui nous intéressent, entrainez en réseau, puis sauvegardez le dans un fichier. 3. Partie vision Ouvrez le fichier detection.py. 3.1 Présentation des données Démarrez le script. Une des images d'exemple issue du robot devrait vous être présentée: Observez les valeurs de pixels ? Quelles sont les valeurs de pixels blancs et noirs ? De manière génerale, la face des cubes est elle semblable aux images de mnist ? 3.2 Binarisation de l'image Appuyez sur entrée, et vous devriez voir s'afficher une image binarisee: Pouvez vous penser à un algorithme permettant d'arriver à un résultat à peu prés similaire ? Dans le code observez la fonction binarize: A quoi sert la fonction threshold_otsu ? Aidez vous de la documentationde scikit-image. Ajoutez une ligne pour afficher la valeur de thresh à chaque appel de la fonction. Cette valeur est elle la même pour toutes les images ? En commentant successivement les lignes les utilisant, décrivez l'impact de chacune des fonctions suivantes: A quoi sert la fonction closing ? A quoi sert la fonction clear_border ? A quoi sert la fonction convex_hull_object ? Concluez en résumant l'enchainement des opérations effectuées dans la fonction binarize. N'hesitez pas à vous aider de la documentation de scikit-image. 3.3 Recherche de contours Appuyez sur entrée pour faire défiler quelques images dont les contours ont été détectés. Observez la fonction get_box_contours: A quoi sert la fonction find_contour ? A quoi sert la fonction approximate_square ? Sur quelle fonction de scikit-image repose-t-elle ? A quoi sert la fonction reorder_contour ? Pourquoi cette opération est elle importante ? Concluez en résumant l'enchainement des opérations effectuées dans la fonction get_box_contours 3.4 Extraction des vignettes Appuyez sur entrée pour faire défiler quelques images dont les vignettes ont été extraites. Observez la fonction get_sprites: Qu'est ce qu'une transformation projective ? Regardez l'ordre des points source et repensez à la fonction reorder_contour. Son importance est elle plus claire maintenant? Dans quelle limites d'orientation, le cube sera-t-il réorienté correctement ? 3.5 Préparation des images Pendant la phase d'apprentissage, nous avons étudié la préparation qui était faite des images. Les vignettes que nous allons présenter au réseau de neurones doivent aussi subir une préparation pour avoir les mêmes caractéristiques que les images d'entrainement. Remplissez la fonction preprocess_sprites pour effectuer cette préparation. Une fois que cela est fait, executez le script jusqu'à la fin. 4. Intégration Une fois terminé, vous pouvez vous rendre dans main.py pour tester l'intégration de la détection et de la reconnaissance. Bravo ! 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"perception/pytorch/":{"url":"perception/pytorch/","title":"Torch","keywords":"","body":"IV. Perception avec Torch (PyTorch) Torch est une bilbiothèque opensource d'apprentissage machine et en particulier d'apprentissage profond. Depuis 2018 seule sa version Python nommée PyTorch est maintenue. Nous allons l'utiliser ici sur des imagettes sur lesquelles sont inscrites des chiffres marqués manuellement au feutre avec différentes calligraphies. Le réseau de neurones que vous allez créer devra apprendre lui-même à déterminer quel chiffre est marqué, ce que l'on appelle classifier. 0. Installation Pour effectuer cet atelier, vous devez installer quelques packages : pip install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html pip install scikit-image imageio git clone https://gitlab.inria.fr/apere/ros_workshop.git && git checkout e78f295b6 1. Documentation Pour remplir ce code, vous aurez besoin de consulter plusieurs documentations. De manière générale, vous aurez besoin de numpy. Si vous n'êtes pas familier avec, vous pouvez consulter : Numpy cheatsheet Numpy Documentation Pour la partie détection et pré-processing, nous utiliserons scikit-image. Vous pouvez vous inspirer des exemples: Scikit-Image General examples Enfin, pour la partie reconnaissance, nous utiliserons pytorch. Vous devriez trouver ce qu'il vous faut dans le tutoriel suivant: Deep learning with pytorch: a 60 minute blitz 2. Partie reconnaissance (Matin) 2.1 Les données et le modèle Commencez par ouvrir le fichier src/models.py. Pour vous familiariser avec les tenseurs, commencez par remplir les fonctions imag_mean et image_std. Quand cela est fait executez le script une première fois avec : python src/models.py Quelle est la taille d'un batch de données d'entrée ? De données de sortie ? Quelle est la moyenne du batch d'entrée ? Son écart type ? Dans le fichier se trouve une classe LeNet vide, qui contiendra votre modèle. En vous référant à la documentation de pytorch, remplissez la classe pour définir l'architecture LeNet vue pendant la présentation. Attention, la méthode forward prend en entrée un tenseur de taille [128, 1, 28, 28] et doit retourner un tenseur de taille [128, 2]. En revanche, la méthode infer doit retourner le label (1 ou 2) trouvé par le réseau de neurones pour une image, soit un simple float. Une fois le réseau défini, ré-exécutez le script. Si votre réseau est bien défini, il devrait être utilisé pour faire une inférence. Les labels trouvés par le réseau de neurones font ils sens ? Remplissez la fonction compute_params_count. Exécutez à nouveau; quel est le nombre de paramêtres du réseau de neurones ? Qu'en pensez vous ? Enfin, si tout le script s'exécute, vous devriez voir s'afficher les noyaux de la première couche. Qu'en pensez vous ? 2.2 L'entrainement. Dirigez vous vers le fichier train.py. La fonction perform_train_epoch contient la procédure d'entrainement. Remplissez-la en suivant la procédure présentée durant la matinée, puis exécutez le script. Si votre algorithme d'entrainement est correctement écrit, vous devriez voir les courbes d'apprentissage apparaitre. Que pensez vous de ces courbes ? Que pensez vous du niveau de performance à la fin de l'entrainement? A la suite de l'entrainement, le script vous présente les noyaux de la couche d'entrée du réseau. Qu'en pensez vous ? Ont ils évolué par rapport à tout à l'heure ? Enfin, pouvez vous entrainer le modêle pendant suffisament longtemps pour voir la phase d'overfitting apparaître ? 3. Partie amont (Après-midi) 3.1 Détection Exécutez le fichier src/detection.py. Les données sont chargées depuis le disque dans un tableau numpy, dont un example vous est montré. Remplissez la fonction rescale qui permet de ré-échelloner les valeurs du tableau entre 0 et 1. La première étape du pipeline de détection consiste à binariser l'image. Remplissez la fonction binarize et ré-exécutez le code. Observez chacun des exemples. Les chiffres ont ils bien disparus ? Les cubes sont ils couverts chacun par une unique zone blanche? Cette zone blanche est elle proche de la surface du cube ? Tant que vous ne pouvez répondre positivement à toutes ces questions (dans la mesure du raisonnable), ameliorez votre programme! Une fois fini, vous pouvez mettre binarize(im, debug=False) pour éviter de revoir toutes les images à chaque fois. Suite à cela, nous allons effectuer la recherche des cubes dans l'image. Remplissez la fonction get_box contour et ré-exécutez le code. À nouveau, les cubes sont ils bien détectés ? Tant que votre algorithme ne positionne pas correctement les coins des différentes boites de l'image, améliorez-le. Encore une fois quand vous aurez terminé, désactivez la visualisation avec get_box_contour(im, debug=False). 3.2 Pré-processing Nous avons maintenant une fonction get_box_contours qui nous donne, à partir d'une image d'entrée, une liste de contours de quadrilatères. Maintenant, il s'ait de transformer ces quaqdrilatères en images de la taille des images de mnist. Pour cela, nous allons calculer une transformation projective pour chacun des cubes. Remplissez la fonction get_sprites, puis ré-executez le code. Si votre code est correct, vous devriez voir s'afficher des imagettes ressemblant (au moin par la taille) aux images de mnist. Une fois cela fait, il ne vous reste plus qu'à remplir la fonction preprocess_sprites. Le but de cette fonction est de faire en sorte que les images données au réseau de neurones ressemblent le plus possible aux images de l'entrainement. Étudiez bien les données d'entrée avant de vous mettre au travail (vous aurez peut être besoin de retourner regarder src/models.py ou de regarder src/misc.py). 4. Final Une fois terminé, vous pouvez vous rendre dans main.py pour tester l'intégration de tout votre travail. Bravo ! 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"integration/ergo-tb-keras/":{"url":"integration/ergo-tb-keras/","title":"Poppy Ergo Jr + Turtlebot + Keras","keywords":"","body":"V. Intégration Poppy Ergo Jr + Turtlebot + Keras L'intégration consiste à intégrer dans une même cellule robotique les 3 briques logicielles travaillées les autres jours, à savoir : La manipulation par le bras robotique La navigation avec le robot roulant La vision avec le réseau de neurones Le scenario de l'intégration est un système de tri robotisé de pièces dans un bac 1 ou un bac 2 selon leur marquage au feutre. Prérequis Avoir suivi les TP Introduction, Manipulation, Navigation, et Perception Avoir réalisé une ébauche de nœud Python pour chacun des TP Manipulation, Navigation, et Perception Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Actions séquentielles de la cellule robotique Les actions de la cellule robotique sont les suivantes : Le réseau de neurone a été entraîné au préalable sur les batchs MNIST avec learn.py Le bras robotique prend une photo au dessus du feeder avec sa camera (étape \"scan\") Cette photo est envoyée au serveur de vision vision_server.py qui va : 3.1. extraire les contours carrés des cubes 3.2. transmettre les imagettes rognées selon leurs contours au réseau de neurones 3.3. le réseau de neurone va effectuer une prédiction sur le label marqué à la main 1 ou 2 Le noeud de manipulation récupère les coordonnées des cubes et leur label Pour chaque cube, il effectue un pick-and-place pour le positionner sur la remorque du Turtlebot. Le label est passé sur un paramètre /ros4pro/label Le Turtlebot lit ce paramètre et se rend au conteneur 1 ou 2 Il effectue une rotation de 360° pour faire chuter le cube dans la zone de tri à l'aide du mât 2. Comment faire ? Selon votre avancement, vous n'êtes pas obligés de suivre l'ordre de travail ci-dessous. Il y a peut-être un sujet plutôt qu'un autre où vous êtes le plus en retard que vous devez rattraper. 2.1 Le ROS master Vous aurez désormais 3 machines interconnectées : le robot roulant, le bras et votre station de travail. Donc pensez à en définir une comme ROS master et à mettre à jour tous les ROS_MASTER_URI. Nous vous conseillons d'utiliser le bras comme ROS master car il est branché sur secteur et n'est donc pas interrompu. Attention aux conflits durant les tests puisque tout le monde aura le même ROS master ! Avez-vous bien personnalisé vos noms de robots lors de l'introduction ? 2.2 Le serveur de paramètres Chaque ROS master démarre un serveur de paramètre qui lui est propre pour y enregistrer des données sous forme de paires de clés/valeurs, par exemple : /robot1/simulated: True /robot2/simulated: False /robot1/speed: 1.0 /robot2/speed: 1.2 Similairement aux commandes rostopic et rosservice dans le terminal, la commande rosparam dispose d'options pour interagir avec les paramètres enregistrés sur le serveur, et les noms des paramètres sont hierarchisés ave des / : rosparam list : Lister les paramètres du serveur rosparam get /frigo/fruit: Consulter la valeur du paramètre /frigo/fruit rosparam set /frigo/fruit Banane: Définir (ou écraser) le paramètre /frigo/fruit à la nouvelle valeur Banane Côté Python, les noeuds peuvent également définir ou consulter des paramètres avec les fonctions : rospy.get_param(\"/frigo/fruit\") : pour consulter rospy.set_param(\"/frigo/fruit\", \"Banane\") : pour définir ou écraser 2.3. Préparer la navigation Créez un nouveau noeud navigate_integrate.py similaire au noeud navigate_waypoints.py utilisé le jour 2. Additionnellement, il devra attendre que le bras robotique ait défini le paramètre /ros4pro/label à la valeur 1 ou 2 avant de démarrer la navigation vers le bac 1 ou le bac 2. Suivant la valeur du paramètre, le robot va déposer le cube à un des deux points. Trouvez les coordonnées de ces points sur la carte et écrivez les dans le script navigate_integrate.py. Après avoir déposé le cube, le robot va recommencer et attendre de recevoir une nouvelle valeur. Enregistrez une nouvelle carte en partance de l'endroit où le bras va déposer le cube (votre place pose) Prenez les coordonnées des points 1 et 2 dans le nouveau repère défini par la nouvelle carte, afin de les inscrire dans navigate_integrate.py Prévoyez le code pour effectuer des tours sur lui-même au moment de faire chuter le cube dans le bac Ce noeud a besoin de la navigation : roslaunch turtlebot3_navigation turtlebot3_navigation.launch Attendez que la navigation soit initialisée pour lancer le noeud : rosrun ros4pro navigate_integrate.py 2.4. Préparer la vision Si vous n'avez pas pris de retard la journée Perception, vous disposez d'un réseau de neurones fonctionnel. Nous allons le transformer en service ROS : on pourra lui faire une requête Quels sont les cubes que tu vois ? et il répondra en indiquant les cubes trouvés et leur label, le cas échéant. Comme chaque service ROS est typé, nous allons utiliser le type existant VisionPredict, qui comprend, dans la requête : image : un objet de type sensor_msgs/Image correspondant à l'image dans laquelle chercher des cubes Dans la réponse de ce service, il n'y a que des listes, car elle comprend entre 0 et n cubes trouvés, qui ont chacun 1 label associé. La réponse comprend donc : label de type std_msgs/UInt8[] : la liste des labels de chaque cube trouvé x_center de type std_msgs/UInt32[] : la liste des coordonées x des barycentres de chaque cube (dans le repère de la caméra) y_center de type std_msgs/UInt32[] : la liste des coordonées y des barycentres de chaque cube (dans le repère de la caméra) x1 de type std_msgs/UInt32[] : la liste des coordonées x du coin haut gauche de chaque cube (dans le repère de la caméra) y1 de type std_msgs/UInt32[] : la liste des coordonées y du coin haut gauche de chaque cube (dans le repère de la caméra) x2 de type std_msgs/UInt32[] : la liste des coordonées x du coin haut droite de chaque cube (dans le repère de la caméra) y2 de type std_msgs/UInt32[] : la liste des coordonées y du coin haut droite de chaque cube (dans le repère de la caméra) x3 de type std_msgs/UInt32[] : la liste des coordonées x du coin bas droite de chaque cube (dans le repère de la caméra) y3 de type std_msgs/UInt32[] : la liste des coordonées y du coin bas droite de chaque cube (dans le repère de la caméra) x4 de type std_msgs/UInt32[] : la liste des coordonées x du coin bas gauche de chaque cube (dans le repère de la caméra) y4 de type std_msgs/UInt32[] : la liste des coordonées y du coin bas gauche de chaque cube (dans le repère de la caméra) Note : Ainsi, pour chaque cube n°i trouvé dans la photo, label[i] est le label détecté par le réseau de neurones, et le point de coordonnées (x_center[i], y_center[i]) est le barycentre de ce cube dans le repère 2D de la caméra Avez l'aide du tutoriel \"Ecrire un service et client Python\", créer un fichier vision_server.py qui fournit service /ros4pro/vision/predict et répond, pour chaque requête contenant une image, la liste des cubes et des labels qu'il a trouvée. Réalisez d'abord script de test rosrun ros4pro call_vision_service_example.py qui sera le client qui va appeler ce service à partir d'une image de votre jeu de test. Cela permet de vérifier que votre modèle et votre entrainement prédit de façon suffisamment fiables les images du jeu de test. Sinon, vérifiez que vous avez bien respecté les consignes de la journée Perception et que votre réseau est entraîné sur un nombre suffisamment élevé d'époques. Rénetraînez-le autant de fois que nécessaire pour disposer d'un service fiable. 2.5. Préparer la manipulation Pour la manipulation, nous utiliserons le même noeud manipulate.py que celui de la journée Manipulation. Lorsqu'on le démarre avec roslaunch on ajoutera l'argument vision pour lui indiquer qu'il doit faire appel au serveur de vision et donc au réseau de neurones plutôt que d'utiliser l'emplacement vert prédéfini. C'est à dire : roslaunch ros4pro manipulate.launch simulate:=false vision:=true 2.6. Votre scenario fonctionne-t-il ? Pour être considéré comme un succès, votre cellule doit permettre de trier au moins 3 cubes de manière complètement autonome une fois que vous avez démarré les roslaunch et rosrun nécessaires. L'objectif est que ce scenario de tri puisse fonctionner dans une cellule en production. Cependant vous constaterez de nombreux défauts. Relevez et adressez un à un ces défauts pour améliorer le taux de succès de votre cellule de tri. 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"integration/sawyer-tb-keras/":{"url":"integration/sawyer-tb-keras/","title":"Sawyer + Turtlebot + Keras","keywords":"","body":"V. Intégration Sawyer + Turtlebot + Keras L'intégration consiste à intégrer dans une même cellule robotique les 3 briques logicielles travaillées les autres jours, à savoir : La manipulation par le bras robotique La navigation avec le robot roulant La vision avec le réseau de neurones Le scenario de l'intégration est un système de tri robotisé de pièces dans un bac 1 ou un bac 2 selon leur marquage au feutre. Prérequis Avoir suivi les TP Introduction, Manipulation, Navigation, et Perception Avoir réalisé une ébauche de nœud Python pour chacun des TP Manipulation, Navigation, et Perception Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Actions séquentielles de la cellule robotique Les actions de la cellule robotique sont les suivantes : Le réseau de neurone a été entraîné au préalable sur les batchs MNIST avec learn.py Sawyer prend une photo au dessus du feeder avec sa camera right_hand_camera (étape \"scan\") Cette photo est envoyée au serveur de vision vision_server.py qui va : 3.1. extraire les contours carrés des cubes 3.2. transmettre les imagettes rognées selon leurs contours au réseau de neurones 3.3. le réseau de neurone va effectuer une prédiction sur le label marqué à la main 1 ou 2 Le noeud de manipulation récupère les coordonnées des cubes et leur label Pour chaque cube, il effectue un pick-and-place pour le positionner sur la remorque du Turtlebot. Le label est passé sur un paramètre /ros4pro/label Le Turtlebot lit ce paramètre et se rend au conteneur 1 ou 2 Il effectue une rotation de 360° pour faire chuter le cube dans la zone de tri à l'aide du mât 2. Comment faire ? Selon votre avancement, vous n'êtes pas obligés de suivre l'ordre de travail ci-dessous. Il y a peut-être un sujet plutôt qu'un autre où vous êtes le plus en retard que vous devez rattraper. VOus pouvez également vous mettre par 2 ou 3 et attribuer un sujet de travail (Sawyer, Turtlebot ou vision) à chacun. 2.1 Le ROS master Sawyer démarrant son propre ROS master au démarrage, afin de communiquer avec lui nous n'avons d'autre choix que de désigner Sawyer comme ROS master et faire pointer les ROS_MASTER_URI des Turtlebots et des stations de travail sur Sawyer, uniquement si Sawyer est nécessaire dans votre expérimentation. Attention aux conflits durant les tests puisque tout le monde aura le même ROS master ! Lorsque vous n'avez pas besoin de Sawyer, repassez sur un ROS master burgerX.local ou localhost. 2.2 Le serveur de paramètres Chaque ROS master démarre un serveur de paramètre qui lui est propre pour y enregistrer des données sous forme de paires de clés/valeurs, par exemple : /robot1/simulated: True /robot2/simulated: False /robot1/speed: 1.0 /robot2/speed: 1.2 Similairement aux commandes rostopic et rosservice dans le terminal, la commande rosparam dispose d'options pour interagir avec les paramètres enregistrés sur le serveur, et les noms des paramètres sont hierarchisés ave des / : rosparam list : Lister les paramètres du serveur rosparam get /frigo/fruit: Consulter la valeur du paramètre /frigo/fruit rosparam set /frigo/fruit Banane: Définir (ou écraser) le paramètre /frigo/fruit à la nouvelle valeur Banane Côté Python, les noeuds peuvent également définir ou consulter des paramètres avec les fonctions : rospy.get_param(\"/frigo/fruit\") : pour consulter rospy.set_param(\"/frigo/fruit\", \"Banane\") : pour définir ou écraser 2.3. Préparer la navigation Le noeud navigate_integrate.py est similaire au noeud navigate_waypoints.py utilisé le jour 2. Additionnellement, il attend que Sawyer définisse le paramètre /ros4pro/label à la valeur 1 ou 2 avant de démarrer la navigation vers le bac 1 ou le bac 2. Suivant la valeur du paramètre le robot va déposer le cube à un des deux points. Trouvez les coordonnées de ces points sur la carte et écrivez les dans le script ros4pro/src/nodes/navigate_integrate.py. Après avoir déposé le cube, le robot va recommencer et attendre de recevoir une nouvelle valeur. Enregistrez une nouvelle carte en partance de l'endroit où Sawyer va déposer le cube (votre place pose) Prenez les coordonnées des points 1 et 2 dans le nouveau repère défini par la nouvelle carte, afin de les inscrire dans navigate_integrate.py Prévoyez le code pour effectuer des tours sur lui-même au moment de faire chuter le cube dans le bac Ce noeud a besoin de la navigation : roslaunch turtlebot3_navigation turtlebot3_navigation.launch Attendez que la navigation soit initialisée pour lancer le noeud : rosrun ros4pro navigate_integrate.py 2.4. Préparer la vision Si vous n'avez pas pris de retard la journée Perception, vous disposez d'un serveur de vision qui est en écoute du service /ros4pro/vision/predict et répondra, pour chaque requête contenant une image, la liste des cubes et des labels qu'il a trouvée. Utilisez le script de test de la vision rosrun ros4pro call_vision_service_example.py pour vérifier que votre modèle et votre entrainement prédit de façon suffisamment fiables les images du jeu de test. Sinon, vérifiez que vous avez bien respecté les consignes de la journée Perception et que votre réseau est entraîné sur un nombre suffisamment élevé d'époques. Rénetraînez-le autant de fois que nécessaire pour disposer d'un serveur fiable. 2.5. Préparer la manipulation Pour la manipulation, nous utiliserons le même noeud manipulate.py que celui de la journée Manipulation. Lorsqu'on le démarre avec roslaunch on ajoutera l'argument vision pour lui indiquer qu'il doit faire appel au serveur de vision et donc au réseau de neurones plutôt que d'utiliser l'emplacement vert prédéfini. C'est à dire : roslaunch ros4pro manipulate.launch simulate:=false vision:=true 2.6. Votre scenario fonctionne-t-il ? Pour être considéré comme un succès, votre cellule doit permettre de trier au moins 3 cubes de manière complètement autonome une fois que vous avez démarré les roslaunch et rosrun nécessaires. L'objectif est que ce scenario de tri puisse fonctionner dans une cellule en production. Cependant vous constaterez de nombreux défauts. Relevez et adressez un à un ces défauts pour améliorer le taux de succès de votre cellule de tri. 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"theory/":{"url":"theory/","title":"VI. Robotique théorique","keywords":"","body":"Robotique théorique Ce cours et TP aborde les notions de base des concepts théoriques sous-jacents à la robotique : les modèles géométriques direct et indirect, ainsi que les probablités appliquées aux filtres en traitement du signal. Prérequis Géométrie et trigonométrie BAC+2 et supérieur voire options scientifiques des lycées Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. Travaux pratiques Modèle géométrique direct Lancez la commande roslaunch ros4pro_robotique_theorique scara_fake_direct.launch pour lancer une simu de SCARA. Une fenêtre avec des sliders permet de bouger le bras. Modifiez le script src/geometrique_direct.py en y mettant votre modèle géométrique direct. Il calcul la position de l'effecteur à partir des angles des articulations. Lancez la coommande rosrun ros4pro_robotique_theorique geometrique_direct.py pour exécuter le script de test du modèle géométrique direct. Modèle géométrique indirect Lancez la commande roslaunch ros4pro_robotique_theorique scara_fake_inverse.launch pour lancer une simu de SCARA. Modifiez le script src/geometrique_inverse.py en y mettant votre modèle géométrique inverse. Il calcul les angles et la position des articulations pour que l'effecteur sooit à la position voulue. Lancez la coommande rosrun ros4pro_robotique_theorique geometrique_inverse.py pour exécuter le script de test du modèle géométrique inverse. Probabilités robotiques Installez les bibliothèques nécessaires à l'exécution des démonstrations de probabilité pour la robotiique, dans un terminal exécutez les coommandes suivantes : sudo apt install python3-pip pip3 install matplotlib scipy python3 histogram_filter.py Pour lancer les démonstrations de probabilité allez dans le dossier proba et utilisez python3 pour exécuter les fichiers, ex: python3 extended_kalman_filter.py. 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"simulation/gazebo/":{"url":"simulation/gazebo/","title":"Gazebo","keywords":"","body":" VII. Simulation avec Gazebo Lancement de Gazebo avec un monde \"vide\" Chargement d'un modèle Un simple cube Chargement du modèle du robot Chargement des contrôleurs de moteurs MoveIt Démarrer MoveIt Créer un node python pour contrôler le robot Créer un package ROS ros4pro Éditer manipulate.py Utiliser la pince Ajouter des obstacles VII. Simulation avec Gazebo En robotique il est souvent très utile de pouvoir travailler en simulation. Un simulateur physique permet essentiellement de simuler des forces/couples sur des objets et des articulations. Gazebo est un environnement de simulation physique pour robotique, supporté par ROS. Nous nous servirons de la simulation dans Gazebo de manière transparente, \"comme si\" il s'agissait du véritable robot. La simulation va nous permettre: De charger des modèles URDF De simuler des moteurs De simuler une caméra De simuler les contacts Notons que Gazebo est constitué d'un serveur (non graphique, gzserver) et d'un client (graphique, gzclient) ce qui permet également de calculer une simulation sur une machine distante par exemple. Lancement de Gazebo avec un monde \"vide\" roslaunch gazebo_ros empty_world.launch Ce que nous allons utiliser, mais il est bien sur possible de créer des environnements plus complexes. Chargement d'un modèle Les modèles sont décrit par un fichier xml selon la norme URDF (Universal Robot Description Format). Un simple cube Dans poppy_ergo_jr_gazebo/urdf on peut voir la définition d'un cube. Pour charger cet URDF dans Gazebo: rosrun gazebo_ros spawn_model -file cube.urdf -urdf -model test -x 0 -y 0 -z 1 Quelles sont les dimensions du cube? Quelle est la masse du cube? Expérimentez pour expliquer la différence entre visual et collision (menu View/Collisions) Observez la position du cube avec rostopic echo -n 1 /gazebo/model_states Modifiez cette position avec rosservice call /gazebo/set_model_state [TAB] (Utilisez la complétion du terminal avec [TAB] pour remplir le message) Appliquez une force de 10N selon l'axe x pendant 5s sur le cube en utilisant: rosservice call /gazebo/apply_body_wrench (Quel est le \"bodyname\" à utiliser?) Pour supprimer le modèle: rosservice call gazebo/delete_model \"model_name: 'test'\" Chargement du modèle du robot On utilise ici un format intermédiaire \"xacro\", permettant d'ajouter un capacité de \"script\" (pour calculer des position par exemple) et générer un URDF. On peut visualiser la topologie du modèle avec: urdf_to_graphiz poppy_ergo_jr.urdf (un pdf est généré) Ouvrez le PDF obtenu puis déterminez : Que représentent les rectangles ? Que représentent les bulles ? Que représentent les flèches et surtout les valeurs xyz et rpy associées ? Pour importer le modèle dans Gazebo: roslaunch poppy_ergo_jr_gazebo load_ergo_model.launch On peut \"explorer\" le modèle dans le menu à gauche. Pour mieux visualiser les articulations: View/Transparent View/Joints Le modèle s'effondre car les moteurs ne sont pas simulés. Appliquez un couple de -0.5Nm sur l'articulation m2 pendant 3s avec: rosservice call /gazebo/apply_joint_effort [TAB] Chargement des contrôleurs de moteurs Afin de rentre la simulation plus réaliste, nous allons lancer des contrôleurs de moteurs qui vont simuler le comportement de moteurs réels. Il existe plusieurs type de contrôleurs disponible dans Gazebo, nous allons tout d'abord expérimenter avec les contrôleurs en position les plus simples. lancez: roslaunch poppy_ergo_jr_gazebo load_ergo_position_controllers.launch On constate la création de topics pour chaque contrôleur Envoyez des commandes en position: rostopic pub /ergo_jr/m2_position_controller/command [TAB] [TAB] Comment contrôler la position de la pince dans l'espace cartésien? MoveIt Dans le cas général, calculer les mouvements nécessaires pour atteindre un objectif sans collision est un problème compliqué (cf. robotique théorique). Cette tâche est effectuée par un planificateur, tel quel MoveIt qui intègre: Le modèle cinématique du robot (à partir de l'URDF) Une gestion des collisions (internes et avec l'environnement) Un planificateur de trajectoire Démarrer MoveIt Précédemment nous avons expérimenté avec les contrôleurs en position. MoveIt a besoin de contrôleurs légèrement différents (contrôleurs de trajectoire). Lancez tout d'abord Gazebo et charger le modèle du robot. Puis lancez les contrôleurs en trajectoire avec: roslaunch poppy_ergo_jr_gazebo load_ergo_controllers.launch ⇒ Vous pouvez aussi combiner ces 3 étapes avec un seul fichier .launch: roslaunch poppy_ergo_jr_gazebo start_gazebo.launch gripper:=true lamp:=false Lancez MoveIt avec: roslaunch poppy_ergo_jr_moveit_config start_moveit.launch gripper:=true lamp:=false Essayez de manipuler le robot: Dans Query/Planning Group sélectionnez arm, dans Options cochez Allow Approx IK Solutions Cliquez sur Planning/Plan and Execute observer Rviz et Gazebo Créer un node python pour contrôler le robot Créer un package ROS ros4pro cd ~/catkin_ws/src catkin_create_pkg ros4pro # Cette commande créé le package mkdir -p ros4pro/src # On créé un dossier src dans le package touch ros4pro/src/manipulate.py # On créé un noeud Python \"manipulate.py\" chmod +x ros4pro/src/manipulate.py # On rend ce noeud exécutable pour pouvoir le lancer avec rosrun Éditer manipulate.py Nous allons avoir besoin des imports suivants: #!/usr/bin/env python import rospy from moveit_commander.move_group import MoveGroupCommander from geometry_msgs.msg import Pose from math import radians, cos, sin import tf_conversions as transform rospy.init_node('ros4pro_node') Pour utiliser le \"commander\" MoveIt il faut déclarer: commander = MoveGroupCommander(\"arm\") Il est possible de récupérer la \"pose\" actuelle: current_pose = commander.get_current_pose().pose Faites bouger le robot et affichez cette pose. Quelle est la pose initiale? Créez une nouvelle pose (cf. le message [[https://docs.ros.org/en/api/geometry_msgs/html/msg/Pose.html][Pose], modifiez la position et l'orientation ce cette pose et exécutez là: commander.set_pose_target(pose) #envoie la pose au commander plan = commander.go(wait=True) #éxécute le mouvement avec attente commander.stop() #force l'arrêt du mouvement pour plus de sécurité commander.clear_pose_targets() #force le nettoyage des objectifs du commander pour plus de sécurité Pour nous aider on peut créer un quaternion à partir d'une rotation au format Roll/Pitch/Yaw avec: q = transform.transformations.quaternion_from_euler(roll, pitch, yaw) q retourné est une liste de 4 éléments contenant x, y, z, w roll, pitch, yaw sont des angles en radian (la fonction radians(angle) permet de convertir des degrés en radians) Utiliser la pince Lancez le service avec rosrun poppy_ergo_jr_gazebo gripper_gz_service.py Il est possible d'ouvrir/fermer la pince avec la commande: rosservice call /ergo_jr/close_gripper \"data: false\" pour ouvrir la pince, et \"true\" pour la fermer. Pour utiliser ce service dans votre node se référer à la documentation On notera que le type du service est SetBool défini dans le module std_srvs.srv Charger des cubes dans l'environnement de simulation avec roslaunch poppy_ergo_jr_gazebo spawn_cubes.launch Les cubes sont placés sur un rayon de 0.21m par rapport au référentiel avec des angles de -25°, 0° et +25° Essayez d'attraper chacun des cubes Essayer d'empiler les cubes en -25° et +25° sur le cube en 0° Ajouter des obstacles Nous allons avoir besoin du message PoseStamped défini dans le module geometry_msgs.msg Il est possible d'ajouter un obstacle pour MoveIt comme ceci par exemple: scene = PlanningSceneInterface() rospy.sleep(1) #petite attente nécessaire ps = PoseStamped() ps.header.frame_id = \"base_link\" ps.pose.position.x = 0.0543519994715 ps.pose.position.y = -0.202844423521 ps.pose.position.z = 0.1 q = transform.transformations.quaternion_from_euler(0, 0, radians(15)) ps.pose.orientation.x = q[0] ps.pose.orientation.y = q[1] ps.pose.orientation.z = q[2] ps.pose.orientation.w = q[3] scene.add_box(\"obstacle\", ps, (0.025, 0.1, 0.2)) #dimensions de la boite On peut ensuite enlever l'obstacle avec: scene.remove_world_object(\"obstacle\") Créez un mouvement pour attraper un cube en évitant cet obstacle Nous voulons maintenant utiliser ce mécanisme pour ajouter les cubes comme obstacles au fur et à mesure que nous les empilons Ajouter cube1 comme obstacle Attraper cube2, le poser sur cube1 et ajouter cube2 comme obstacle idem pour cube3 Les positions des objets sont publiés en temps réel par Gazebo dans le \"topic\" /gazebo/model_states 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"faq/pi/":{"url":"faq/pi/","title":"FAQ robots basés sur Raspberry Pi","keywords":"","body":"📥 Flasher la carte SD Note préliminaire : la carte SD du robot ne se comporte pas tout-à-fait comme une carte SD \"normale\". Elle ne permet pas de stocker des fichiers dessus ; il est également normal qu'une carte SD insérée dans votre ordinateur n'apparaisse pas dans le Poste de Travail avant de l'avoir flashée. Téléchargez ces images ROS en vue de remettre à zéro les cartes SD des robots pour ROS4PRO (⚠️⏳ Optimisez votre temps, le téléchargement peut prendre 1 heure) : Image du Turtlebot pour ROS4PRO Image de Poppy Ergo Jr pour ROS4PRO Pour flasher l'une de ces images sur une carte SD : extrayez le fichier compressé .zip ou .7z (généralement clic droit > Extraire) dans un dossier de votre ordinateur (pas sur la carte SD) : vous obtenez un fichier d'extension .img ⚠️ ne faîtes pas de glisser-déposer ni de copier-coller de cette image vers la carte SD comme s'il s'agissait d'une clé USB : Il est nécessaire d'utiliser un logiciel de flash comme Etcher ou dd 📀 Tapez la commande etcher dans le terminal Ubuntu pour ouvrir l'utilitaire de flash préinstallé (ou bien téléchargez Etcher s'il n'existe pas encore) Dans Etcher, \"Flash from file\", sélectionnez le fichier image ainsi que la destination (la carte SD) et validez Le flash de la carte SD est en cours ... ⚠️⏳ Optimisez votre temps, la copie dure environ 15 minutes. Dès qu'Etcher a terminé, votre carte SD est prête à être configurée pour le Wifi et/ou insérée dans le robot Optionnellement, en cas de besoin de restaurer les robots avec les images d'usine, voici les liens (mais ces images ne sont pas utilisables avec ROS4PRO) : Image d'usine du Turtlebot (pas de namespace complet, n'inclut pas la posibilité d'intégrer plusieurs robots) Image d'usine de Poppy Ergo Jr (avec l'interface graphique http://poppy.local mais sans ROS) 📡 Connecter le robot en Wifi ⚠️ La mise en place de la connexion du robot en Wifi ne nécessite pas de démarrer le robot Insérer la carde SD du robot en question dans votre poste de travail (pas dans votre robot) et ouvrir la partition nommée boot Télécharger le fichier wpa_supplicant.conf dans boot et modifiez-le pour renseigner le bon mot de passe wifi à l'intérieur (sans changer le nom de fichier). Respectez la casse : majuscules/minuscules. Créer un fichier vide nommé ssh au même endroit dans boot (par exemple avec la commande touch ssh dans le dossier courant) Taper la commande sync puis éjectez proprement la carte SD dans le navigateur de fichier pour éviter toute perte de données avant de la retirer. Ces 2 fichiers wpa_supplicant.conf et ssh seront supprimés au prochain démarrage du robot, signalant que la demande de connexion Wifi a bien été prise en compte. C'est donc normal que vous ne les trouviez plus en regardant à nouveau le contenu de boot après un premier démarrage du robot. En cas de problème, il est possible de connecter un écran HDMI à la Raspberry Pi, le gestionnaire réseau se trouve en haut à droite. La connexion Wifi fonctionne aussi avec les points d'accès mobiles d'Android et iOS. 🖧 Se connecter via SSH à un robot SSH (Secure SHell) permet d'ouvrir un terminal à distance sur une autre machine que celle sur laquelle on tape les commandes (par exemple le robot, qui n'a ni clavier ni écran pour interagir avec un terminal). Il est nécessaire de connaître : Le nom de la machine distante (par ex poppy.local ou raspberrypi.local) Le nom d'utilisateur propriétaire de la session sur laquelle ouvrir un terminal (toujours pi dans notre cas) Le mot de passe de cette session (cf mots ce passe par défaut ci-dessous) La commande est la suivante, à taper dans un terminal sur Ubuntu : ssh pi@poppy.local Taper yes pour confirmer la connexion puis taper le mot de passe. Votre invite de commande devrait désormais indiquer pi@poppy.local~$ : toute commande tapée dans ce terminal sera exécutée par le robot. En cas d'erreur, consultez la procédure de diagnostic ci-dessous. 🔑 Mots de passe par défaut Turtlebot Nom d'utilisateur pi Nom de machine raspberrypi (ajouter .local dans les commandes : raspberrypi.local) Mot de passe turtlebot Poppy Nom d'utilisateur pi Nom de machine poppy (ajouter .local dans les commandes : poppy.local) Mot de passe raspberry 🌈 Personnaliser les noms de robots et ordinateurs Au démarrage du TP, tous les robots et les ordinateurs possèdent le même nom à savoir ubuntu (votre ordinateur), poppy (le robot manipulateur), turtlebot (le robot roulant), ce qui posera problème lorsqu'on les fera communiquer ensemble. Pour ces 3 machines, nous allons donc changer leur nom, en ajoutant juste votre numéro de groupe à la fin, par exemple poppy5. 💻🤖 Pour personnaliser votre nom, il faut ouvrir un terminal sur la machine à renommer (via SSH pour les robots) puis : sudo hostnamectl set-hostname sudo reboot Veillez bien à utiliser ensuite ce nouveau nom dans vos futures commandes (SSH ou ROS_MASTER_URI, ...). Si vous avez nommé votre robot poppy5 par exemple, il faudra donc utiliser poppy5.local. 🔧 Procédure de diagnostic 💻 Dans un terminal taper ping poppy.local (pour Poppy) ou ping raspberrypi.local (pour Turtlebot) : Si 1 ligne s'affiche chaque seconde avec des statistiques de temps en millisecondes ➡️ Test réseau réussi. Vous avez peut-être oublié de démarrer le roscore ou bien ROS_MASTER_URI dans le fichier ~/.bashrc pointe vers le mauvais robot Si une erreur survient et la commande s'arrête ➡️ Test réseau échoué. Vérifiez que la LED verte ACT de la Raspberry Pi vacille pendant environ 45 secondes lorsque vous venez de brancher l'alimentation : Si ACT vacille en 🟢 ➡️ Votre Raspberry Pi démarre correctement mais la configuration réseau est incorrecte. Vérifiez que vous avez placé le fichier wpa_supplicant.conf au bon endroit dans la partition boot sur la carte SD si vous êtes en Wifi ; ou bien connectez-vous avec un câble RJ45 sur un routeur Si ACT ne vacille pas ➡️ Votre Raspberry Pi ne démarre pas correctement. La LED rouge PWR s'allume-t-elle ? Si PWR s'allume en 🔴 ➡️ Votre Raspberry Pi est fonctionnelle mais la carte SD ne possède pas une image valable. Recommencez la procédure de flash ci-dessus. Si PWR ne s'allume pas ➡️ Votre Raspberry Pi n'est pas fonctionnelle. Vous avez peut-être mal branché la Pixl (Poppy) ou bien le câble rouge-noir (Turtlebot) 🔔 Mon Turtlebot bipe 🔋 Il s'agit du signal de batterie faible et il ne doit pas être ignoré. Turtlebot est alimenté par une batterie puissante de type Li-Po. Ce type de batterie rend dangereux leur utilisation lorsque la charge est très faible. Dans un cas extrême elle pourrait chauffer et prendre feu. Mettre en charge rapidement la batterie lorsque Turtlebot bipe. 📚 Auteurs Jessica Colombel (Inria), Rémi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre Péré (Inria), Steve N'Guyen (LaBRI) . 💬 Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. 📅 Dernière mise à jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "}}