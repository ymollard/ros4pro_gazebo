{"./":{"url":"./","title":"Qu'est-ce que c'est ?","keywords":"","body":"Cours et Travaux pratiques de robotique ROS4PRO est un ensemble de ressources francophones pour l'apprentissage de la robotique opensource avec le framework ROS qui √©quipera plus de la moiti√© des robots vendus en 2024. Vous pouvez librement les suivre, les dupliquer et les enrichir puis partager vos am√©liorations. Comment suivre les TP ? ü§ñ Il est probable que vous suiviez ces travaux pratiques dans le cadre d'une formation qui met √† votre disposition du mat√©riel robotique. Selon votre mat√©riel, plusieurs parcours sont disponibles ci-dessous. Si vous n'avez pas de mat√©riel, certains parcours peuvent √™tre suivis partiellement en simulation. üíª Vous devez disposer d'un ordinateur de type PC ainsi que Ubuntu 18.04 et ROS Melodic install√©s. üìÄ Pour suivre tous les parcours sans avoir √† installer ROS et les d√©pendances vous-m√™me, vous pouvez t√©l√©charger cette cl√© USB Live et suivre les instructions de flash. Parcours possibles Parcours n¬∞ 1 : Poppy Ergo Jr + Keras + Turtlebot (possible partiellement en simulation) Introduction Navigation Manipulation Perception Int√©gration Parcours n¬∞ 2 : Sawyer + Keras + Turtlebot Introduction Navigation Manipulation Perception Int√©gration L√©gende Les pictogrammes suivants sont utilis√©s : üíª : Proc√©dure ex√©cuter sur votre poste de travail Ubuntu ü§ñ : Proc√©dure √† ex√©cuter sur le robot, en utilisant SSH üìÄ : Cette proc√©dure est d√©j√† fa√Æte pour vous si vous lancez Ubuntu via une cl√© USB Live üêç : Code Python √† enregistrer et ex√©cuter sur votre poste de travail üì• : Ressource √† T√©l√©charger ‚úç : R√©pondre aux questions par √©crit Photos de quelques TP pass√©s en pr√©sentiel ou distanciel üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"introduction/":{"url":"introduction/","title":"I. Introduction √† Linux et ROS","keywords":"","body":"I. Introduction √† Linux, ROS et ROS-I Pr√©requis Lyc√©e et + Notions de programmes informatiques, terminaux et commandes Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. D√©marrer Ubuntu et ROS Selon votre situation, une cl√© USB bootable peut vous √™tre fournie. Dans ce cas, vous devez faire \"booter\" votre poste de travail sur la cl√© USB Live fournie. Dans le cas contraire, il est n√©cessaire d'avoir vous m√™me install√© Ubuntu 18.04 et ROS Melodic. Dans ce cas il se peut que vous ayez √† installer vous-m√™me des √©l√©ments suppl√©mentaires tout le long des travaux pratiques. 2. Prise en main du terminal : le rosier üåπ ‚å®Ô∏è Pour prendre en main le terminal Linux et les commandes de base, √† partir d'un terminal, cr√©ez les fichiers et dossiers n√©cessaires pour r√©aliser cette hierarchie de fichiers ressemblant √† un rosier : Vous aurez besoin des commandes suivantes :h ls, pour lister les fichiers et dossiers courants cd, pour changer le dossier courant mkdir, pour cr√©er un nouveau dossier touch, pour cr√©er un nouveau fichier vide nano, pour cr√©er un nouveau fichier et √©crire √† l'int√©rieur tree, pour afficher la hierarchie de fichiers 3. Tutoriels üßë‚Äçüè´ Vous √™tes d√©sormais pr√™t √† utiliser ROS ! Suivez les tutoriels ROS suivants pour d√©couvrir et tester les concepts de base, sachant que votre distribution ROS s'appelle melodic : Understanding ROS Nodes : Ma√Ætriser ROS master (roscore) et lancer des n≈ìuds (rosrun) Understanding ROS Topics : Envoyer et recevoir des messages dans un topic (rostopic) Understanding ROS Services and Parameters : D√©clarer et appeler un service requ√™te/r√©ponse (rosservice, rossrv) ‚ùì Quizz : quizz au tableau pour m√©moriser les commandes importantes 4. ‚öôÔ∏è Pr√©parer vos robots Pour l'un ou l'autre de vos 2 robots, r√©alisez les √©tapes de pr√©paration suivantes expliqu√©es dans la FAQ robots : Flasher sa carte SD Connecter le robot en wifi Se connecter via SSH au robot Personnaliser le nom de votre robot (si n√©cessaire) 5. FAQ üì• Mise √† jour pendant le TP Il se peut que l'enseignant mette √† jour les ressources pendant le cours. Dans ce cas ex√©cutez les commandes suivantes pour r√©cup√©rer les derni√®res mises-√†-jour : roscd ros4pro git pull origin poppy_tb3_keras Si l'erreur suivante survient : error: Vos modifications locales aux fichiers suivants seraient √©cras√©es par la fusion : Veuillez valider ou remiser vos modifications avant la fusion. Abandon Alors les fichiers sp√©cifi√©s ne peuvent pas √™tre mis √† jour car cela d√©truirait les modifications que vous avez apport√©es √† la liste des fichiers indiqu√©e. Il est recommand√© de demander conseil avant d'essayer une autre action pour r√©cup√©rer la mise √† jour. Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"navigation/turtlebot/":{"url":"navigation/turtlebot/","title":"Turtlebot","keywords":"","body":"II. Robotique de navigation avec Turtlebot Pr√©requis Lyc√©e et + Notions de Python et commandes dans un terminal Aisance en g√©om√©trie 2D Le TP d'introduction Ce TP est compatible avec la simulation si vous n'avez pas de Turtlebot Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. Travaux pratiques 1. Assemblage du Turtlebot (avec un robot r√©el) ‚ö†Ô∏è Attention la documentation officielle du Turtlebot convient tr√®s bien pour l'√©lectrom√©canique mais la documentaiton logicielle est obsol√®te, ne tapez aucune commande de la documentation sans avoir demand√© si elle convient ! ‚ö†Ô∏è Attention : vous ne pourrez faire aucune erreur de c√¢blage sauf avec le c√¢ble d'alimentation de la Raspberry Pi qui doit imp√©rativement √™tre branch√© comme sur le sch√©ma ci-dessous au risque de d√©teriorer d√©finitivement le mat√©riel. ‚ñ∂Ô∏è Suivez cette vid√©o pour assembler votre Turtlebot Burger : 2. Bringup du TB3 (avec un robot r√©el) üîç V√©rifiez d'abord la configuration r√©seau de ROS sur votre PC et sur le TB3 : ROS_MASTER_URI doit pointer vers le Turtlebot. V√©rifiez √©galement que vous avez connect√© le robot au Wifi et renomm√© votre robot en y ajoutant votre num√©ro de groupe (par ex burger8) avec les instructions de l'introduction. üíª Lancez roscore dans un premier terminal. ü§ñ Sur le TB3 lancer la commande roslaunch turtlebot3_bringup turtlebot3_robot.launch. S'il n'y a aucune erreur vous √™tes pr√™t √† piloter le robot depuis votre poste de travail, que ce soit pour la t√©l√©op√©ration, la cartographie ou la navigation autonome. 2.bis. Bringup du Turtlebot (en simulation) ‚ö†Ô∏è Attention la simulation du TB3 n'est a utiliser qu'en dernier recours pour remplacer votre robot s'il ne fonctionne pas. Avant de passer en simulation demandez de l'aide pour r√©parer votre robot. üì• Vous devez t√©l√©charger et installer le paquet ROS de simulation du TB3 : üíª Lancez cd ~/catkin_ws/src dans un terminal pour vous d√©placer dans le dossier contenant les sources de vos paquets ROS. üíª Lancez git clone https://github.com/ros4pro/turtlebot3_simulations.git dans le m√™me terminal, le dossier turtlebot3_simulations est cr√©√© dans le r√©pertoire ~/catkin_ws/src. üíª Lancez cd ..; catkin_make, le nouveau paquet est install√©. Apr√®s la compilation lancez source ~/.bashrc dans chaque terminal pour les mettre √† jour ou fermez les tous. üîç La simulation remplace le robot donc vous ne devez ni essayer de lancer le bringup du TB3 et ni vous connecter au robot. √Ä la place vous devez lancer le simulateur et configurer ROS_MASTER_URI pour pointer vers votre PC (ROS master = cette machine). üíª Lancez roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch, le simulateur Gazebo se lance et vous devez voir le TB3 au milieu de la fen√™tre. Plusieurs environnements de simulation sont disponibles : turtlebot3_empty_world.launch : un monde vide, ne contenant que le TB3 et un sol. turtlebot3_house.launch : une maison avec plusieurs pi√®ces et du mobilier. turtlebot3_world.launch : le TB3 est au milieu d'un carr√©. turtlebot3_stage_1.launch : le TB3 est dans une ar√®ne carr√©e. turtlebot3_stage_2.launch : le TB3 est dans une ar√®ne carr√© avec 4 obstacles fixes. turtlebot3_stage_3.launch : le TB3 est dans une ar√®ne carr√© avec 4 obstacles fixes. turtlebot3_stage_4.launch : le TB3 est dans une grande ar√®ne carr√©e avec plusieurs obstacles et des murs. 3. T√©l√©op√©ration üéÆ La premi√®re √©tape consiste √† v√©rifier que votre poste de travail peut effectivement prendre le contr√¥le du Turtlebot, en le t√©l√©op√©rant via les touches du clavier. üíª Dans un nouveau terminal lancez la commande roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch et gardez le focus sur le terminal pour controler le robot avec le clavier gr√¢ce aux touches indiqu√©es. V√©rifiez que vous pouvez avancer, reculer, tourner √† gauche et √† droite. Vous pouvez tuer ce dernier avec Ctrl+C lorsque vous avez termin√©. 4. Cartographie üó∫Ô∏è Nous allons d√©sormais cr√©er la carte de l'environnement dans lequel votre Turtlebot √©voluera lorsqu'il naviguera de mani√®re autonome. üíª Lancez le commande roslaunch turtlebot3_slam turtlebot3_slam.launch. RViz se lance et vous devriez apercevoir le robot, les scans du LIDAR et la carte en construction. üíª Dans un nouveau terminal lancez la commande roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch et gardez le focus sur le terminal pour contr√¥ler le robot avec le clavier comme pr√©c√©demment. Cependant cette fois-ci, votre carte est en cours d'enregistrement. Quand la carte est termin√©e ne quittez ni RViz ni le terminal de la cartographie. üíª Dans un nouveau terminal lancez la commande roscd turtlebot3_navigation/maps/ pour aller dans le dossier o√π la carte est enregistr√©e. üíæ La commande qui va suivre va supprimer la carte pr√©c√©dente s'il y en a une, le cas √©ch√©ant faites-en une copie si vous souhaitez la conserver. Lancez la commande roslaunch ros4pro map_saver.launch qui va sauvegarder la carte dans les fichiers maps.yaml et maps.pgm et √©craser les anciens. 5. Navigation üíª Lancez le commande roslaunch turtlebot3_navigation turtlebot3_navigation.launch pour lancer la localisation et la navigation autonome. üëÄ Sur RViz vous devez voir le robot, les scans du LIDAR, les particules de AMCL et la carte que vous avez enregistr√©e. üìç Si le robot est mal localis√©, utilisez l'outil 2D Pose Estimate sur RViz. Cliquez et Glissez avec la souris pour positionner le robot sur la carte. üìç Pour donner des ordres de navigation, utilisez l'outil 2D Nav Goal sur RViz. Cliquez et Glissez avec la souris sur la carte l√† o√π le robot doit aller. 6. Scenario de navigation üöó L'objectif final du TP est de faire passer le robot par une suite de 4 ou 5 points de passage, comme pour une patrouille, avec un retour au point de d√©part. Si cela n'est pas d√©j√† fait, choisissez plusieurs points de passage faciles √† mesurer avec un m√®tre depuis le point de d√©part, avec un grand nombre d'obstacles sur le chemin. Si l'environnement a fortement chang√©, pensez √† enregistrer une nouvelle carte. üêç Les commandes pour naviguer jusqu'√† chaque point de passage seront des instructions dans un fichier Python. Le noeud navigation_scenario.py auquel vous pourrez acc√©der en tapant roscd ros4pro/src/nodes est une √©bauche de script Python pour y parvenir. üêç Compl√©tez ce fichier Python afin d'ex√©cuter le scenario et ainsi effectuer la patrouille. Pour ex√©cuter le sc√©nario lancez la navigation en arri√®re plan comme indiqu√© dans 2.5 Navigation puis lancez la commande rosrun ros4pro navigate_waypoints.py. üß≥ Challenge additionnel : Carry my luggage Challenge inspir√© de l'√©preuve \"Carry my luggage\" de la RoboCup @Home. Pour info, le r√©glement de la comp√©tition se trouve ici (mais √ßa n'apporte rien pour votre projet) : https://athome.robocup.org/wp-content/uploads/2019_rulebook.pdf üó∫Ô∏è Pr√©requis : avoir une carte repr√©sentative de l'environnement. ‚û°Ô∏è Phase 1 : Follow me Vous avez toute libert√© pour pr√©parer le d√©but de l'√©preuve (ex. comment faire que le robot soit bien localis√© d√®s le d√©but ?). Le robot part d'un point connu et doit suivre un humain qui va √† un endroit inconnu par le robot (mais √† l'int√©rieur de la carte). L'humain commence l'√©preuve en √©tant en face du robot √† une distance de 50 cm. Le robot doit suivre l'humain en maintenant une distance comprise entre 20cm minimum et 1m maximum. Pour √™tre valide, l'humain doit avoir un d√©placement non trivial : il ne va pas toujours tout droit et il fait varier sa vitesse de marche dans la limite du raisonnable. Distance minimum de marche demand√©e 4 m√®tres (mais vous √™tes libres de faire plus si √ßa vous arrange, √ßa n'impactera pas directement la note). Il faut obligatoirement que le robot traverse une porte. Lorsque l'humain est arriv√© √† sa destination, il s'arr√™te pendant une dur√©e d'au moins 3 secondes. Le robot doit alors comprendre que la phase 1 est termin√©e et passer √† la phase 2. ‚Ü©Ô∏è Phase 2 : Go home Le robot doit repartir et naviguer en totale autonomie jusqu'√† son point de d√©part. Sur le retour, vous rajouterez jusqu'√† : 1 obstacle statique sur son chemin de retour 1 obstacle dynamique (typiquement un humain qui lui coupe la route) 1 obstacle qui bloque compl√®tement le passage pr√©vu par le robot (il faut qu'il ait la possiblit√© d'arriver √† destination par un autre chemin) Si le robot arrive √† destination (√† +-20cm, +-15¬∞) la phase 2 est valid√©e. ‚ÜôÔ∏è Phase 3 : Dock Le robot doit chercher o√π se trouve sa base et s'y accoster. La position grossi√®re de la base est connue mais cette partie n'est valid√©e que si le robot r√©ussi un accostage pr√©cis sans contact : la distance entre le robot et la base soit √™tre sup√©riere √† 5mm et inf√©rieure √† 2cm. Vous avez toute libert√© pour choisir un objet qui repr√©sentera la base du robot. Un pot de peinture par exemple serait un choix pertinent (la sym√©trie radiale peut simplifier la d√©tection). Documentation FAQ des robots Documentation du TB3 (obsol√®te pour les commandes logicielles !) gmapping move_base üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"manipulation/edo/":{"url":"manipulation/edo/","title":"E.DO","keywords":"","body":"III. Robotique de manipulation avec E.DO La robotique de manipulation regroupe la manipulation d'objets avec des robots : des bras articul√©s √† 5 ou 6 axes, les robots SCARA (Selective Compliance Assembly Robot Arm), les robots cart√©siens (lin√©aires), les robots parall√®les ... Dans ce TP nous utilisons un robot E.DO du fabriquant Comau. Pr√©requis Lyc√©e et + Notions de commandes dans un terminal et d'adressage IP Le TP d'introduction Ce TP est compatible avec la simulation si vous n'avez pas d'E.DO : sauter directement au 2.3.bis Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Pr√©parer le mat√©riel üîå Votre E.DO comporte un connecteur Ethernet sur la base, c'est sur ce connecteur que vous pouvez connecter un c√¢ble r√©seau. Nous vous conseillons de brancher un c√¢ble RJ45 entre le robot et votre ordinateur pour commencer.Plus tard vous pourrez aussi vous connecter au point d'acc√®s Wifi du robot pour communiquer en sans-fil. A l'int√©rieur de la base se trouve une Raspberry Pi et une carte SD pr√©configur√©e par le fabriquant, elle est accessible en d√©vissant les trappes. Votre robot est donc compatible avec la plupart des proc√©dures de la FAQ Raspberry Pi. ‚ö†Ô∏è Le robot peut √™tre vendu avec plusieurs mod√®les d'effecteurs (= pince) ou bien sans effecteur du tout. Ce TP pr√©suppose que vous avez la version avec la pince √† 2 √©tats : ouvert et ferm√©. 1.1 Changer l'adresse IP de l'ordinateur ROS E.DO est livr√© pr√©configur√© avec son propre r√©seau IP. Veuillez √©diter la configuration r√©seau Ethernet (c√¢bl√©) de votre ordinateur Ubuntu en utilisant le gestionnaire r√©seau (network manager, en haut √† droite, vers l'horloge). Attribuez la configuration IP fixe suivante : Adresse IP statique 10.42.0.1 Masque de sous-r√©seau 24 ou bien 255.255.255.0 ‚ö†Ô∏è Attention, si vous souhaitez disposer aussi d'une connexion √† Internet, cela ne focntionnera plus √† cause du changement d'adresse IP. Vous pouvez cependant connecter votre ordinateur √† un r√©seau Wifi pour disposer aussi d'un acc√®s Internet. 1.2 Pinguer le robot Mettez votre robot sous-tension puis depuis un terminal Ubuntu tapez ping 10.42.0.49. Si tout va bien, un message apparaitra chaque seconde en indiquant le d√©lai de communication avec le robot en millisecondes (ms), jsuqu'√† ce que vous l'interompiez avec Ctrl + C : $ ping 10.42.0.49 PING 10.42.0.49 (10.42.0.49) 56(84) bytes of data. 64 bytes from 10.42.0.49: icmp_seq=1 ttl=114 time=3.7 ms 64 bytes from 10.42.0.49: icmp_seq=2 ttl=114 time=5.6 ms ^C Si un message d'erreur s'affichage √† la place du d√©lai en millisecondes, vous avez un probl√®me r√©seau. V√©rifiez l'√©tape 1.1 et que votre robot a bien d√©marr√© avec sa configuraiton r√©seau d'origine. Si vous voyez les d√©lais en millisecondes, vous pouvez continuer. 1.3. Configurer l'environnement ROS du robot üíª Avec SSH, connectez-vous √† la Raspberry Pi du robot : ssh edo@10.42.0.49 Le mot de passe par d√©faut est raspberry. Si la connexion est un succ√®s vous deviez voir un message d'information italien comandi tmux üáÆüáπ avec quelques explications √† propos de rostopic, que nous devez conna√Ætre puisqu nous l'd√©j√† abord√©e dans l'introduction. Si ce message n'apparait pas, v√©rifiez la configuration r√©seau du titre 1.1. ü§ñ Avec la commande nano, √©ditez ensuite le script ministarter du robot via SSH : nano ~/ministarter ü§ñ Descendez avec les fl√®ches du clavier pour identifier ces deux lignes : export ROS_MASTER_URI=http://192.168.12.1:11311 export ROS_IP=192.168.12.1 puis remplacez-les par ces 2 lignes modifi√©es : export ROS_MASTER_URI=http://10.42.0.49:11311 export ROS_IP=10.42.0.49 Quittez en tapant Ctrl-X, puis y et Entr√©e pour valider le nom de fichier et l'enregistrer. ü§ñ Tapez ensuite sudo reboot pour red√©marrer le robot et attendez qu'il soit de nouveau pr√™t. ‚ö†Ô∏è Il n'est n√©cessaire de configurer l'environnement ROS qu'une seule fois sur chaque nouveau robot, car la nouvelle configuration est ensuite enregistr√©e d√©finitivement sur la carte SD de la Raspberry Pi internet √† E.DO. 1.4 Configurer l'environnement ROS de l'ordinateur üìÄüíª Pour installer tous les packages ROS n√©cessaires sur votre ordinateur, ex√©cutez les commandes suivantes : roscd && cd src git clone https://github.com/eDO-community/eDO_control_v3.git git clone https://github.com/eDO-community/eDO_moveit.git git clone https://github.com/eDO-community/eDO_description.git git clone https://github.com/eDO-community/eDO_core_msgs.git pip install getkey numpy cd .. catkin_make Un script de d√©marrage nomm√© start.bash configure les variables d'environnement our vous √† chque fois que vous devrez travailler avec votre E.DO. Ce script ajoute un pr√©fixe jaune devant l'invite de commande pour savoir quel est le ROS master actuellement s√©lectionn√©. üìÄüíª Pour ex√©cutez ce script tapez : roscd edo_control ./start.bash Vous devriez voir appara√Ætre en pr√©fixe le ROS master de votre E.DO, avant de taper toute autre commande ROS, comme ci-dessous. Essayez un rostopic echo pour v√©rifier si tout va bien : [http://10.42.0.49:11311] me@workstation :~$ rostopic echo /machine_state -n1 current_state: 0 opcode: 0 Les valeurs current_state: 0 et opcode: 0 indiquent l'√©tat actuel du robot, dans un topic ROS d√©di√© nomm√© /machine_state. Si vous ne voyez pas ces deux valeurs en tapant la commande, vous pourriez avoir un probl√®me de r√©seau ou de configuration ROS. 2. Travaux pratique 2.1. Calibrer le robot Pourquoi calibrer ? Chacun des joints (axes moteurs) de votre robot poss√®de un encodeur : un capteur permettant au moteur de d√©terminer sa position angulaire courante (par exemple tourn√© √† 90¬∞ ou √† 150¬∞, etc). Plusieurs technologies d'encodeurs existent ayant chacune des avantages et inconv√©nients. Un des inconv√©nients des encodeurs d'E.DO est qu'ils ne peuvent mesurer que des d√©placements angulaires relatifs √† leur angle de d√©marrage, mais ne savent pas o√π est l'angle 0¬∞. Vous ne pouvez pas commander un moteur d'aller en position 90¬∞ s'il ne sait pas o√π et le 0¬∞. L'√©tape de calibration sert √† indique au robot o√π sont les angles de r√©f√©rence 0¬∞ de chacun de ses moteurs, un par un. Quand calibrer ? Vous devrez calibrer votre robot apr√®s chaque cycle de marche-arr√™t. La raison pour cela est que seule une partie des moteurs poss√®dent des freins, pour √©viter q'ils se d√©calibrent en √©tant √† l'arr√™t. En effet, si vos moteurs sont √† l'arr√™t et sans frein les emp√™chant de tourner, la gravit√© ou une action humaine pourrait les faire tourner malgr√© eux, sans qu'ils puissent enregistrer ces rotations puisqu'ils ne sont pas sous tension. Ils perdraient ainsi la trace de leur angle 0¬∞. Il faut donc calibrer √† chaque d√©marrage, ou alors s'assurer qu'aucun des moteurs ne bouge pendant que le robot n'est pas sous tension, ce qui est difficile √† s'assurer sans frein. Comment calibrer ? E.DO est livr√© avec une application Android, permettant nottamment de le calibrer. Le constructeur fournit une application Android permettant entre autre de calibrer le robot. Vous pourriez utiliser cette application mais puisque nous sommes sur un TP ROS, nous allons le faire avec ROS. üíª D√©marrez la proc√©dure de calibration en tapant : roslaunch edo_control calibrate.launch Vous devirez d'avord voir un message d'avertissement JOINT_UNCALIBRATED qui indique que les joints ne sont effectivement pas calibr√©s, et qui d√©taille la proc√©dure de calibration en anglais. Suivez cette proc√©dure jusqu'au bout. A chaque fois que vous voyez Calibrating joint X cela signifie que la proc√©dure va calibrer le joint X. Pour chaque joint appuyez sur les touches gauche et droite pour aligner physiquement les marqueurs d'alignement de chaque moteur. Une fois que votre joint est calibr√© appuyez sur Entr√©e pour passer au suivant. ‚ÑπÔ∏è La position 0¬∞ calibr√©e de chaque joint doit conduire petit-√†-petit votre robot √† se tenir droit comme un i. Si √† la fin de la calibration votre robot n'est pas droit comme un i pointant vers le plafond, vous vous √™tes tromp√© sur l'alignement d'un ou plusieurs moteurs, vous pouvez relancer la proc√©dure. La commande de calibration se ferme d'elle-m√™me lorsque tous les joints ont √©t√© calibr√©s. 2.2. D√©marrer le contr√¥le du robot et ouvrir/fermer la pince üíª Lancez le launchfile de contr√¥le du robot sur votre ordinateur : roslaunch edo_control control.launch Cela va activer les interfaces de communication suivant avec le robot : Le topic /joint_states, qui affiche tout l'√©tat des joints √† environ 90 Hz : positions angulaires, vitesses et torques Le serveur d'action /follow_joint_trajectory qui permettra d'ex√©cuter des trajectoires avec MoveIt un peu plus tard dans le TP Le topic /open_gripper qui permet d'ouvrir et de ferme la pince Tester la commande de la pince : Sur ce dernier topic, vous pouvez publier true pour ouvrir la pince (course maximale de 60 mm) et false pour la fermer (course minimale de 0 mm). üíª Conservez le contr√¥le du robot d√©marr√© dans un terminal. Dans un autre terminal testez l'ouverture de la pince avec rostopic : rostopic pub /open_gripper std_msgs/Bool \"data: true\" Remplacez true par false pour refermer la pince ! 2.3. D√©marrer MoveIt pour planifier des trajectoires (avec un robot r√©el) üíª Conservez le contr√¥le du robot d√©marr√© dans un terminal. Tapez la commande suivante pour d√©marrer MoveIt avec E.DO : roslaunch edo_moveit demo.launch Vous devriez voir l'interface de MoveIt qui d√©marre dans RViz similairement √† la capture d'√©cran ci-dessous : Utilisez ensuite les outils MoveIt pour planifier et ex√©cuter des trajectoires sur votre robot : Dans la zone Motion Planning en bas √† gauche de RViz, s√©lectionnez l'onglet Planning Dans la vue 3D du robot, bougez la balle bleue correspondant √† l'effecteur (la pince) quelque part dans l'espace autour du robot Le robot orange correspond √† la configuration cible que vous allez demander d'atteindre √† votre robot Cliquez sur Plan and Execute pour planifier une trajectoire vers cette cible et l'ex√©cuter sur le robot r√©el Si vous ne voyez pas votre vrai robot bouger, v√©rifier dans le terminal √† partir duquel vous avez d√©marr√© MoveIt : il se peut que des messages d'erreurs apparaissent en rouge pour vous aider √† localiser le probl√®me. Ce TP s'arr√™te ici mais il est possible d'ajouter des √©l√©ments de collision (obstacles) que le planificateur de trajectoire contournera. Pour ce faire vous pouvez charger un fichier comprenant les coordonn√©es des obstacles dans l'onglet Scene Objects de la zone Motion Planning ou bien pour le faire via Python vous pouvez vous inspirer de la partie de d√©claration des obstacles de cet autre TP. Que faire si les freins des moteurs s'activent al√©atoirement pendant le mouvement Si vos trajectoires sont interrompues par des activations intempestives des freins (suivies d'une d√©sactivation), v√©rifiez d'abord que votre robot a √©t√© calibr√© avec pr√©cision. Si c'est le cas, cela signifie que votre robot subit trop de force pour ex√©cuter le mouvement que vous lui demander. Le syst√®me de s√©curit√© active donc les freins. Ralentissez les vitesses des joints dans kinematics.yaml ou bien all√©ger le poids de votre robot ou bien demandez une position cible qui fera moins forcer les moteurs. 2.3.bis. D√©marrer MoveIt pour planifier des trajectoires (avec un robot simul√©) üìÄüíª Pour installer tous les packages ROS n√©cessaires sur votre ordinateur et travailler avec un robot simul√©, il ne faut pas avoir configur√© l'environnement d√©crit en 1.4, ex√©cutez directement les commandes suivantes : roscd && cd src git clone https://github.com/eDO-community/eDO_control_v3.git git clone https://github.com/eDO-community/eDO_moveit.git git clone https://github.com/eDO-community/eDO_description.git git clone https://github.com/eDO-community/eDO_core_msgs.git roscd && catkin_make source ~/.bashrc üíª Enfin, cette commande ci-dessous lancera MoveIt avec un E.DO simul√© : roslaunch edo_moveit demo.launch simulated:=true Vous devriez voir l'interface de MoveIt qui d√©marre dans RViz similairement √† la capture d'√©cran ci-dessous : Utilisez ensuite les outils MoveIt pour planifier et ex√©cuter des trajectoires sur votre robot simul√© : Dans la zone Motion Planning en bas √† gauche de RViz, s√©lectionnez l'onglet Planning Dans la vue 3D du robot, bougez la balle bleue correspondant √† l'effecteur (la pince) quelque part dans l'espace autour du robot Le robot orange correspond √† la configuration cible que vous allez demander d'atteindre √† votre robot Cliquez sur Plan and Execute pour planifier une trajectoire vers cette cible et l'ex√©cuter sur le robot simul√© Ce TP s'arr√™te ici mais il est possible d'ajouter des √©l√©ments de collision (obstacles) que le planificateur de trajectoire contournera. Pour ce faire vous pouvez charger un fichier comprenant les coordonn√©es des obstacles dans l'onglet Scene Objects de la zone Motion Planning ou bien pour le faire via Python vous pouvez vous inspirer de la partie de d√©claration des obstacles de cet autre TP. üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"manipulation/ergo-jr/":{"url":"manipulation/ergo-jr/","title":"Poppy Ergo Jr","keywords":"","body":"III. Robotique de manipulation avec Poppy Ergo Jr La robotique de manipulation regroupe la manipulation d'objets avec des robots. Dans ce TP nous utilisons un robot opensource Poppy Ergo Jr qui peut √™tre 100% imprim√© en 3D √† la maison ou √† l'√©cole. Pr√©requis Lyc√©e et + Notions de commandes dans un terminal et d'adressage IP Notions de Python Notions de g√©om√©trie 3D Le TP d'introduction Ce TP est compatible avec la simulation si vous n'avez pas de Poppy Ergo Jr : sauter directement au 2.3.bis Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Pr√©parer le mat√©riel (avec un robot r√©el) 1.1. Pr√©parer la carte SD üì• Pour √©viter tout probl√®me li√© √† une pr√©c√©dente utilisation du robot, commencez par flasher la carte SD fournie avec l'image ROS en utilisant la proc√©dure vue lors de l'introduction. Pendant cette √©tape, assemblez votre robot en parrall√®le. 1.2. Assembler Poppy Ergo Jr üîß Pour assembler votre robot, veuillez suivre le guide d'assemblage, en suivant les √©tapes fa√Ætes pour ROS le cas √©ch√©ant ; et en comparant minutieusement chaque pi√®ce aux photos pour v√©rifier leur orientation car il est tr√®s facile d'assembler ce robot √† l'envers m√™me s'il a au final la m√™me allure. Si votre robot est pr√©-assembl√©, recommencez √† minima toutes les configurations des moteurs qui pourraient √™tre incorrectes. ‚úÖ V√©rification : Pour v√©rifier que votre assemblage est correct, connectez-vous en SSH au robot (si ce n'est pas d√©j√† fait) puis ex√©cutez : ssh pi@poppy.local # password raspberrypi # Effacer √©ventuellement l'ancienne cl√© ECDSA si vous avez un message d'erreur roslaunch poppy_controllers control.launch Vous devriez voir appara√Ætre Connection successful. Si l'erreur \"Connection to the robot can't be established\" est affich√©e, alors votre robot n'a pas √©t√© mont√© correctement. La suite de ce message d'erreur indique quel(s) moteur(s) pose(nt) probl√®me pour vous aider √† le r√©soudre. Fermez avec Ctrl+C puis utilisez de nouveau Poppy Configure si un moteur est mal configur√©. Remarque : Si vos moteurs clignotent en rouge : votre code a cr√©√© une collision et ils se sont mis en alarme. Pour d√©sactiver l'alarme il faut d√©brancher et rebrancher l'alimentation, ce qui fera aussi red√©marrer le robot 2. Travaux pratiques 2.1. Comprendre la repr√©sentation d'un robot ROS Un robot int√©gr√© √† ROS est compos√© d'au minimum : un descripteur URDF un contr√¥leur qui g√®re les E/S avec le robot 2.1.1. Comprendre le descripteur URDF üíªüìÄ Clonez le package ROS Poppy Ergo Jr Description sur votre PC, il contient le fichier de description URDF du robot : git clone https://github.com/poppy-project/poppy_ergo_jr_description.git üíª Compilez votre workspace puis sourcez votre .bashrc, enfin rdv dans le dossier urdf de ce package, puis ex√©cutez la commande urdf_to_graphiz qui convertit un fichier URDF en repr√©sentation graphique dans un PDF : sudo apt install liburdfdom-tools roscd poppy_ergo_jr_description/urdf urdf_to_graphiz poppy_ergo_jr.urdf Ouvrez le PDF obtenu puis d√©terminez : Que repr√©sentent les rectangles ? Que repr√©sentent les bulles ? Que repr√©sentent les fl√®ches et surtout les valeurs xyz et rpy associ√©es ? 2.1.2. Comprendre les E/S du contr√¥leur ü§ñ Le contr√¥leur se trouve d√©j√† sur le robot. Vous pouvez directement vous connecter au robot et le d√©marrer : ssh pi@poppy.local # password raspberrypi # Effacer √©ventuellement l'ancienne cl√© ECDSA si vous avez un message d'erreur roslaunch poppy_controllers control.launch üíª Sur votre PC, fa√Ætes pointer votre ROS_MASTER_URI sur poppy.local. Rappel : nano ~/.bashrc # Pour changer votre ROS_MASTER_URI source ~/.bashrc # Pour charger votre .bashrc et donc le nouveau master 2.1.2.a. Topics du robot ‚úç Avec l'utilitaire rostopic, lister les topics disponibles puis consultez celui qui d√©crit l'√©tat courant des joints, en particulier : Quel est son nom ? Quel est le type de message qu'il transmet ? A quelle fr√©quence (en Hertz) est-ce qu'il met √† jour l'√©tat des joints ? 2.1.2.b. Services du robot ‚úç Avec les utilitaires rosservice et rossrv, listez les services disponibles puis consultez celui qui met le robot en mode compliant. En particulier : Quel est le nom de topic du service mettant le robot en compliant ? Quel est le type de ce service ? Consultez le d√©tail des champs. Quels sont les champs de la requ√™te de ce service ? Consultez le d√©tail des champs. Quels sont les champs de la r√©ponse de ce service ? Appelez ce service pour activer et d√©sactiver le mode compliant et essayez de faire bouger votre robot √† la main √† chaque fois. Que d√©duisez-vous de la signification du mode compliant ? Conseil : aidez-vous de l'autocompl√©tion avec la touche 2.1.2.c. Tracer la courbe des positions des moteurs en temps r√©el Mettez votre robot en mode compliant, d√©marrez rqt_plot pour tracer les positions des 6 moteurs ... bougez les moteurs √† la main et v√©rifiez que rqt_plot actualise la courbe en temps r√©el. 2.2. Cin√©matique, et planification avec MoveIt dans RViz 2.2.1. D√©marrer avec MoveIt üíªüìÄ Installez MoveIt puis clonez le package ROS Poppy Ergo Jr MoveIt Configuration, il contient le code n√©cessaire pour que ce robot fonctionne avec MoveIt : sudo apt install ros-melodic-moveit git clone https://github.com/poppy-project/poppy_ergo_jr_moveit_config.git üíª Compilez votre workspace puis sourcez votre .bashrc. D√©marrez MoveIt avec roslaunch avec le param√®tre fake_execution √† false pour se connecter au vrai robot : roslaunch poppy_ergo_jr_moveit_config demo.launch fake_execution:=false gripper:=true Rviz doit d√©marrer avec un Poppy Ergo Jr en visu. Note : si vous devez passer en simulation √† ce moment suite √† un d√©faut mat√©riel, pensez √† changer votre ROS_MASTER_URI pour localhost puis mettre simplement fake_execution √† true. 2.2.2. Planification üíª Dans l'onglet Planning, section Query puis Planning group, s√©lectionnez le groupe arm_and_finger, bougez le goal (la sph√®re 3D bleue) en position et en orientation puis cliquez sur Plan. ‚úç Trois repr√©sentations 3D de robots se superposent, d√©terminez le r√¥le de chacun d'entre eux en testant √©galement la fonctionnalit√© Plan and Execute : Que d√©signe le robot gris parfois mobile mais lent ? Que d√©signe le robot orange (fixe) ? Que d√©signe le robot gris qui r√©p√®te infiniment un mouvement rapide ? Dans RViz, activer l'affichage du mod√®le de collision dans Displays, Scene Robot, Show Robot Collision, quelle est la forme de ce mod√®le utilis√© par OMPL pour simplifier le calcul des collisions ? 2.2.3. Planning groups üíª‚úç Testez √©galement le groupe arm en plus du premier arm_and_finger et lancez des planifications de mouvement pour tester : Quelle est la diff√©rence entre ces 2 groupes ? Quel est le groupe pour lequel le goal est le plus facilement manipulable ? Pourquoi ce groupe est-il plus facilement manipulable que l'autre ? D√©duisez-en ce que d√©signe exactement un planning group 2.2.4. Transformations tf Nous allons visualiser et interroger l'arbre des transformations nomm√© tf üíª‚úç D√©marrer MoveIt puis dans un autre terminal lancer rosrun tf2_tools view_frames.py. Un fichier PDF nomm√© frames.pdf a √©t√© cr√©√© : les frames (rep√®res g√©om√©triques) qu'ils contient sont les m√™mes que ceux dessin√©s par Rviz en rouge-vert-bleu. Comment est nomm√© le rep√®re de base ? Comment sont nomm√©s les deux effecteurs finaux possibles ? La commande rosrun tf2_tools echo.py frameA frameB renvoie la transformation actuelle de frameB dans frameA. Modifiez cette commande pour d√©terminer quelle est la position actuelle d'un des effecteurs dans le rep√®re de base. Ses coordonn√©es peuvent vous servir par la suite, pour les d√©finir comme cible √† atteindre. 2.3. Ecrire un noeud Python ROS pour l'Ergo Jr 2.3.1. Cr√©er un nouveau package et un nouveau noeud Python üíª Nous allons cr√©er un nouveau package ROS nomm√© ros4pro_custom sur votre laptop de d√©veloppement, qui contient notre code: cd ~/catkin_ws/src catkin_create_pkg ros4pro_custom # Cette commande cr√©√© le package mkdir -p ros4pro_custom/src # On cr√©√© un dossier src dans le package touch ros4pro_custom/src/manipulate.py # On cr√©√© un noeud Python \"manipulate.py\" chmod +x ros4pro_custom/src/manipulate.py # On rend ce noeud ex√©cutable pour pouvoir le lancer avec rosrun üíªüêç Bien que vous devriez avoir compris comment cr√©er un noeud ROS en Python dans les tutoriels d'introduction, voici un rappel de noeud ROS minimal qui boucle toutes les secondes en Python : #!/usr/bin/env python import rospy rospy.init_node('ros4pro_custom_node') rate = rospy.Rate(1) while not rospy.is_shutdown(): rospy.loginfo(\"Hello world from our new node!\") rate.sleep() üíª Compilez votre workspace puis sourcez votre .bashrc. Ex√©cutez votre noeud avec rosrun : cd ~/ros_ws catkin_make rosrun ros4pro_custom manipulate.py Votre noeud doit afficher un message toutes les secondes, vous pouvez le tuer avec Ctrl+C. Nous allons ajouter du code petit √† petit. Attention √† l'ajouter au bon endroit pour cr√©er un script coh√©rent. 2.3.2. Planifier et ex√©cuter des mouvements avec MoveIt Le MoveGroupCommander est le commandeur de robot de MoveIt, il suffit de lui indiquer quel est le nom du groupe √† commander puis donner une cible et appeler la fonction go() pour l'atteindre en √©vitant les obstacles. Cette cible peut √™tre dans l'espace cart√©sien ou dans l'espace des joints : 2.3.2.a. üêç Cible dans l'espace cart√©sien from moveit_commander.move_group import MoveGroupCommander commander = MoveGroupCommander(\"arm_and_finger\") commander.set_pose_target([0.00, 0.079, 0.220] + [0.871, -0.014, 0.079, 0.484]) commander.go() Les coordonn√©es cart√©siennes de la cible sont les coordonn√©es de l'effecteur (c√†d moving_tip pour le groupe arm_and_finger ou bien fixed_tip pour le groupe arm) dans le rep√®re base_link, exprim√©es sous la forme x, y, z, qx, qy, qz, qw. 2.3.2.b. üêç Cible dans l'espace des joints (sans √©vitement de collision) Il est √©galement possible de d√©finir une cible dans l'espace des joints en fournissant une liste des 6 angles moteurs dans ce cas il n'y a pas d'√©vitement de collision: commander.set_joint_value_target([0, 0, 0, 0, 0, 0]) commander.go() 2.3.2.c. ‚úç Mise en pratique A l'aide des fonctions et commandes vues en 3.1.4. et 4.2.1., v√©rifiez que vous savez prendre les coordonn√©es cart√©siennes courante et les d√©finir comme cible puis l'atteindre A l'aide des fonctions et commandes vues en 2.2.1. et 4.2.2., v√©rifiez que vous savez prendre les positions des joints courantes et les d√©finir comme cible puis l'atteindre A l'aide du mode compliant, prendre les coordonn√©es cart√©siennes de l'effecteur et et les positions des joints pour deux configurations diff√©rentes du robot A et B (e.g. effecteur vers le haut et effecteur vers le bas) Fa√Ætes bouger le robot infiniement entre les cibles cart√©siennes A et B, nous y ajouterons des obstacles plus tard 2.3.3. D√©clarer des obstacles Afin que les algorithmes de planification de trajectoire d'OMPL (tels que RRTConnect) puissent √©viter les obstacles, il est n√©cessaire que MoveIt ait connaissance de leur position et leur forme. Il est possible d'utiliser une cam√©ra de profondeur (aka cam√©ra RGB-D, mais nous n'en avons pas ici) ou bien d√©clarer les objets depuis le code Python gr√¢ce √† l'interface PlanningSceneInterface. üêç Par exemple, ce code d√©clarer une boite de c√©r√©ales comme objet de collision en sp√©cifiant sa position et son orientation sous forme d'objet PosteStamped ainsi que sa taille en m√®tres : from geometry_msgs.msg import PoseStamped from moveit_commander.planning_scene_interface import PlanningSceneInterface scene = PlanningSceneInterface() rospy.sleep(1) ps = PoseStamped() ps.header.frame_id = \"base_link\" ps.pose.position.x = 0.15 ps.pose.position.y = 0 ps.pose.position.z = 0.15 ps.pose.orientation.w = 1 scene.add_box(\"boite_de_cereales\", ps, (0.08, 0.24, 0.3)) rospy.sleep(1) Les coordonn√©es des objets de collision sont donn√©es sous la forme d'objet PoseStamped incluant la position, l'orientation et le rep√®re frame_id, et la taille est donn√©e sous forme de tuple (longueur, largeur, hauteur). Modifier l'obstacle \"boite_de_cereales\" propos√© en exemple afin qu'un obstacle viennent perturber le mouvement entre les deux poses de votre programme en 3.2.2. et v√©rifiez que MoveIt contourne toujours ces obstacles sans jamais les collisionner. Note: Accessoirement, il est possible d'attacher et de d√©tacher les objets de collision au robot, ceci permet par exemple de simuler la saisie et la d√©pose d'objets physique dans RViz avec MoveIt. cf la documentation MoveIt pour Python ou m√™me le code de PlanningSceneInterface 2.3.4. Enregistrer et rejouer un mouvement de pick-and-place R√©f√©rez-vous √† la documentation du Poppy Controllers afin d'enregistrer et de rejouer des mouvements en utilisant la compliance du robot. Fa√Ætes quelques essais avec plusieurs mouvements qui s'alternent pour bien comprendre le fonctionnement. Enregistrez un mouvement de pick-and-place pour attraper un cube et le d√©poser √† un autre endroit 2.4. R√©cup√©rer les images de la cam√©ra en Python üíªüìÄ Avec la carte SD ROS, l'image de la cam√©ra est accessible par appel d'un service d√©di√©. Nous aurons besoin de r√©cup√©rer le package Poppy Controllers et le compiler d'abord : cd ~/ros_ws/src git clone https://github.com/poppy-project/poppy_controllers.git # Nous aurons besoin de ce package cd ~/ros_ws/ catkin_make source ~/.bashrc üêç Testez ce code pour v√©rifier que vous pouvez r√©cup√©rer l'image en Python via le service ROS /get_image fourni par le contr√¥leur. import cv2 from poppy_controllers.srv import GetImage from cv_bridge import CvBridge get_image = rospy.ServiceProxy(\"get_image\", GetImage) response = get_image() bridge = CvBridge() image = bridge.imgmsg_to_cv2(response.image) cv2.imshow(\"Poppy camera\", image) cv2.waitKey(200) Cette image peut ensuite √™tre trait√©e par un r√©seau de neurones, une fonction OpenCV, etc ... Documentation Tutoriaux de MoveIt Code du MoveIt Commander Python Documentation de l‚ÄôAPI MoveIt en Python Documentation de Poppy Ergo Jr üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"manipulation/sawyer/":{"url":"manipulation/sawyer/","title":"Sawyer","keywords":"","body":"III. Robotique de manipulation avec Sawyer La robotique de manipulation regroupe la manipulation d'objets avec des robots : des bras articul√©s √† 5 ou 6 axes, les robots SCARA (Selective Compliance Assembly Robot Arm), les robots cart√©siens (lin√©aires), les robots parall√®les ... Dans ce TP nous utilisons un robot Sawyer du fabriquant Rethink Robotics. Pr√©requis BAC+2 et + Aisance en Python et commandes dans un terminal Aisance en g√©om√©trie 3D Le TP d'introduction Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Documentation 1.1. Les liens Tutoriaux de MoveIt Code du MoveIt Commander Python Documentation de l‚ÄôAPI MoveIt en Python Tutoriaux du SDK Sawyer 1.2. Le plus important Voici les quelques fonctions les plus importantes traduites depuis la documentation. 1.2.1. D√©clarer un commandeur de robot commander = MoveGroupCommander(\"right_arm\") Ce commandeur est celui qui identifie notre robot et permettra d'en prendre le contr√¥le. 1.2.2. Ex√©cuter un mouvement vers une cible D√©finir une cible, dans l'espace des joints : commander.set_joint_value_target([-1.0, -2.0, 2.8, 1.5, 0.1, -0.3, 3.0]) Les 7 valeurs sont les angles cibles des 7 joints en radians Attention : les cibles dans l'espace des joints n'auront pas d'√©vitement de collisions Ou bien d√©finir une cible dans l'espace cart√©sien : commander.set_pose_target([0.5, 0.05, 1.1, 0, 0, 0, 1]) Les 7 valeurs sont la position et l'orientation [x, y, z, qx, qy, qz, qw] cible de l'effecteur dans le rep√®re base 1.2.3. Planifier & ex√©cuter le mouvement vers la cible La fonction \"go\" d√©clenche le calcul de trajectoire et l'ex√©cution instantan√©e si le calcul a r√©ussi : commander.go() 1.2.4. Ex√©cuter une trajectoire cart√©sienne Par opposition √† la cible cart√©sienne, dans cet exemple au lieu de ne d√©finir qu'une cible finale, on demande √† MoveIt de suivre une trajectoire rectiligne dans l'espace cart√©sien. Cette trajectoire est pr√©calcul√©e en entier gr√¢ce √† la fonction commander.compute_cartesian_path([pose1, pose2]), resolution, jump) o√π : [pose1, pose2] est la liste des points √† suivre de mani√®re rectiligne de type geometry_msgs.Pose resolution est la distance en m√®tre entre 2 points cart√©siens g√©n√©r√©s (par exemple 0.01 pour g√©n√©rer un point tous les centim√®tres) jump est le seuil maximal autoris√© au del√† duquel un saut trop important entre 2 positions angulaires en radians ne sera pas ex√©cut√©e car il demanderait une vitesse excessive. jump est la somme des seuils sur tous les joints (par exemple 3.14). La fonction commander.compute_cartesian_path(...) renvoie : trajectory: la trajectoire cart√©sienne calcul√©e ratio: Un ratio entre 0 et 1 indique la quantit√© de la trajectoire qui a pu √™tre calcul√©e avec succ√®s sans g√©n√©rer de saut. Un ratio inf√©rieur √† 0.95 signifie probablement que la trajectoire est inutilisble car elle ne suit pas le chemin demand√©. Par exemple, √©tant donn√©es 2 points p1 et p2 de type geometry_msgs.Pose, cet appel est valide : trajectory, ratio = commander.compute_cartesian_path([pose1, pose2]), 0.01, 3.14) Enfin, ex√©cuter la trajectoire, uniquement si le ratio indique au moins 95% de succ√®s : commander.execute(trajectory) 1.2.5. D√©finir des objets de collision Lorsqu'on ajoute des objets de collision, les appels √† go() dans l'espace cart√©sien planifieront, si possible, une trajectoire en √©vitant les objets d√©clar√©s comme objets de collision. La sc√®ne de planification est notre interface pour ajouter ou supprimer des objets de collision : scene = PlanningSceneInterface() On peut ensuite effectuer les ajouts ou suppressions. Par exemple, on ajoute un objet de collision cubique de taille 10x8x2 cm √† la position [1.2, 0.5, 0.55] et avec l'orientation [0, 0, 0, 1] (= rotation identit√©) dans le rep√®re base: ps = PoseStamped() ps.header.frame_id = \"base\" ps.pose.position.x = 1.2 ps.pose.position.y = 0.5 ps.pose.position.z = 0.55 ps.pose.orientation.x = 0 ps.pose.orientation.y = 0 ps.pose.orientation.z = 0 ps.pose.orientation.w = 1 scene.add_box(\"ma_boite\", list_to_pose_stamped2([[1.2, 0.5, 0.55], [0, 0, 0, 1]]), (0.10, 0.08, 0.02)) Les objets de collision apparaissent en vert dans RViz s'ils sont d√©finis correctement. Note: apr√®s une modification de la sc√®ne, est g√©n√©ralement utile de faire une pause rospy.sleep(1) afin de laisser le temps √† la sc√®ne d'√™tre mise √† jour sur tout le syst√®me avant toute nouvelle planification de trajectoire 2. Travaux pratiques 2.1. Utiliser MoveIt dans le visualisateur Rviz Avec roslaunch, lancer sawyer_moveit.launch provenant du package sawyer_moveit_config: roslaunch sawyer_moveit_config sawyer_moveit.launch Via l‚Äôinterface graphique, changer l‚Äôorientation et la position de l‚Äôeffecteur puis demander √† MoveIt de planifier et ex√©cuter une trajectoire pour l‚Äôatteindre. Cochez la bonne r√©ponse : Cette m√©thode permet-elle de d√©finir une cible dans l‚Äôespace : ‚óª cart√©sien ‚óª des joints Trois robots semblent superpos√©s en 3D, quelles sont leurs diff√©rences : Le robot orange est ... ? Le robot rapide est ... ? Le robot lent est ... ? Utilisez rostopic echo pour afficher en temps r√©el les messages du topic /robot/joint_states. Ex√©cutez un mouvement et observer les valeurs changer. Que repr√©sente le topic /robot/joint_states ? Indiquez comment se nomment les 7 joints de Sawyer depuis la base jusqu‚Äô√† l‚Äôeffecteur : Premier joint : Deuxi√®me joint : Troisi√®me joint : Quatri√®me joint : Cinqui√®me joint : Sixi√®me joint : Dernier joint : 2.2. Utiliser MoveIt via son client Python Dans le package ros4pro, ouvrir le n≈ìud manipulate.py. Rep√©rez les 3 exemples : d'ex√©cution d'une trajectoire cart√©sienne de planification vers une cible cart√©sienne de planification vers une cible dans l'espace des joints Durant la suite du TP, nous d√©marrerons notre programme de manipulation avec le launchfile manipulate.launch du package ros4pro qui fonctionne par d√©faut en mode simul√©, √† savoir : roslaunch ros4pro manipulate.launch Celui-ci d√©marre automatiquement manipulate.py, il est donc inutile de le d√©marrer par un autre moyen. 2.2.1. Modifier la sc√®ne de planification La sc√®ne repr√©sente tout ce qui rentre en compte dans les mouvements du robot et qui n‚Äôest pas le robot lui-m√™me : les obstacles et/ou les objets √† attraper. Ces √©l√©ments sont d√©clar√©s √† MoveIt comme des objets de collision (d√©tach√©s du robot ou attach√©s c‚Äôest-√†-dire qu‚Äôils bougent avec lui). Prenez les mesures du feeder puis d√©clarez-les comme objets de collision dans votre noeud Python via l‚Äôinterface PlanningSceneInterface. Compl√©tez le TODO associ√© √† la question 3.2.1. dans manipulate.py. Planifiez et ex√©cutez un mouvement RViz pour v√©rifier que les collisions sont √©vit√©es. D√©clarez un nouvel obstacle qui entrave forc√©ment le chemin du robot et v√©rifiez. V√©rifiez ce qu‚Äôil se passe lorsque le planner ne trouve aucune solution. 2.2.2. Effectuer un pick-and-place avec un cube simul√© (optionnel) Nous consid√©rons un cube situ√© √† la pose ·µá·µÉÀ¢·µâPÍúÄ·µ§‚ÇÜ‚Çë ·µá·µÉÀ¢·µâPÍúÄ·µ§‚ÇÜ‚Çë_·µ•‚Çë·µ£‚Çú = [[0.32, 0.52, 0.32], [1, 0, 0, 0]], ce qui correspond exactement √† l'emplacement entour√© en vert sur le feeder. Pour l‚Äôapproche, on positionnera le gripper 18cm au dessus du cube le long de son axe z. Sachant cela, d√©duire la matrice de transformation ·∂ú·µò·µá·µâP‚Çâ·µ£·µ¢‚Çö‚Çö‚Çë·µ£ en notation [[x, y, z], [x, y, z, w]] ? Exprimez ·µá·µÉÀ¢·µâP‚Çâ·µ£·µ¢‚Çö‚Çö‚Çë·µ£ en fonction de la pose ·µá·µÉÀ¢·µâPÍúÄ·µ§‚ÇÜ‚Çë du cube dans le rep√®re base via une multiplication matricielle Ensuite, dans votre code : Inventez un cube en simulation dans votre code √† la pose ·µá·µÉÀ¢·µâPÍúÄ·µ§‚ÇÜ‚Çë_·µ•‚Çë·µ£‚Çú = [[0.32, 0.52, 0.32], [1, 0, 0, 0]] c‚Äôest-√†-dire √† la surface du feeder. Pour ce faire utilisez : l‚Äôinterface PlanningSceneInterface pour ajouter, supprimer un cube ou l‚Äôattacher √† l‚Äôeffecteur right_gripper_tip du robot TransformBroadcaster pour publier la frame tf nomm√©e cube au centre du cube √† attraper 2.2.3. G√©n√©rer les 4 trajectoires du pick-and-place Pour rappel, voici les 4 √©tapes d'un pick pour attraper et relacher le cube simul√© : 1. trajectoire d‚Äôapproche : aller sans collision √† `·µá·µÉÀ¢·µâPÍúÄ·µ§‚ÇÜ‚Çë_·µ•‚Çë·µ£‚Çú` c'est √† dire 18cm au dessus du cube sur son axe z (axe bleu) 2. trajectoire de descente : suivre une trajectoire cart√©sienne de 50 points descendant le long de l'axe z pour atteindre `·µá·µÉÀ¢·µâPÍúÄ·µ§‚ÇÜ‚Çë_·µ•‚Çë·µ£‚Çú` avec l‚Äôeffecteur `right_gripper_tip`. Puis fermer l'effecteur. 3. trajectoire de retraite : retourner au point d‚Äôapproche par une trajectoire cart√©sienne 4. trajectoire de d√©pose : si le cube a bel-et-bien √©t√© attrap√© avec succ√®s, aller sans collision au point de d√©pose `·µá·µÉÀ¢·µâPÍúÄ‚Çî‚Çë‚Çö‚Çí‚Çõ‚Çë = [[0.5, 0, 0.1], [0.707, 0.707, 0, 0]]` Dans manipulate.py, les deux derni√®res trajectoires sont incompl√®tes : retraite et d√©pose nomm√©es release et place. Compl√©ter les 2 TODO associ√©s √† la question 3.2.3 dans manipulate.py V√©rifier que votre pick-and-place a l'air correct en simulation d'abord et que vous distinguez correctement les 4 √©tapes du pick-and-place. 2.3. Ex√©cutez le pick-and-place sur le Sawyer r√©el Changez votre rosmaster pour celui de Sawyer :export ROS_MASTER_URI=http://021608CP00013.local:11311 Le centre du cube ·µá·µÉÀ¢·µâPÍúÄ·µ§‚ÇÜ‚Çë_·µ•‚Çë·µ£‚Çú pr√©c√©dent est celui de la zone verte sur le feeder. V√©rifiez que votre pick-and-place fonctionne avec 1 seul cube √† l‚Äôemplacement vert. Pour commander le robot r√©el modifiez le param√®tre simulate : roslaunch ros4pro manipulate.launch simulate:=false Vous devriez constater que votre pick-and-place fonctionne parfois et qu'il √©choue dans certaines situations. 2.4. Pr√©parer le pipeline du scenario final (optionnel) Ajoutez √† manipulate.py le code n√©cessaire pour votre scenario final : Prise de photo (scan) : positionner l‚Äôeffecteur de telle mani√®re que ¬´ right_hand_camera ¬ª se trouve √† la verticale du feeder R√©cup√©rer l‚Äôimage rectifi√©e sur le topic d√©di√© : image_view peut aider √† visualiser l‚Äôimage pour d√©boguer dans un terminal pour r√©cup√©rer l‚Äôimage en Python, impl√©mentez un Subscriber. Cette photo sera envoy√©e au r√©seau de neurones lorsqu‚Äôil sera impl√©ment√© Pick-and-Place successifs : Effectuez 3 pick-and-place successifs sans intervention humaine de 3 cubes dont la position est connue √† l‚Äôavance. Ces positions seront retourn√©es par le r√©seau de neurones lorsqu‚Äôil sera impl√©ment√© Si vous n‚Äôavez pas de collision de mani√®re reproductible, vous pouvez acc√©l√©rer les vitesses dans sawyer_moveit_config/config/joint_limits.yaml (pas les acc√©l√©rations) 2.5. Pr√©venir les √©checs de planification Le path-planning √©tant r√©alis√© pendant l‚Äôex√©cution et non pr√©calcul√©, il est possible que MoveIt ne trouve aucune solution de mouvement. Pour rem√©dier √† cela, plusieurs pistes s‚Äôoffrent √† nous : R√©essayer encore et encore ‚Ä¶ b√™tement, jusqu‚Äô√† un potentiel succ√®s Planifier toute les trajectoires d‚Äôun coup avant ex√©cution, si l‚Äôune ne peut √™tre calcul√©e, reg√©n√©rer la pr√©c√©dente et recommencer. Cela montre ses limites : et si le robot n‚Äôest pas √† l‚Äôemplacement attendu entre deux trajectoires, par exemple si l‚Äôop√©rateur l‚Äôa boug√© manuellement ? Fournir une ¬´ seed ¬ª √† l‚ÄôIK ou au path-planning. Cette solution est approfondie ci-apr√®s : MoveIt poss√®de un syst√®me de calcul de la g√©om√©trie directe et inverse, respectivement via les services /compute_fk et /compute_ik. Observer les types de service (rosservice info) et le contenu de la requ√™te (rossrv show) puis appeler ces deux services sur ces deux exemples : Calculer la FK pour les angles -œÄ/2, œÄ/2, -œÄ/2, 0, 0, 0, 0. Donner le r√©sultat au format [[x, y, z,], [qx, qy, qz, qw]] Calculer l‚ÄôIK pour la position d‚Äôeffecteur [[0.5, 0, 0.1], [0.707, 0.707, 0, 0]]. Donner le r√©sultat au format [angle1, angle2, angle3, angle4, angle5, angle6, angle7] R√©ex√©cutez le m√™me calcul d‚ÄôIK sur la m√™me position d‚Äôeffecteur, une seconde puis une troisi√®me fois. Observez que le r√©sultat est diff√©rent. Pourquoi ? Fournissez une ¬´ seed ¬ª au format [angle1, angle2, angle3, angle4, angle5, angle6, angle7] de votre choix √† l‚ÄôIK pour influencer le r√©sultat. Quelle seed proposez-vous pour maximiser les chances de succ√®s du path-planning ? üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"perception/opencv/":{"url":"perception/opencv/","title":"OpenCV","keywords":"","body":" IV. Perception avec OpenCV Introduction √† OpenCV Ouverture d'une image Seuil sur la couleur D√©tection des cubes Int√©gration avec ROS IV. Perception avec OpenCV Le domaine de \"Computer Vision\" (CV, ou vision par ordinateur) est une branche de l'intelligence artificielle, qui traite des techniques permettant d'extraire des informations de \"haut niveau\" utiles √† partir d'images. Donc ce domaine d√©velopp√© depuis les ann√©es 60, on retrouve g√©n√©ralement des techniques provenant des math√©matiques, du traitement d'images, des neurosciences, de l'apprentissage artificiel‚Ä¶ Nous allons ici effleurer ce domaine en nous familiarisant avec OpenCV. Introduction √† OpenCV OpenCV est une biblioth√®que logicielle qui est devenue le \"standard\" du domaine. Cette biblioth√®que fournit un √©norme ensemble de fonctionnalit√©s et d'algorithmes √† la pointe de l'√©tat de l'art. Entre autres sont disponibles: Des m√©canismes d'entr√©es/sorties des images et flux vid√©os (cam√©ras, fichiers‚Ä¶) Des m√©canismes de traitement d'images (gestion des formats, couleurs, d√©formations‚Ä¶ ) Des milliers d'algorithmes d√©velopp√©s par la communaut√© et les industriels (reconnaissance d'image, suivi d'objet, vision 3D, apprentissage‚Ä¶) Ouverture d'une image T√©l√©chargez l'image: Cr√©ez un fichier couleurs.py import numpy as np import cv2 as cv img = cv.imread('ergo_cubes.jpg') Quelle information nous donne print(img.shape) ? On peut acc√©der √† chaque pixel par indexation du tableau img avec img[LIGNE, COLONNE] (ce qui est tr√®s inefficace), que repr√©sente la valeurs donn√©es par img[170,255] ? Pour acc√©der au diff√©rents canaux de couleur on peut de m√™me utiliser: img[:,:,CANAL] avec CANAL la couleur voulue. On peut facilement cr√©er des r√©gions d'int√©r√™t (ROI) en utilisant les m√©canismes disponibles dans python: roi=img[140:225, 210:310] OpenCV offre √©galement quelques fonctionnalit√©s pratiques d'interface utilisateur (GUI). Pour afficher une image: cv.imshow(\"Mon image\", roi) #on donne un nom unique √† chaque fen√™tre cv.waitKey(0) #permet d'attendre √† a l'infini Enfin, on peut √©crire les images dans des fichiers: cv.imwrite(\"roi.png\", roi) Affichez les trois canaux de couleur dans des fen√™tres diff√©rentes Seuil sur la couleur Nous avons vu que les images sont g√©n√©ralement repr√©sent√©s dans l'espace BGR, ce qui est coh√©rent avec le fonctionnement du pixel de l'√©cran (et du capteur), mais moins √©vidant lorsque l'on souhaite travailler sur les couleurs. Comment par exemple d√©finir le volume 3D dans l'espace BGR repr√©sentant le \"rose\"? C'est pourquoi pour traiter la couleur, il est recommand√© de convertir l'encodage de l'image dans un autre espace. L'espace le plus couramment utilis√© est le HSV (Hue, Saturation, Value ou Teinte, Saturation, Valeur). Pour convertir une image de BGR vers HSV il suffit d'utiliser: img_HSV = cv.cvtColor(img, cv.COLOR_BGR2HSV) On notera que l'espace HSV est encod√© avec H dans [0, 179], S dans [0,255] et V dans [0,255] On peut ensuite appliquer un seuil avec: img_seuil = cv.inRange(img_HSV, (MIN_H, MIN_S, MIN_V), (MAX_H, MAX_S, MAX_V)) Le r√©sultat de la fonction de seuil inRange est une image binaire Exp√©rimentez avec les valeurs de seuil pour ne faire appara√Ætre que le cube rouge Note: il est facile de cr√©er des \"trackbars\" pour changer en temps r√©el les valeurs, voir le tutoriel D√©tection des cubes Nous sommes maintenant capable de s√©lectionner des pixels en fonction de leur couleur, il nous faut encore \"regrouper\" ces informations afin de d√©tecter et reconna√Ætre les cubes. Une m√©thode simple consiste √† consid√©rer que les pixels d'une couleur choisie font partie d'un \"blob\" (une r√©gion de pixels voisins) repr√©sentant le m√™me objet. Dans l'image binaire r√©sultat du seuil, il nous suffit de chercher le contour des zones blanches. Pour cela nous allons utiliser la fonction findContours() (voir le tutoriel) imgret, contours, hierarchy = cv.findContours( img_seuil, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE) imgret est la m√™me image que img_seuil contours est une liste contenant tous les contours trouv√©s hierarchy contient les informations sur la hi√©rarchie des contours (les contours √† l'int√©rieur des contours) Sur une image \"naturelle\" (avec du bruit) les contours trouv√©s seront rarement parfaits. Il est possible de \"filtrer\" ces contours en ne consid√©rant par exemple que ceux dons la surface est coh√©rente avec les objets recherch√©s (voir le tutoriel) Parcourez la liste des contours et dessinez les contours dont la surface est comprise entre 2500 et 3500 On utilisera une boucle sur contours, la fonction contourArea() retournant la surface d'un contour, ainsi que la fonction de dessin drawContours() (dessinez sur l'image d'origine) Une fois le contour du cube trouv√©, nous pouvons chercher son centre avec la fonction moments() avec une fonction telle que: def trouver_centroid(cnt): M = cv.moments(cnt) if M['m00'] > 0.0: cx = int(M['m10']/M['m00']) cy = int(M['m01']/M['m00']) return (x, y) else: return (0, 0) Nous pouvons ensuite utiliser la position obtenue pour √©crire un texte: cv.putText(img, 'cube', (x, y), cv.FONT_HERSHEY_SIMPLEX, 1,(255, 255, 255), 1, cv.LINE_AA) Maintenant que nous sommes capable de d√©tecter un cube d'une couleur, √©tendez le programme pour d√©tecter la pr√©sence et la position des 3 cubes Int√©gration avec ROS Nous allons maintenant int√©grer cette d√©tection de cube color√© √† ROS en lisant l'image de la cam√©ra de Ergo Jr simul√©e par Gazebo. On peut visualiser les images avec l'outil rqt_image_view: rosrun rqt_image_view rqt_image_view Les images brutes sont publi√©es sur le topic: /ergo_jr/camera_ergo/image_raw Attrapez chacun des cubes et r√©cup√©rez des images de la cam√©ra qui vous servirons √† v√©rifier le bon fonctionnement de votre programme pr√©c√©dant Dans votre package ROS cr√©ez le fichier ros4pro/src/vision.py ```python import rospy from sensor_msgs.msg import Image from std_srvs.srv import Trigger, TriggerResponse from cv_bridge import CvBridge import cv2 as cv import numpy as np class NodeVision(object): def __init__(self): # Params self.image = None self.debug_img = None self.br = CvBridge() #pour la conversion entre les imags OpenCV et les images ROS # Node cycle rate (in Hz). self.loop_rate = rospy.Rate(10) # Pour publier des images pour le debuggage self.img_pub = rospy.Publisher( '/ergo_jr/camera_ergo/debug_img', Image, queue_size=1) # Pour r√©cup√©rer les images du robot simul√© rospy.Subscriber( '/ergo_jr/camera_ergo/image_raw', Image, self.callback) # Cr√©action d'un service (on utilise le srv standard Trigger) self.service_vision = rospy.Service( '/ergo_jr/cube_detection', Trigger, self.handle_cube) def trouver_cube(self,img): raise NotImplementedError(\"Compl√©tez la partie 2.4 avant d'ex√©cuter\") # ICI le traitement OpenCV # retour du r√©sultat resp = TriggerResponse() # Si pas de cube # resp.success = False # Sinon # resp.success = True # resp.message=\"COULEUR\" return resp def handle_cube(self, req): #M√©thode callback qui sera √©x√©cut√©e √† chaque appel du service # retour du r√©sultat resp = TriggerResponse() resp.success = False # uniquement si l'image existe if self.image is not None: imgtmp = self.image.copy() # on appelle la m√©thode de traitement d'image resp = self.trouver_cube(imgtmp) return resp def callback(self, msg): #m√©thode callback qui sera √©x√©cut√©e √† chaque reception d'un message self.image = self.br.imgmsg_to_cv2(msg, \"bgr8\") #On converti l'image ROS en une image OpenCV def start(self): rospy.loginfo(\"D√©marage du node vision\") while not rospy.is_shutdown(): if self.image is not None: # √©ventuellement, publication d'une image de d√©bug, ici une copie de l'image d'origine self.debug_img = self.image.copy() self.img_pub.publish( self.br.cv2_to_imgmsg(self.debug_img, \"bgr8\")) #On converti l'image OpenCV en une image ROS self.loop_rate.sleep() if __name__ == '__main__': rospy.init_node(\"Vision\") vision = NodeVision() vision.start() ``` √Ä partir de ce squelette, int√©grez votre programme de d√©tection des cubes color√©s On notera qu'il est n√©cessaire d'utiliser CvBridge() afin de faire le lien entre les images OpenCV et les images ROS. On peut appeler le service cr√©√© avec la commande: rosservice call /ergo_jr/cube_detection [TAB] Dans votre programme de mouvement, utilisez l'appel √† ce service afin de d√©tecter la couleur du cube attrap√© et faites une pile de cube Rouge/Vert/Bleu Modifiez les couleurs dans le fichier launch spawn_cubes.launch pour tester diff√©rentes combinaisons üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"perception/keras/":{"url":"perception/keras/","title":"Keras","keywords":"","body":"IV. Perception avec Keras Keras est un syst√®me d'apprentissage automatique utilisant des r√©seaux de neurones. Nous allons l'utiliser ici sur des imagettes sur lesquelles sont inscrites des chiffres marqu√©s manuellement au feutre avec diff√©rentes calligraphies. Le r√©seau de neurones que vous allez cr√©er devra apprendre lui-m√™me √† d√©terminer quel chiffre est marqu√©, ce que l'on appelle classifier. Pr√©requis BAC+2 et + Bonne compr√©hension de Python et numpy Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 0. Installation Le code source se trouve √† cet emplacement. Pour effectuer cet atelier, vous devez installer quelques packages: pip install tensorflow keras imageio matplotlib scikit-image numpy git clone https://gitlab.inria.fr/apere/ros_workshop.git 1. Documentation Pour manipuler le code, vous aurez besoin de consulter plusieurs documentations. De mani√®re g√©n√©rale, vous aurez besoin de numpy. Si vous n'√™tes pas familier avec, vous pouvez consulter: Numpy cheatsheet Numpy Documentation Pour la partie d√©tection et pr√©-processing, nous utiliserons scikit-image: Scikit-Image Documentation Enfin, pour la partie reconnaissance, nous utiliserons keras. Vous devriez trouver ce qu'il vous faut dans le tutoriel suivant: Keras Documentation 2. Partie apprentissage Commencez par ouvrir le fichier learning.py. 2.1 Chargement des donn√©es Prenez connaissance du code, puis lancez le en ex√©cutant √† la ligne de commande: python learning.py Avant d'appuyer sur entr√©e, r√©pondez aux questions suivantes: Que contiennent les variables x_train et y_train? Pourquoi la fonction load_data renvoie-t-elle √©galement les variables x_test et y_test? Quelles sont les formes respectives de x_train et y_train? 2.2 Pr√©visualisation des donn√©es brutes Appuyez sur entr√©e pour continuer, et observez les images. R√©pondez aux questions suivantes: Quelle sont les valeurs des pixels blancs (repr√©sent√©s en jaune) et des pixels noirs ? Observez bien les donn√©es et leurs labels. Toutes les images sont elles simples √† classifier correctement? Ferriez vous des erreurs en les classifiants ? 2.3 Pr√©paration des donn√©es Fermez la fen√™tre et appuyez √† nouveau sur entr√©e. R√©pondez aux questions suivantes: Quelle sont les formes respectives de x_train et y_train maintenant? Pourquoi ces changements ? 2.4 Pr√©visulation des donn√©es pr√©par√©es Appuyez √† nouveau sur entr√©e et observez les images: Quelles sont les valeurs des pixels blanc et des pixels noirs maintenant? Regardez la fonction prepare_input. Quelle transformation des images est effectu√©e? 2.5 Le mod√™le Arr√™tez le script en appuyant sur ctrl+c. Dans le fichier, modifiez la fonction build_model pour impl√©menter le r√©seau LeNet vu pendant la pr√©sentation. Une fois cela fait, relancez le script et faites d√©filer jusqu'√† la partie 2.5. Observez le r√©sum√© du mod√™le: Observez le nombre de param√™tres par couche. Quelles sont les couches qui ont le plus grand nombre de param√™tre? Qu'en concluez vous sur l'utilit√© des couches par convolution ? 2.6 La fonction de cout et l'optimiseur Arr√™tez le script en appuyant sur ctrl+c. Durant la pr√©sentation, nous avons vu que deux fonctions de co√ªt sont r√©guli√®rement utilis√©es dans l'entrainement des r√©seaux de neurones: L'√©rreur quadratique moyenne (Mean squared error) L'entropie crois√©e (Cross entropy) Dans le fichier, modifiez la fonction get_loss pour impl√©menter la fonction de co√ªt adapt√©e √† notre probl√™me. Pendant la pr√©sentation, nous avons vu que l'optimiseur est l'algorithme qui permet de se d√©placer sur la surface d√©ssin√©e par la fonction de cout dans l'espace des param√™tres. Cet algorithme permet de chercher l'endroit ou la fonction de cout est minimale. Un des algorithmes les plus simples s'appelle la descente de gradient (GD) et consiste √† se d√©placer dans le sens de la pente la plus forte √† chaque pas de temps: Dans quelle hypoth√®se cet algorithme permet il de trouver le minimum global de la fonction de co√ªt selon vous ? Pensez vous que cette hypoth√®se soit v√©rifi√©e pour les r√©seaux de neurones ? Que se passe-t-il si cette hypoth√®se n'est pas v√©rifi√©e ? Adam est un optimiseur plus complexe que GD. Sur l'image suivante, on voit plusieurs optimiseurs se d√©placer sur une fonction de cout. Concentrez-vous sur Adam et GD: Quelle semble √™tre la caract√©ristique de Adam compar√©e a GD? Une autre caracteristique de l'algorithme GD, est que la taille du pas qui est effectu√© √† chaque it√©ration est fixe. L'image suivante montre Adam et GD dans un cas ou la pente devient tr√©s forte. Repondez aux questions suivantes: GD arrive-t-il √† converger ? Comprenez vous pourquoi ? Adam ne semble pas soumis au m√™me probl√™me que GD ? Quelle autre caract√©ristique de Adam cela montre t il? Enfin: Quelle conclusion pouvez vous tirer sur l'utilit√© de GD pour entrainer des r√©seaux de neurones ? Quel est l'algorithme utilis√© dans le code ? 2.7 Entrainement Relancez le code et appuyez sur entr√©e jusqu'au d√©clenchement de la partie 2.7. Vous devriez voir les it√©rations d'entrainement se succ√©der: Observez l'√©volution de la pr√©cision sur l'ensemble d'entrainement et l'ensemble de test. Les valeurs sont elles identiques ? √Ä partir de combien d'√©poques le r√©seau est il entrain√© selon vous ? En r√©glant le nombre d'it√©ration d'apprentissage dans le code (argument epochs de la fonction fit), arrivez vous √† observer une phase de sur-apprentissage ? 2.8 Poids appris Appuyez sur entr√©e pour visualiser les noyaux appris par le r√©seau de neurones: En observant les noyaux de la premi√®re couche, arrivez vous √† distinguer le genre de features qui seront extraites par chacun? Pouvez vous en faire de m√™me pour la deuxi√®me couche ? 2.9 Activations Appuyez sur entr√©e, puis rentrez un indice (un entier de n'importe quelle valeur inferieure a 12000): Apr√®s la premi√®re couche de convolution, les features extraites correspondent elles √† celles que vous imaginiez ? Apr√®s la premi√®re couche de pooling, les features pr√©sentes auparavant sont elles conserv√©es ? Apr√®s la deuxi√®me couche de pooling, diriez vous que de l'information spatiale est toujours pr√©sente ? Autrement dit, les activations ressemblent elles toujours √† des images ? 2.10 Entrainement final Arr√™tez le script en appuyant sur ctrl+c. Jusqu'√† pr√©sent, nous avons travaill√© sur l'ensemble des donn√©es, mais pour la suite nous n'aurons besoin que des images de 1 et de 2. Changez la valeur de la variable CLASSES pour ne garder que les classes qui nous int√©ressent, entrainez en r√©seau, puis sauvegardez le dans un fichier. 3. Partie vision Ouvrez le fichier detection.py. 3.1 Pr√©sentation des donn√©es D√©marrez le script. Une des images d'exemple issue du robot devrait vous √™tre pr√©sent√©e: Observez les valeurs de pixels ? Quelles sont les valeurs de pixels blancs et noirs ? De mani√®re g√©nerale, la face des cubes est elle semblable aux images de mnist ? 3.2 Binarisation de l'image Appuyez sur entr√©e, et vous devriez voir s'afficher une image binarisee: Pouvez vous penser √† un algorithme permettant d'arriver √† un r√©sultat √† peu pr√©s similaire ? Dans le code observez la fonction binarize: A quoi sert la fonction threshold_otsu ? Aidez vous de la documentationde scikit-image. Ajoutez une ligne pour afficher la valeur de thresh √† chaque appel de la fonction. Cette valeur est elle la m√™me pour toutes les images ? En commentant successivement les lignes les utilisant, d√©crivez l'impact de chacune des fonctions suivantes: A quoi sert la fonction closing ? A quoi sert la fonction clear_border ? A quoi sert la fonction convex_hull_object ? Concluez en r√©sumant l'enchainement des op√©rations effectu√©es dans la fonction binarize. N'hesitez pas √† vous aider de la documentation de scikit-image. 3.3 Recherche de contours Appuyez sur entr√©e pour faire d√©filer quelques images dont les contours ont √©t√© d√©tect√©s. Observez la fonction get_box_contours: A quoi sert la fonction find_contour ? A quoi sert la fonction approximate_square ? Sur quelle fonction de scikit-image repose-t-elle ? A quoi sert la fonction reorder_contour ? Pourquoi cette op√©ration est elle importante ? Concluez en r√©sumant l'enchainement des op√©rations effectu√©es dans la fonction get_box_contours 3.4 Extraction des vignettes Appuyez sur entr√©e pour faire d√©filer quelques images dont les vignettes ont √©t√© extraites. Observez la fonction get_sprites: Qu'est ce qu'une transformation projective ? Regardez l'ordre des points source et repensez √† la fonction reorder_contour. Son importance est elle plus claire maintenant? Dans quelle limites d'orientation, le cube sera-t-il r√©orient√© correctement ? 3.5 Pr√©paration des images Pendant la phase d'apprentissage, nous avons √©tudi√© la pr√©paration qui √©tait faite des images. Les vignettes que nous allons pr√©senter au r√©seau de neurones doivent aussi subir une pr√©paration pour avoir les m√™mes caract√©ristiques que les images d'entrainement. Remplissez la fonction preprocess_sprites pour effectuer cette pr√©paration. Une fois que cela est fait, executez le script jusqu'√† la fin. 4. Int√©gration Une fois termin√©, vous pouvez vous rendre dans main.py pour tester l'int√©gration de la d√©tection et de la reconnaissance. Bravo ! üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"perception/pytorch/":{"url":"perception/pytorch/","title":"Torch","keywords":"","body":"IV. Perception avec Torch (PyTorch) Torch est une bilbioth√®que opensource d'apprentissage machine et en particulier d'apprentissage profond. Depuis 2018 seule sa version Python nomm√©e PyTorch est maintenue. Nous allons l'utiliser ici sur des imagettes sur lesquelles sont inscrites des chiffres marqu√©s manuellement au feutre avec diff√©rentes calligraphies. Le r√©seau de neurones que vous allez cr√©er devra apprendre lui-m√™me √† d√©terminer quel chiffre est marqu√©, ce que l'on appelle classifier. 0. Installation Pour effectuer cet atelier, vous devez installer quelques packages : pip install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html pip install scikit-image imageio git clone https://gitlab.inria.fr/apere/ros_workshop.git && git checkout e78f295b6 1. Documentation Pour remplir ce code, vous aurez besoin de consulter plusieurs documentations. De mani√®re g√©n√©rale, vous aurez besoin de numpy. Si vous n'√™tes pas familier avec, vous pouvez consulter : Numpy cheatsheet Numpy Documentation Pour la partie d√©tection et pr√©-processing, nous utiliserons scikit-image. Vous pouvez vous inspirer des exemples: Scikit-Image General examples Enfin, pour la partie reconnaissance, nous utiliserons pytorch. Vous devriez trouver ce qu'il vous faut dans le tutoriel suivant: Deep learning with pytorch: a 60 minute blitz 2. Partie reconnaissance (Matin) 2.1 Les donn√©es et le mod√®le Commencez par ouvrir le fichier src/models.py. Pour vous familiariser avec les tenseurs, commencez par remplir les fonctions imag_mean et image_std. Quand cela est fait executez le script une premi√®re fois avec : python src/models.py Quelle est la taille d'un batch de donn√©es d'entr√©e ? De donn√©es de sortie ? Quelle est la moyenne du batch d'entr√©e ? Son √©cart type ? Dans le fichier se trouve une classe LeNet vide, qui contiendra votre mod√®le. En vous r√©f√©rant √† la documentation de pytorch, remplissez la classe pour d√©finir l'architecture LeNet vue pendant la pr√©sentation. Attention, la m√©thode forward prend en entr√©e un tenseur de taille [128, 1, 28, 28] et doit retourner un tenseur de taille [128, 2]. En revanche, la m√©thode infer doit retourner le label (1 ou 2) trouv√© par le r√©seau de neurones pour une image, soit un simple float. Une fois le r√©seau d√©fini, r√©-ex√©cutez le script. Si votre r√©seau est bien d√©fini, il devrait √™tre utilis√© pour faire une inf√©rence. Les labels trouv√©s par le r√©seau de neurones font ils sens ? Remplissez la fonction compute_params_count. Ex√©cutez √† nouveau; quel est le nombre de param√™tres du r√©seau de neurones ? Qu'en pensez vous ? Enfin, si tout le script s'ex√©cute, vous devriez voir s'afficher les noyaux de la premi√®re couche. Qu'en pensez vous ? 2.2 L'entrainement. Dirigez vous vers le fichier train.py. La fonction perform_train_epoch contient la proc√©dure d'entrainement. Remplissez-la en suivant la proc√©dure pr√©sent√©e durant la matin√©e, puis ex√©cutez le script. Si votre algorithme d'entrainement est correctement √©crit, vous devriez voir les courbes d'apprentissage apparaitre. Que pensez vous de ces courbes ? Que pensez vous du niveau de performance √† la fin de l'entrainement? A la suite de l'entrainement, le script vous pr√©sente les noyaux de la couche d'entr√©e du r√©seau. Qu'en pensez vous ? Ont ils √©volu√© par rapport √† tout √† l'heure ? Enfin, pouvez vous entrainer le mod√™le pendant suffisament longtemps pour voir la phase d'overfitting appara√Ætre ? 3. Partie amont (Apr√®s-midi) 3.1 D√©tection Ex√©cutez le fichier src/detection.py. Les donn√©es sont charg√©es depuis le disque dans un tableau numpy, dont un example vous est montr√©. Remplissez la fonction rescale qui permet de r√©-√©chelloner les valeurs du tableau entre 0 et 1. La premi√®re √©tape du pipeline de d√©tection consiste √† binariser l'image. Remplissez la fonction binarize et r√©-ex√©cutez le code. Observez chacun des exemples. Les chiffres ont ils bien disparus ? Les cubes sont ils couverts chacun par une unique zone blanche? Cette zone blanche est elle proche de la surface du cube ? Tant que vous ne pouvez r√©pondre positivement √† toutes ces questions (dans la mesure du raisonnable), ameliorez votre programme! Une fois fini, vous pouvez mettre binarize(im, debug=False) pour √©viter de revoir toutes les images √† chaque fois. Suite √† cela, nous allons effectuer la recherche des cubes dans l'image. Remplissez la fonction get_box contour et r√©-ex√©cutez le code. √Ä nouveau, les cubes sont ils bien d√©tect√©s ? Tant que votre algorithme ne positionne pas correctement les coins des diff√©rentes boites de l'image, am√©liorez-le. Encore une fois quand vous aurez termin√©, d√©sactivez la visualisation avec get_box_contour(im, debug=False). 3.2 Pr√©-processing Nous avons maintenant une fonction get_box_contours qui nous donne, √† partir d'une image d'entr√©e, une liste de contours de quadrilat√®res. Maintenant, il s'ait de transformer ces quaqdrilat√®res en images de la taille des images de mnist. Pour cela, nous allons calculer une transformation projective pour chacun des cubes. Remplissez la fonction get_sprites, puis r√©-executez le code. Si votre code est correct, vous devriez voir s'afficher des imagettes ressemblant (au moin par la taille) aux images de mnist. Une fois cela fait, il ne vous reste plus qu'√† remplir la fonction preprocess_sprites. Le but de cette fonction est de faire en sorte que les images donn√©es au r√©seau de neurones ressemblent le plus possible aux images de l'entrainement. √âtudiez bien les donn√©es d'entr√©e avant de vous mettre au travail (vous aurez peut √™tre besoin de retourner regarder src/models.py ou de regarder src/misc.py). 4. Final Une fois termin√©, vous pouvez vous rendre dans main.py pour tester l'int√©gration de tout votre travail. Bravo ! üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"integration/ergo-tb-keras/":{"url":"integration/ergo-tb-keras/","title":"Poppy Ergo Jr + Turtlebot + Keras","keywords":"","body":"V. Int√©gration Poppy Ergo Jr + Turtlebot + Keras L'int√©gration consiste √† int√©grer dans une m√™me cellule robotique les 3 briques logicielles travaill√©es les autres jours, √† savoir : La manipulation par le bras robotique La navigation avec le robot roulant La vision avec le r√©seau de neurones Le scenario de l'int√©gration est un syst√®me de tri robotis√© de pi√®ces dans un bac 1 ou un bac 2 selon leur marquage au feutre. Pr√©requis Avoir suivi les TP Introduction, Manipulation, Navigation, et Perception Avoir r√©alis√© une √©bauche de n≈ìud Python pour chacun des TP Manipulation, Navigation, et Perception Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Actions s√©quentielles de la cellule robotique Les actions de la cellule robotique sont les suivantes : Le r√©seau de neurone a √©t√© entra√Æn√© au pr√©alable sur les batchs MNIST avec learn.py Le bras robotique prend une photo au dessus du feeder avec sa camera (√©tape \"scan\") Cette photo est envoy√©e au serveur de vision vision_server.py qui va : 3.1. extraire les contours carr√©s des cubes 3.2. transmettre les imagettes rogn√©es selon leurs contours au r√©seau de neurones 3.3. le r√©seau de neurone va effectuer une pr√©diction sur le label marqu√© √† la main 1 ou 2 Le noeud de manipulation r√©cup√®re les coordonn√©es des cubes et leur label Pour chaque cube, il effectue un pick-and-place pour le positionner sur la remorque du Turtlebot. Le label est pass√© sur un param√®tre /ros4pro/label Le Turtlebot lit ce param√®tre et se rend au conteneur 1 ou 2 Il effectue une rotation de 360¬∞ pour faire chuter le cube dans la zone de tri √† l'aide du m√¢t 2. Comment faire ? Selon votre avancement, vous n'√™tes pas oblig√©s de suivre l'ordre de travail ci-dessous. Il y a peut-√™tre un sujet plut√¥t qu'un autre o√π vous √™tes le plus en retard que vous devez rattraper. 2.1 Le ROS master Vous aurez d√©sormais 3 machines interconnect√©es : le robot roulant, le bras et votre station de travail. Donc pensez √† en d√©finir une comme ROS master et √† mettre √† jour tous les ROS_MASTER_URI. Nous vous conseillons d'utiliser le bras comme ROS master car il est branch√© sur secteur et n'est donc pas interrompu. Attention aux conflits durant les tests puisque tout le monde aura le m√™me ROS master ! Avez-vous bien personnalis√© vos noms de robots lors de l'introduction ? 2.2 Le serveur de param√®tres Chaque ROS master d√©marre un serveur de param√®tre qui lui est propre pour y enregistrer des donn√©es sous forme de paires de cl√©s/valeurs, par exemple : /robot1/simulated: True /robot2/simulated: False /robot1/speed: 1.0 /robot2/speed: 1.2 Similairement aux commandes rostopic et rosservice dans le terminal, la commande rosparam dispose d'options pour interagir avec les param√®tres enregistr√©s sur le serveur, et les noms des param√®tres sont hierarchis√©s ave des / : rosparam list : Lister les param√®tres du serveur rosparam get /frigo/fruit: Consulter la valeur du param√®tre /frigo/fruit rosparam set /frigo/fruit Banane: D√©finir (ou √©craser) le param√®tre /frigo/fruit √† la nouvelle valeur Banane C√¥t√© Python, les noeuds peuvent √©galement d√©finir ou consulter des param√®tres avec les fonctions : rospy.get_param(\"/frigo/fruit\") : pour consulter rospy.set_param(\"/frigo/fruit\", \"Banane\") : pour d√©finir ou √©craser 2.3. Pr√©parer la navigation Cr√©ez un nouveau noeud navigate_integrate.py similaire au noeud navigate_waypoints.py utilis√© le jour 2. Additionnellement, il devra attendre que le bras robotique ait d√©fini le param√®tre /ros4pro/label √† la valeur 1 ou 2 avant de d√©marrer la navigation vers le bac 1 ou le bac 2. Suivant la valeur du param√®tre, le robot va d√©poser le cube √† un des deux points. Trouvez les coordonn√©es de ces points sur la carte et √©crivez les dans le script navigate_integrate.py. Apr√®s avoir d√©pos√© le cube, le robot va recommencer et attendre de recevoir une nouvelle valeur. Enregistrez une nouvelle carte en partance de l'endroit o√π le bras va d√©poser le cube (votre place pose) Prenez les coordonn√©es des points 1 et 2 dans le nouveau rep√®re d√©fini par la nouvelle carte, afin de les inscrire dans navigate_integrate.py Pr√©voyez le code pour effectuer des tours sur lui-m√™me au moment de faire chuter le cube dans le bac Ce noeud a besoin de la navigation : roslaunch turtlebot3_navigation turtlebot3_navigation.launch Attendez que la navigation soit initialis√©e pour lancer le noeud : rosrun ros4pro navigate_integrate.py 2.4. Pr√©parer la vision Si vous n'avez pas pris de retard la journ√©e Perception, vous disposez d'un r√©seau de neurones fonctionnel. Nous allons le transformer en service ROS : on pourra lui faire une requ√™te Quels sont les cubes que tu vois ? et il r√©pondra en indiquant les cubes trouv√©s et leur label, le cas √©ch√©ant. Comme chaque service ROS est typ√©, nous allons utiliser le type existant VisionPredict, qui comprend, dans la requ√™te : image : un objet de type sensor_msgs/Image correspondant √† l'image dans laquelle chercher des cubes Dans la r√©ponse de ce service, il n'y a que des listes, car elle comprend entre 0 et n cubes trouv√©s, qui ont chacun 1 label associ√©. La r√©ponse comprend donc : label de type std_msgs/UInt8[] : la liste des labels de chaque cube trouv√© x_center de type std_msgs/UInt32[] : la liste des coordon√©es x des barycentres de chaque cube (dans le rep√®re de la cam√©ra) y_center de type std_msgs/UInt32[] : la liste des coordon√©es y des barycentres de chaque cube (dans le rep√®re de la cam√©ra) x1 de type std_msgs/UInt32[] : la liste des coordon√©es x du coin haut gauche de chaque cube (dans le rep√®re de la cam√©ra) y1 de type std_msgs/UInt32[] : la liste des coordon√©es y du coin haut gauche de chaque cube (dans le rep√®re de la cam√©ra) x2 de type std_msgs/UInt32[] : la liste des coordon√©es x du coin haut droite de chaque cube (dans le rep√®re de la cam√©ra) y2 de type std_msgs/UInt32[] : la liste des coordon√©es y du coin haut droite de chaque cube (dans le rep√®re de la cam√©ra) x3 de type std_msgs/UInt32[] : la liste des coordon√©es x du coin bas droite de chaque cube (dans le rep√®re de la cam√©ra) y3 de type std_msgs/UInt32[] : la liste des coordon√©es y du coin bas droite de chaque cube (dans le rep√®re de la cam√©ra) x4 de type std_msgs/UInt32[] : la liste des coordon√©es x du coin bas gauche de chaque cube (dans le rep√®re de la cam√©ra) y4 de type std_msgs/UInt32[] : la liste des coordon√©es y du coin bas gauche de chaque cube (dans le rep√®re de la cam√©ra) Note : Ainsi, pour chaque cube n¬∞i trouv√© dans la photo, label[i] est le label d√©tect√© par le r√©seau de neurones, et le point de coordonn√©es (x_center[i], y_center[i]) est le barycentre de ce cube dans le rep√®re 2D de la cam√©ra Avez l'aide du tutoriel \"Ecrire un service et client Python\", cr√©er un fichier vision_server.py qui fournit service /ros4pro/vision/predict et r√©pond, pour chaque requ√™te contenant une image, la liste des cubes et des labels qu'il a trouv√©e. R√©alisez d'abord script de test rosrun ros4pro call_vision_service_example.py qui sera le client qui va appeler ce service √† partir d'une image de votre jeu de test. Cela permet de v√©rifier que votre mod√®le et votre entrainement pr√©dit de fa√ßon suffisamment fiables les images du jeu de test. Sinon, v√©rifiez que vous avez bien respect√© les consignes de la journ√©e Perception et que votre r√©seau est entra√Æn√© sur un nombre suffisamment √©lev√© d'√©poques. R√©netra√Ænez-le autant de fois que n√©cessaire pour disposer d'un service fiable. 2.5. Pr√©parer la manipulation Pour la manipulation, nous utiliserons le m√™me noeud manipulate.py que celui de la journ√©e Manipulation. Lorsqu'on le d√©marre avec roslaunch on ajoutera l'argument vision pour lui indiquer qu'il doit faire appel au serveur de vision et donc au r√©seau de neurones plut√¥t que d'utiliser l'emplacement vert pr√©d√©fini. C'est √† dire : roslaunch ros4pro manipulate.launch simulate:=false vision:=true 2.6. Votre scenario fonctionne-t-il ? Pour √™tre consid√©r√© comme un succ√®s, votre cellule doit permettre de trier au moins 3 cubes de mani√®re compl√®tement autonome une fois que vous avez d√©marr√© les roslaunch et rosrun n√©cessaires. L'objectif est que ce scenario de tri puisse fonctionner dans une cellule en production. Cependant vous constaterez de nombreux d√©fauts. Relevez et adressez un √† un ces d√©fauts pour am√©liorer le taux de succ√®s de votre cellule de tri. üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"integration/sawyer-tb-keras/":{"url":"integration/sawyer-tb-keras/","title":"Sawyer + Turtlebot + Keras","keywords":"","body":"V. Int√©gration Sawyer + Turtlebot + Keras L'int√©gration consiste √† int√©grer dans une m√™me cellule robotique les 3 briques logicielles travaill√©es les autres jours, √† savoir : La manipulation par le bras robotique La navigation avec le robot roulant La vision avec le r√©seau de neurones Le scenario de l'int√©gration est un syst√®me de tri robotis√© de pi√®ces dans un bac 1 ou un bac 2 selon leur marquage au feutre. Pr√©requis Avoir suivi les TP Introduction, Manipulation, Navigation, et Perception Avoir r√©alis√© une √©bauche de n≈ìud Python pour chacun des TP Manipulation, Navigation, et Perception Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. 1. Actions s√©quentielles de la cellule robotique Les actions de la cellule robotique sont les suivantes : Le r√©seau de neurone a √©t√© entra√Æn√© au pr√©alable sur les batchs MNIST avec learn.py Sawyer prend une photo au dessus du feeder avec sa camera right_hand_camera (√©tape \"scan\") Cette photo est envoy√©e au serveur de vision vision_server.py qui va : 3.1. extraire les contours carr√©s des cubes 3.2. transmettre les imagettes rogn√©es selon leurs contours au r√©seau de neurones 3.3. le r√©seau de neurone va effectuer une pr√©diction sur le label marqu√© √† la main 1 ou 2 Le noeud de manipulation r√©cup√®re les coordonn√©es des cubes et leur label Pour chaque cube, il effectue un pick-and-place pour le positionner sur la remorque du Turtlebot. Le label est pass√© sur un param√®tre /ros4pro/label Le Turtlebot lit ce param√®tre et se rend au conteneur 1 ou 2 Il effectue une rotation de 360¬∞ pour faire chuter le cube dans la zone de tri √† l'aide du m√¢t 2. Comment faire ? Selon votre avancement, vous n'√™tes pas oblig√©s de suivre l'ordre de travail ci-dessous. Il y a peut-√™tre un sujet plut√¥t qu'un autre o√π vous √™tes le plus en retard que vous devez rattraper. VOus pouvez √©galement vous mettre par 2 ou 3 et attribuer un sujet de travail (Sawyer, Turtlebot ou vision) √† chacun. 2.1 Le ROS master Sawyer d√©marrant son propre ROS master au d√©marrage, afin de communiquer avec lui nous n'avons d'autre choix que de d√©signer Sawyer comme ROS master et faire pointer les ROS_MASTER_URI des Turtlebots et des stations de travail sur Sawyer, uniquement si Sawyer est n√©cessaire dans votre exp√©rimentation. Attention aux conflits durant les tests puisque tout le monde aura le m√™me ROS master ! Lorsque vous n'avez pas besoin de Sawyer, repassez sur un ROS master burgerX.local ou localhost. 2.2 Le serveur de param√®tres Chaque ROS master d√©marre un serveur de param√®tre qui lui est propre pour y enregistrer des donn√©es sous forme de paires de cl√©s/valeurs, par exemple : /robot1/simulated: True /robot2/simulated: False /robot1/speed: 1.0 /robot2/speed: 1.2 Similairement aux commandes rostopic et rosservice dans le terminal, la commande rosparam dispose d'options pour interagir avec les param√®tres enregistr√©s sur le serveur, et les noms des param√®tres sont hierarchis√©s ave des / : rosparam list : Lister les param√®tres du serveur rosparam get /frigo/fruit: Consulter la valeur du param√®tre /frigo/fruit rosparam set /frigo/fruit Banane: D√©finir (ou √©craser) le param√®tre /frigo/fruit √† la nouvelle valeur Banane C√¥t√© Python, les noeuds peuvent √©galement d√©finir ou consulter des param√®tres avec les fonctions : rospy.get_param(\"/frigo/fruit\") : pour consulter rospy.set_param(\"/frigo/fruit\", \"Banane\") : pour d√©finir ou √©craser 2.3. Pr√©parer la navigation Le noeud navigate_integrate.py est similaire au noeud navigate_waypoints.py utilis√© le jour 2. Additionnellement, il attend que Sawyer d√©finisse le param√®tre /ros4pro/label √† la valeur 1 ou 2 avant de d√©marrer la navigation vers le bac 1 ou le bac 2. Suivant la valeur du param√®tre le robot va d√©poser le cube √† un des deux points. Trouvez les coordonn√©es de ces points sur la carte et √©crivez les dans le script ros4pro/src/nodes/navigate_integrate.py. Apr√®s avoir d√©pos√© le cube, le robot va recommencer et attendre de recevoir une nouvelle valeur. Enregistrez une nouvelle carte en partance de l'endroit o√π Sawyer va d√©poser le cube (votre place pose) Prenez les coordonn√©es des points 1 et 2 dans le nouveau rep√®re d√©fini par la nouvelle carte, afin de les inscrire dans navigate_integrate.py Pr√©voyez le code pour effectuer des tours sur lui-m√™me au moment de faire chuter le cube dans le bac Ce noeud a besoin de la navigation : roslaunch turtlebot3_navigation turtlebot3_navigation.launch Attendez que la navigation soit initialis√©e pour lancer le noeud : rosrun ros4pro navigate_integrate.py 2.4. Pr√©parer la vision Si vous n'avez pas pris de retard la journ√©e Perception, vous disposez d'un serveur de vision qui est en √©coute du service /ros4pro/vision/predict et r√©pondra, pour chaque requ√™te contenant une image, la liste des cubes et des labels qu'il a trouv√©e. Utilisez le script de test de la vision rosrun ros4pro call_vision_service_example.py pour v√©rifier que votre mod√®le et votre entrainement pr√©dit de fa√ßon suffisamment fiables les images du jeu de test. Sinon, v√©rifiez que vous avez bien respect√© les consignes de la journ√©e Perception et que votre r√©seau est entra√Æn√© sur un nombre suffisamment √©lev√© d'√©poques. R√©netra√Ænez-le autant de fois que n√©cessaire pour disposer d'un serveur fiable. 2.5. Pr√©parer la manipulation Pour la manipulation, nous utiliserons le m√™me noeud manipulate.py que celui de la journ√©e Manipulation. Lorsqu'on le d√©marre avec roslaunch on ajoutera l'argument vision pour lui indiquer qu'il doit faire appel au serveur de vision et donc au r√©seau de neurones plut√¥t que d'utiliser l'emplacement vert pr√©d√©fini. C'est √† dire : roslaunch ros4pro manipulate.launch simulate:=false vision:=true 2.6. Votre scenario fonctionne-t-il ? Pour √™tre consid√©r√© comme un succ√®s, votre cellule doit permettre de trier au moins 3 cubes de mani√®re compl√®tement autonome une fois que vous avez d√©marr√© les roslaunch et rosrun n√©cessaires. L'objectif est que ce scenario de tri puisse fonctionner dans une cellule en production. Cependant vous constaterez de nombreux d√©fauts. Relevez et adressez un √† un ces d√©fauts pour am√©liorer le taux de succ√®s de votre cellule de tri. üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"theory/":{"url":"theory/","title":"VI. Robotique th√©orique","keywords":"","body":"Robotique th√©orique Ce cours et TP aborde les notions de base des concepts th√©oriques sous-jacents √† la robotique : les mod√®les g√©om√©triques direct et indirect, ainsi que les probablit√©s appliqu√©es aux filtres en traitement du signal. Pr√©requis G√©om√©trie et trigonom√©trie BAC+2 et sup√©rieur voire options scientifiques des lyc√©es Diapositives View PDF This browser does not support PDFs. Please download the PDF to view it: Download PDF. Travaux pratiques Mod√®le g√©om√©trique direct Lancez la commande roslaunch ros4pro_robotique_theorique scara_fake_direct.launch pour lancer une simu de SCARA. Une fen√™tre avec des sliders permet de bouger le bras. Modifiez le script src/geometrique_direct.py en y mettant votre mod√®le g√©om√©trique direct. Il calcul la position de l'effecteur √† partir des angles des articulations. Lancez la coommande rosrun ros4pro_robotique_theorique geometrique_direct.py pour ex√©cuter le script de test du mod√®le g√©om√©trique direct. Mod√®le g√©om√©trique indirect Lancez la commande roslaunch ros4pro_robotique_theorique scara_fake_inverse.launch pour lancer une simu de SCARA. Modifiez le script src/geometrique_inverse.py en y mettant votre mod√®le g√©om√©trique inverse. Il calcul les angles et la position des articulations pour que l'effecteur sooit √† la position voulue. Lancez la coommande rosrun ros4pro_robotique_theorique geometrique_inverse.py pour ex√©cuter le script de test du mod√®le g√©om√©trique inverse. Probabilit√©s robotiques Installez les biblioth√®ques n√©cessaires √† l'ex√©cution des d√©monstrations de probabilit√© pour la robotiique, dans un terminal ex√©cutez les coommandes suivantes : sudo apt install python3-pip pip3 install matplotlib scipy python3 histogram_filter.py Pour lancer les d√©monstrations de probabilit√© allez dans le dossier proba et utilisez python3 pour ex√©cuter les fichiers, ex: python3 extended_kalman_filter.py. üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"simulation/gazebo/":{"url":"simulation/gazebo/","title":"Gazebo","keywords":"","body":" VII. Simulation avec Gazebo Lancement de Gazebo avec un monde \"vide\" Chargement d'un mod√®le Un simple cube Chargement du mod√®le du robot Chargement des contr√¥leurs de moteurs MoveIt D√©marrer MoveIt Cr√©er un node python pour contr√¥ler le robot Cr√©er un package ROS ros4pro √âditer manipulate.py Utiliser la pince Ajouter des obstacles VII. Simulation avec Gazebo En robotique il est souvent tr√®s utile de pouvoir travailler en simulation. Un simulateur physique permet essentiellement de simuler des forces/couples sur des objets et des articulations. Gazebo est un environnement de simulation physique pour robotique, support√© par ROS. Nous nous servirons de la simulation dans Gazebo de mani√®re transparente, \"comme si\" il s'agissait du v√©ritable robot. La simulation va nous permettre: De charger des mod√®les URDF De simuler des moteurs De simuler une cam√©ra De simuler les contacts Notons que Gazebo est constitu√© d'un serveur (non graphique, gzserver) et d'un client (graphique, gzclient) ce qui permet √©galement de calculer une simulation sur une machine distante par exemple. Lancement de Gazebo avec un monde \"vide\" roslaunch gazebo_ros empty_world.launch Ce que nous allons utiliser, mais il est bien sur possible de cr√©er des environnements plus complexes. Chargement d'un mod√®le Les mod√®les sont d√©crit par un fichier xml selon la norme URDF (Universal Robot Description Format). Un simple cube Dans poppy_ergo_jr_gazebo/urdf on peut voir la d√©finition d'un cube. Pour charger cet URDF dans Gazebo: rosrun gazebo_ros spawn_model -file cube.urdf -urdf -model test -x 0 -y 0 -z 1 Quelles sont les dimensions du cube? Quelle est la masse du cube? Exp√©rimentez pour expliquer la diff√©rence entre visual et collision (menu View/Collisions) Observez la position du cube avec rostopic echo -n 1 /gazebo/model_states Modifiez cette position avec rosservice call /gazebo/set_model_state [TAB] (Utilisez la compl√©tion du terminal avec [TAB] pour remplir le message) Appliquez une force de 10N selon l'axe x pendant 5s sur le cube en utilisant: rosservice call /gazebo/apply_body_wrench (Quel est le \"bodyname\" √† utiliser?) Pour supprimer le mod√®le: rosservice call gazebo/delete_model \"model_name: 'test'\" Chargement du mod√®le du robot On utilise ici un format interm√©diaire \"xacro\", permettant d'ajouter un capacit√© de \"script\" (pour calculer des position par exemple) et g√©n√©rer un URDF. On peut visualiser la topologie du mod√®le avec: urdf_to_graphiz poppy_ergo_jr.urdf (un pdf est g√©n√©r√©) Ouvrez le PDF obtenu puis d√©terminez : Que repr√©sentent les rectangles ? Que repr√©sentent les bulles ? Que repr√©sentent les fl√®ches et surtout les valeurs xyz et rpy associ√©es ? Pour importer le mod√®le dans Gazebo: roslaunch poppy_ergo_jr_gazebo load_ergo_model.launch On peut \"explorer\" le mod√®le dans le menu √† gauche. Pour mieux visualiser les articulations: View/Transparent View/Joints Le mod√®le s'effondre car les moteurs ne sont pas simul√©s. Appliquez un couple de -0.5Nm sur l'articulation m2 pendant 3s avec: rosservice call /gazebo/apply_joint_effort [TAB] Chargement des contr√¥leurs de moteurs Afin de rentre la simulation plus r√©aliste, nous allons lancer des contr√¥leurs de moteurs qui vont simuler le comportement de moteurs r√©els. Il existe plusieurs type de contr√¥leurs disponible dans Gazebo, nous allons tout d'abord exp√©rimenter avec les contr√¥leurs en position les plus simples. lancez: roslaunch poppy_ergo_jr_gazebo load_ergo_position_controllers.launch On constate la cr√©ation de topics pour chaque contr√¥leur Envoyez des commandes en position: rostopic pub /ergo_jr/m2_position_controller/command [TAB] [TAB] Comment contr√¥ler la position de la pince dans l'espace cart√©sien? MoveIt Dans le cas g√©n√©ral, calculer les mouvements n√©cessaires pour atteindre un objectif sans collision est un probl√®me compliqu√© (cf. robotique th√©orique). Cette t√¢che est effectu√©e par un planificateur, tel quel MoveIt qui int√®gre: Le mod√®le cin√©matique du robot (√† partir de l'URDF) Une gestion des collisions (internes et avec l'environnement) Un planificateur de trajectoire D√©marrer MoveIt Pr√©c√©demment nous avons exp√©riment√© avec les contr√¥leurs en position. MoveIt a besoin de contr√¥leurs l√©g√®rement diff√©rents (contr√¥leurs de trajectoire). Lancez tout d'abord Gazebo et charger le mod√®le du robot. Puis lancez les contr√¥leurs en trajectoire avec: roslaunch poppy_ergo_jr_gazebo load_ergo_controllers.launch ‚áí Vous pouvez aussi combiner ces 3 √©tapes avec un seul fichier .launch: roslaunch poppy_ergo_jr_gazebo start_gazebo.launch gripper:=true lamp:=false Lancez MoveIt avec: roslaunch poppy_ergo_jr_moveit_config start_moveit.launch gripper:=true lamp:=false Essayez de manipuler le robot: Dans Query/Planning Group s√©lectionnez arm, dans Options cochez Allow Approx IK Solutions Cliquez sur Planning/Plan and Execute observer Rviz et Gazebo Cr√©er un node python pour contr√¥ler le robot Cr√©er un package ROS ros4pro cd ~/catkin_ws/src catkin_create_pkg ros4pro # Cette commande cr√©√© le package mkdir -p ros4pro/src # On cr√©√© un dossier src dans le package touch ros4pro/src/manipulate.py # On cr√©√© un noeud Python \"manipulate.py\" chmod +x ros4pro/src/manipulate.py # On rend ce noeud ex√©cutable pour pouvoir le lancer avec rosrun √âditer manipulate.py Nous allons avoir besoin des imports suivants: #!/usr/bin/env python import rospy from moveit_commander.move_group import MoveGroupCommander from geometry_msgs.msg import Pose from math import radians, cos, sin import tf_conversions as transform rospy.init_node('ros4pro_node') Pour utiliser le \"commander\" MoveIt il faut d√©clarer: commander = MoveGroupCommander(\"arm\") Il est possible de r√©cup√©rer la \"pose\" actuelle: current_pose = commander.get_current_pose().pose Faites bouger le robot et affichez cette pose. Quelle est la pose initiale? Cr√©ez une nouvelle pose (cf. le message [[https://docs.ros.org/en/api/geometry_msgs/html/msg/Pose.html][Pose], modifiez la position et l'orientation ce cette pose et ex√©cutez l√†: commander.set_pose_target(pose) #envoie la pose au commander plan = commander.go(wait=True) #√©x√©cute le mouvement avec attente commander.stop() #force l'arr√™t du mouvement pour plus de s√©curit√© commander.clear_pose_targets() #force le nettoyage des objectifs du commander pour plus de s√©curit√© Pour nous aider on peut cr√©er un quaternion √† partir d'une rotation au format Roll/Pitch/Yaw avec: q = transform.transformations.quaternion_from_euler(roll, pitch, yaw) q retourn√© est une liste de 4 √©l√©ments contenant x, y, z, w roll, pitch, yaw sont des angles en radian (la fonction radians(angle) permet de convertir des degr√©s en radians) Utiliser la pince Lancez le service avec rosrun poppy_ergo_jr_gazebo gripper_gz_service.py Il est possible d'ouvrir/fermer la pince avec la commande: rosservice call /ergo_jr/close_gripper \"data: false\" pour ouvrir la pince, et \"true\" pour la fermer. Pour utiliser ce service dans votre node se r√©f√©rer √† la documentation On notera que le type du service est SetBool d√©fini dans le module std_srvs.srv Charger des cubes dans l'environnement de simulation avec roslaunch poppy_ergo_jr_gazebo spawn_cubes.launch Les cubes sont plac√©s sur un rayon de 0.21m par rapport au r√©f√©rentiel avec des angles de -25¬∞, 0¬∞ et +25¬∞ Essayez d'attraper chacun des cubes Essayer d'empiler les cubes en -25¬∞ et +25¬∞ sur le cube en 0¬∞ Ajouter des obstacles Nous allons avoir besoin du message PoseStamped d√©fini dans le module geometry_msgs.msg Il est possible d'ajouter un obstacle pour MoveIt comme ceci par exemple: scene = PlanningSceneInterface() rospy.sleep(1) #petite attente n√©cessaire ps = PoseStamped() ps.header.frame_id = \"base_link\" ps.pose.position.x = 0.0543519994715 ps.pose.position.y = -0.202844423521 ps.pose.position.z = 0.1 q = transform.transformations.quaternion_from_euler(0, 0, radians(15)) ps.pose.orientation.x = q[0] ps.pose.orientation.y = q[1] ps.pose.orientation.z = q[2] ps.pose.orientation.w = q[3] scene.add_box(\"obstacle\", ps, (0.025, 0.1, 0.2)) #dimensions de la boite On peut ensuite enlever l'obstacle avec: scene.remove_world_object(\"obstacle\") Cr√©ez un mouvement pour attraper un cube en √©vitant cet obstacle Nous voulons maintenant utiliser ce m√©canisme pour ajouter les cubes comme obstacles au fur et √† mesure que nous les empilons Ajouter cube1 comme obstacle Attraper cube2, le poser sur cube1 et ajouter cube2 comme obstacle idem pour cube3 Les positions des objets sont publi√©s en temps r√©el par Gazebo dans le \"topic\" /gazebo/model_states üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "},"faq/pi/":{"url":"faq/pi/","title":"FAQ robots bas√©s sur Raspberry Pi","keywords":"","body":"üì• Flasher la carte SD Note pr√©liminaire : la carte SD du robot ne se comporte pas tout-√†-fait comme une carte SD \"normale\". Elle ne permet pas de stocker des fichiers dessus ; il est √©galement normal qu'une carte SD ins√©r√©e dans votre ordinateur n'apparaisse pas dans le Poste de Travail avant de l'avoir flash√©e. T√©l√©chargez ces images ROS en vue de remettre √† z√©ro les cartes SD des robots pour ROS4PRO (‚ö†Ô∏è‚è≥ Optimisez votre temps, le t√©l√©chargement peut prendre 1 heure) : Image du Turtlebot pour ROS4PRO Image de Poppy Ergo Jr pour ROS4PRO Pour flasher l'une de ces images sur une carte SD : extrayez le fichier compress√© .zip ou .7z (g√©n√©ralement clic droit > Extraire) dans un dossier de votre ordinateur (pas sur la carte SD) : vous obtenez un fichier d'extension .img ‚ö†Ô∏è ne fa√Ætes pas de glisser-d√©poser ni de copier-coller de cette image vers la carte SD comme s'il s'agissait d'une cl√© USB : Il est n√©cessaire d'utiliser un logiciel de flash comme Etcher ou dd üìÄ Tapez la commande etcher dans le terminal Ubuntu pour ouvrir l'utilitaire de flash pr√©install√© (ou bien t√©l√©chargez Etcher s'il n'existe pas encore) Dans Etcher, \"Flash from file\", s√©lectionnez le fichier image ainsi que la destination (la carte SD) et validez Le flash de la carte SD est en cours ... ‚ö†Ô∏è‚è≥ Optimisez votre temps, la copie dure environ 15 minutes. D√®s qu'Etcher a termin√©, votre carte SD est pr√™te √† √™tre configur√©e pour le Wifi et/ou ins√©r√©e dans le robot Optionnellement, en cas de besoin de restaurer les robots avec les images d'usine, voici les liens (mais ces images ne sont pas utilisables avec ROS4PRO) : Image d'usine du Turtlebot (pas de namespace complet, n'inclut pas la posibilit√© d'int√©grer plusieurs robots) Image d'usine de Poppy Ergo Jr (avec l'interface graphique http://poppy.local mais sans ROS) üì° Connecter le robot en Wifi ‚ö†Ô∏è La mise en place de la connexion du robot en Wifi ne n√©cessite pas de d√©marrer le robot Ins√©rer la carde SD du robot en question dans votre poste de travail (pas dans votre robot) et ouvrir la partition nomm√©e boot T√©l√©charger le fichier wpa_supplicant.conf dans boot et modifiez-le pour renseigner le bon mot de passe wifi √† l'int√©rieur (sans changer le nom de fichier). Respectez la casse : majuscules/minuscules. Cr√©er un fichier vide nomm√© ssh au m√™me endroit dans boot (par exemple avec la commande touch ssh dans le dossier courant) Taper la commande sync puis √©jectez proprement la carte SD dans le navigateur de fichier pour √©viter toute perte de donn√©es avant de la retirer. Ces 2 fichiers wpa_supplicant.conf et ssh seront supprim√©s au prochain d√©marrage du robot, signalant que la demande de connexion Wifi a bien √©t√© prise en compte. C'est donc normal que vous ne les trouviez plus en regardant √† nouveau le contenu de boot apr√®s un premier d√©marrage du robot. En cas de probl√®me, il est possible de connecter un √©cran HDMI √† la Raspberry Pi, le gestionnaire r√©seau se trouve en haut √† droite. La connexion Wifi fonctionne aussi avec les points d'acc√®s mobiles d'Android et iOS. üñß Se connecter via SSH √† un robot SSH (Secure SHell) permet d'ouvrir un terminal √† distance sur une autre machine que celle sur laquelle on tape les commandes (par exemple le robot, qui n'a ni clavier ni √©cran pour interagir avec un terminal). Il est n√©cessaire de conna√Ætre : Le nom de la machine distante (par ex poppy.local ou raspberrypi.local) Le nom d'utilisateur propri√©taire de la session sur laquelle ouvrir un terminal (toujours pi dans notre cas) Le mot de passe de cette session (cf mots ce passe par d√©faut ci-dessous) La commande est la suivante, √† taper dans un terminal sur Ubuntu : ssh pi@poppy.local Taper yes pour confirmer la connexion puis taper le mot de passe. Votre invite de commande devrait d√©sormais indiquer pi@poppy.local~$ : toute commande tap√©e dans ce terminal sera ex√©cut√©e par le robot. En cas d'erreur, consultez la proc√©dure de diagnostic ci-dessous. üîë Mots de passe par d√©faut Turtlebot Nom d'utilisateur pi Nom de machine raspberrypi (ajouter .local dans les commandes : raspberrypi.local) Mot de passe turtlebot Poppy Nom d'utilisateur pi Nom de machine poppy (ajouter .local dans les commandes : poppy.local) Mot de passe raspberry üåà Personnaliser les noms de robots et ordinateurs Au d√©marrage du TP, tous les robots et les ordinateurs poss√®dent le m√™me nom √† savoir ubuntu (votre ordinateur), poppy (le robot manipulateur), turtlebot (le robot roulant), ce qui posera probl√®me lorsqu'on les fera communiquer ensemble. Pour ces 3 machines, nous allons donc changer leur nom, en ajoutant juste votre num√©ro de groupe √† la fin, par exemple poppy5. üíªü§ñ Pour personnaliser votre nom, il faut ouvrir un terminal sur la machine √† renommer (via SSH pour les robots) puis : sudo hostnamectl set-hostname sudo reboot Veillez bien √† utiliser ensuite ce nouveau nom dans vos futures commandes (SSH ou ROS_MASTER_URI, ...). Si vous avez nomm√© votre robot poppy5 par exemple, il faudra donc utiliser poppy5.local. üîß Proc√©dure de diagnostic üíª Dans un terminal taper ping poppy.local (pour Poppy) ou ping raspberrypi.local (pour Turtlebot) : Si 1 ligne s'affiche chaque seconde avec des statistiques de temps en millisecondes ‚û°Ô∏è Test r√©seau r√©ussi. Vous avez peut-√™tre oubli√© de d√©marrer le roscore ou bien ROS_MASTER_URI dans le fichier ~/.bashrc pointe vers le mauvais robot Si une erreur survient et la commande s'arr√™te ‚û°Ô∏è Test r√©seau √©chou√©. V√©rifiez que la LED verte ACT de la Raspberry Pi vacille pendant environ 45 secondes lorsque vous venez de brancher l'alimentation : Si ACT vacille en üü¢ ‚û°Ô∏è Votre Raspberry Pi d√©marre correctement mais la configuration r√©seau est incorrecte. V√©rifiez que vous avez plac√© le fichier wpa_supplicant.conf au bon endroit dans la partition boot sur la carte SD si vous √™tes en Wifi ; ou bien connectez-vous avec un c√¢ble RJ45 sur un routeur Si ACT ne vacille pas ‚û°Ô∏è Votre Raspberry Pi ne d√©marre pas correctement. La LED rouge PWR s'allume-t-elle ? Si PWR s'allume en üî¥ ‚û°Ô∏è Votre Raspberry Pi est fonctionnelle mais la carte SD ne poss√®de pas une image valable. Recommencez la proc√©dure de flash ci-dessus. Si PWR ne s'allume pas ‚û°Ô∏è Votre Raspberry Pi n'est pas fonctionnelle. Vous avez peut-√™tre mal branch√© la Pixl (Poppy) ou bien le c√¢ble rouge-noir (Turtlebot) üîî Mon Turtlebot bipe üîã Il s'agit du signal de batterie faible et il ne doit pas √™tre ignor√©. Turtlebot est aliment√© par une batterie puissante de type Li-Po. Ce type de batterie rend dangereux leur utilisation lorsque la charge est tr√®s faible. Dans un cas extr√™me elle pourrait chauffer et prendre feu. Mettre en charge rapidement la batterie lorsque Turtlebot bipe. üìö Auteurs Jessica Colombel (Inria), R√©mi Fabre (CATIE), Jean-Baptiste Horel (CATIE), Yoan Mollard (Bordeaux INP), Alexandre P√©r√© (Inria), Steve N'Guyen (LaBRI) . üí¨ Besoin d'aide ? Posez votre question sur le forum francophone des utilisateurs de ROS. üìÖ Derni√®re mise √† jour : 2021-01-03T19:14:24+01:00 $(\"div\").first().addClass(\"color-theme-2\"); "}}